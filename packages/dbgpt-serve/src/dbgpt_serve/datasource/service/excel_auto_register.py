#!/usr/bin/env python3
"""
Excel è‡ªåŠ¨æ³¨å†Œåˆ°æ•°æ®æºæœåŠ¡
æ”¯æŒè‡ªåŠ¨ç¼“å­˜å’Œå¢é‡å¯¼å…¥
"""
# ruff: noqa: E501

import hashlib
import json
import logging
import os
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import openpyxl
import pandas as pd

logger = logging.getLogger(__name__)


class ExcelCacheManager:
    """Excel ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º pilot/data/excel_cache
        """
        if cache_dir is None:
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
            current_dir = Path(__file__).parent
            cache_dir = (
                current_dir.parent.parent.parent.parent.parent
                / "pilot"
                / "data"
                / "excel_cache"
            )

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # å…ƒæ•°æ®æ•°æ®åº“è·¯å¾„
        self.meta_db_path = self.cache_dir / "excel_metadata.db"
        self._init_metadata_db()

    def _init_metadata_db(self):
        """åˆå§‹åŒ–å…ƒæ•°æ®æ•°æ®åº“"""
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS excel_metadata (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                content_hash VARCHAR(64) UNIQUE NOT NULL,
                original_filename VARCHAR(255) NOT NULL,
                table_name VARCHAR(255) NOT NULL,
                db_name VARCHAR(255) NOT NULL,
                db_path TEXT NOT NULL,
                row_count INTEGER NOT NULL,
                column_count INTEGER NOT NULL,
                columns_info TEXT NOT NULL,
                summary_prompt TEXT,
                data_schema_json TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                access_count INTEGER DEFAULT 0
            )
        """)

        try:
            cursor.execute("SELECT data_schema_json FROM excel_metadata LIMIT 1")
        except sqlite3.OperationalError:
            cursor.execute(
                "ALTER TABLE excel_metadata ADD COLUMN data_schema_json TEXT"
            )

        conn.commit()
        conn.close()

    @staticmethod
    def calculate_excel_hash(df: pd.DataFrame, filename: str) -> str:
        """
        è®¡ç®— Excel çš„å†…å®¹å“ˆå¸Œå€¼ï¼ˆåŸºäºDataFrameï¼Œç”¨äºå‘åå…¼å®¹ï¼‰

        Args:
            df: DataFrame å¯¹è±¡
            filename: æ–‡ä»¶åï¼ˆåŒ…å«åœ¨å“ˆå¸Œè®¡ç®—ä¸­ï¼‰

        Returns:
            SHA256 å“ˆå¸Œå€¼
        """
        # ç»„åˆæ–‡ä»¶åã€åˆ—åå’Œæ•°æ®å†…å®¹
        content_parts = [
            filename,
            ",".join(df.columns.tolist()),
            df.to_csv(index=False),
        ]
        content = "\n".join(content_parts)

        return hashlib.sha256(content.encode("utf-8")).hexdigest()

    @staticmethod
    def calculate_file_hash(file_path: str, sheet_names: List[str] = None) -> str:
        """
        è®¡ç®—æ–‡ä»¶çº§åˆ«çš„å“ˆå¸Œå€¼ï¼ˆåŸºäºæ–‡ä»¶å†…å®¹å’Œsheetåˆ—è¡¨ï¼‰

        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_names: è¦å¤„ç†çš„sheetåç§°åˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneåˆ™ä¸è€ƒè™‘sheetä¿¡æ¯ï¼‰

        Returns:
            SHA256 å“ˆå¸Œå€¼
        """
        sha256_hash = hashlib.sha256()

        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è®¡ç®—å“ˆå¸Œ
        with open(file_path, "rb") as f:
            # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)

        # å¦‚æœæŒ‡å®šäº†sheet_namesï¼Œå°†å…¶ä¹Ÿçº³å…¥å“ˆå¸Œè®¡ç®—ï¼ˆç¡®ä¿ä¸åŒsheetç»„åˆäº§ç”Ÿä¸åŒå“ˆå¸Œï¼‰
        if sheet_names:
            sheet_info = ",".join(sorted(sheet_names))
            sha256_hash.update(sheet_info.encode("utf-8"))

        return sha256_hash.hexdigest()

    def get_cached_info(self, content_hash: str) -> Optional[Dict]:
        """
        æ ¹æ®å†…å®¹å“ˆå¸Œè·å–ç¼“å­˜ä¿¡æ¯

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼

        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute(
            """
            SELECT 
                content_hash, original_filename, table_name, db_name, db_path,
                row_count, column_count, columns_info, summary_prompt, data_schema_json,
                created_at, last_accessed, access_count
            FROM excel_metadata
            WHERE content_hash = ?
        """,
            (content_hash,),
        )

        row = cursor.fetchone()

        if row:
            # æ›´æ–°è®¿é—®ç»Ÿè®¡
            cursor.execute(
                """
                UPDATE excel_metadata
                SET last_accessed = CURRENT_TIMESTAMP,
                    access_count = access_count + 1
                WHERE content_hash = ?
            """,
                (content_hash,),
            )
            conn.commit()

            result = {
                "content_hash": row[0],
                "original_filename": row[1],
                "table_name": row[2],
                "db_name": row[3],
                "db_path": row[4],
                "row_count": row[5],
                "column_count": row[6],
                "columns_info": json.loads(row[7]),
                "summary_prompt": row[8],
                "data_schema_json": row[9],
                "created_at": row[10],
                "last_accessed": row[11],
                "access_count": row[12],
            }
            conn.close()
            return result

        conn.close()
        return None

    def save_cache_info(
        self,
        content_hash: str,
        original_filename: str,
        table_name: str,
        db_name: str,
        db_path: str,
        df: pd.DataFrame,
        summary_prompt: str = None,
        data_schema_json: str = None,
    ):
        """
        ä¿å­˜ç¼“å­˜ä¿¡æ¯

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼
            original_filename: åŸå§‹æ–‡ä»¶å
            table_name: è¡¨å
            db_name: æ•°æ®åº“å
            db_path: æ•°æ®åº“è·¯å¾„
            df: DataFrame å¯¹è±¡
            summary_prompt: æ•°æ®ç†è§£æç¤ºè¯
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        columns_info = [
            {"name": col, "dtype": str(df[col].dtype)} for col in df.columns
        ]

        cursor.execute(
            """
            INSERT OR REPLACE INTO excel_metadata
            (content_hash, original_filename, table_name, db_name, db_path,
             row_count, column_count, columns_info, summary_prompt, data_schema_json)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            (
                content_hash,
                original_filename,
                table_name,
                db_name,
                db_path,
                len(df),
                len(df.columns),
                json.dumps(columns_info, ensure_ascii=False),
                summary_prompt,
                data_schema_json,
            ),
        )

        conn.commit()
        conn.close()

    def update_summary_prompt(self, content_hash: str, summary_prompt: str):
        """
        æ›´æ–°æ•°æ®ç†è§£æç¤ºè¯

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼
            summary_prompt: æ•°æ®ç†è§£æç¤ºè¯
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute(
            """
            UPDATE excel_metadata
            SET summary_prompt = ?
            WHERE content_hash = ?
        """,
            (summary_prompt, content_hash),
        )

        conn.commit()
        conn.close()

    def delete_cache_by_hash(self, content_hash: str):
        """
        æ ¹æ®å†…å®¹å“ˆå¸Œåˆ é™¤ç¼“å­˜

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute(
            "DELETE FROM excel_metadata WHERE content_hash = ?", (content_hash,)
        )
        deleted = cursor.rowcount

        conn.commit()
        conn.close()

        return deleted > 0

    def delete_cache_by_filename(self, filename: str):
        """
        æ ¹æ®æ–‡ä»¶ååˆ é™¤ç¼“å­˜

        Args:
            filename: åŸå§‹æ–‡ä»¶å
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute(
            "DELETE FROM excel_metadata WHERE original_filename = ?", (filename,)
        )
        deleted = cursor.rowcount

        conn.commit()
        conn.close()

        return deleted > 0

    def list_all_cache(self) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰ç¼“å­˜è®°å½•

        Returns:
            ç¼“å­˜è®°å½•åˆ—è¡¨
        """
        conn = sqlite3.connect(str(self.meta_db_path))
        cursor = conn.cursor()

        cursor.execute("""
            SELECT 
                content_hash, original_filename, table_name, db_name,
                row_count, column_count, created_at, last_accessed, access_count
            FROM excel_metadata
            ORDER BY last_accessed DESC
        """)

        records = cursor.fetchall()
        conn.close()

        result = []
        for row in records:
            result.append(
                {
                    "content_hash": row[0],
                    "original_filename": row[1],
                    "table_name": row[2],
                    "db_name": row[3],
                    "row_count": row[4],
                    "column_count": row[5],
                    "created_at": row[6],
                    "last_accessed": row[7],
                    "access_count": row[8],
                }
            )

        return result


class ExcelAutoRegisterService:
    """Excel è‡ªåŠ¨æ³¨å†Œåˆ°æ•°æ®æºæœåŠ¡"""

    _instance = None
    _lock = None

    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿åªæœ‰ä¸€ä¸ªå®ä¾‹"""
        if cls._instance is None:
            # åˆ›å»ºé”ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if cls._lock is None:
                import threading

                cls._lock = threading.Lock()

            with cls._lock:
                # åŒé‡æ£€æŸ¥
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, llm_client=None, model_name=None):
        """åˆå§‹åŒ–æœåŠ¡"""
        if not hasattr(self, "_initialized"):
            self.cache_manager = ExcelCacheManager()
            self.llm_client = llm_client
            self.model_name = model_name

            current_dir = Path(__file__).parent
            base_dir = (
                current_dir.parent.parent.parent.parent.parent / "pilot" / "meta_data"
            )
            self.db_storage_dir = base_dir / "excel_dbs"
            self.db_storage_dir.mkdir(parents=True, exist_ok=True)

            self._initialized = True
        else:
            if llm_client is not None:
                self.llm_client = llm_client
            if model_name is not None:
                self.model_name = model_name

    def _get_cell_value(self, cell) -> Optional[str]:
        """è·å–å•å…ƒæ ¼å€¼ï¼Œå¤„ç†å…¬å¼"""
        if cell.value is None:
            return None

        if cell.data_type == "f":
            try:
                if isinstance(cell.value, str) and cell.value.startswith("="):
                    cleaned = self._clean_excel_formula(cell.value)
                    return cleaned if cleaned else None
            except Exception as e:
                logger.warning(f"è·å–å…¬å¼è®¡ç®—ç»“æœå¤±è´¥: {e}")

        value_str = str(cell.value)
        return (
            value_str.replace("\n", "")
            .replace("\r", "")
            .replace("\t", "")
            .replace(" ", "")
        )

    def _get_cell_bg_color(self, cell) -> Optional[str]:
        """è·å–å•å…ƒæ ¼èƒŒæ™¯è‰²"""
        fill = cell.fill
        if fill.fill_type == "solid" or fill.fill_type == "patternFill":
            fg_color = fill.fgColor
            if fg_color.type == "rgb":
                rgb = fg_color.rgb
                if rgb and len(rgb) == 8:
                    return rgb[2:]
                return rgb
            elif fg_color.type == "indexed":
                return f"indexed_{fg_color.indexed}"
            elif fg_color.type == "theme":
                tint = fg_color.tint if fg_color.tint else 0
                return f"theme_{fg_color.theme}_{tint:.2f}"
        return None

    def _detect_header_rows_with_color(
        self, excel_file_path: str, sheet_name: str = None
    ) -> Tuple[List[int], Dict]:
        """ä½¿ç”¨é¢œè‰²ä¿¡æ¯å’ŒLLMæ£€æµ‹è¡¨å¤´è¡Œ

        Args:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: sheetåç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨active sheet
        """
        wb = openpyxl.load_workbook(excel_file_path)
        ws = wb[sheet_name] if sheet_name else wb.active

        max_check_rows = min(20, ws.max_row)
        max_cols = ws.max_column

        rows_data = []
        rows_colors = []

        for row_idx in range(1, max_check_rows + 1):
            row_values = []
            row_colors = []
            for col_idx in range(1, max_cols + 1):
                cell = ws.cell(row=row_idx, column=col_idx)
                cell_value = self._get_cell_value(cell)
                row_values.append(cell_value if cell_value is not None else "")
                row_colors.append(self._get_cell_bg_color(cell))
            rows_data.append(row_values)
            rows_colors.append(row_colors)

        if self.llm_client:
            try:
                header_rows = self._detect_header_rows_with_llm_and_color(
                    rows_data, rows_colors
                )
                if header_rows:
                    color_info = {
                        "rows_data": rows_data,
                        "rows_colors": rows_colors,
                        "header_rows": header_rows,
                        "max_cols": max_cols,
                    }
                    return header_rows, color_info
            except Exception as e:
                logger.warning(f"LLMæ£€æµ‹å¤±è´¥: {e}")

        df_raw = pd.DataFrame(rows_data)
        header_rows = self._detect_header_rows_rule_based(df_raw)
        color_info = {
            "rows_data": rows_data,
            "rows_colors": rows_colors,
            "header_rows": header_rows,
            "max_cols": max_cols,
        }
        return header_rows, color_info

    def _detect_header_rows_with_llm_and_color(
        self, rows_data: List[List], rows_colors: List[List]
    ) -> List[int]:
        """
        ä½¿ç”¨LLMå’Œé¢œè‰²ä¿¡æ¯æ£€æµ‹è¡¨å¤´è¡Œ

        Args:
            rows_data: å‰20è¡Œçš„æ•°æ®
            rows_colors: å‰20è¡Œçš„é¢œè‰²ä¿¡æ¯

        Returns:
            è¡¨å¤´è¡Œçš„ç´¢å¼•åˆ—è¡¨ï¼ˆä»0å¼€å§‹ï¼‰
        """
        import asyncio
        import inspect

        from dbgpt.core import (
            ModelMessage,
            ModelMessageRoleType,
            ModelRequest,
            ModelRequestContext,
        )

        # æ„å»ºè¡¨æ ¼æ–‡æœ¬è¡¨ç¤ºï¼ˆåŒ…å«é¢œè‰²ä¿¡æ¯ï¼‰
        table_text = "è¡Œå·\tåˆ—1\tåˆ—2\tåˆ—3\t...\té¢œè‰²ä¿¡æ¯\n"
        for idx, (row_data, row_colors) in enumerate(
            zip(rows_data[:20], rows_colors[:20])
        ):
            # åªæ˜¾ç¤ºå‰10åˆ—æ•°æ®
            row_values = [str(val) if val else "" for val in row_data[:10]]
            # ç»Ÿè®¡é¢œè‰²åˆ†å¸ƒ
            color_counts = {}
            for color in row_colors:
                if color:
                    color_counts[color] = color_counts.get(color, 0) + 1
            color_info = (
                ", ".join(
                    [f"{color[:8]}({count}åˆ—)" for color, count in color_counts.items()]
                )
                if color_counts
                else "æ— èƒŒæ™¯è‰²"
            )
            table_text += f"{idx}\t" + "\t".join(row_values) + f"\t[{color_info}]\n"

        # æ„å»ºprompt
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªExcelæ•°æ®åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹Excelæ–‡ä»¶çš„å‰20è¡Œæ•°æ®ï¼Œåˆ¤æ–­å“ªäº›è¡Œæ˜¯è¡¨å¤´è¡Œï¼ˆåˆ—åè¡Œï¼‰ã€‚

æ³¨æ„ï¼š
1. è¡¨å¤´è¡Œé€šå¸¸åŒ…å«åˆ—åï¼Œå¦‚"ID"ã€"æ—¥æœŸ"ã€"åç§°"ã€"é‡‘é¢"ç­‰
2. å¯èƒ½æœ‰å¤šçº§è¡¨å¤´ï¼ˆå¤šè¡Œè¡¨å¤´ï¼‰ï¼Œæœ€åä¸€è¡Œæ˜¯æœ€å…·ä½“çš„åˆ—å
3. è¡¨å¤´è¡Œé€šå¸¸æœ‰ç‰¹æ®Šçš„èƒŒæ™¯è‰²ï¼ŒåŒä¸€çº§çš„è¡¨å¤´é€šå¸¸ä½¿ç”¨ç›¸åŒæˆ–ç›¸ä¼¼çš„èƒŒæ™¯è‰²
4. è¡¨å¤´è¡Œä¹‹å‰å¯èƒ½æœ‰æ±‡æ€»ä¿¡æ¯è¡Œã€è¯´æ˜è¡Œï¼ˆå¦‚"è¯·å‹¿åˆ é™¤"ã€å…¬å¼ç­‰ï¼‰ï¼Œè¿™äº›ä¸æ˜¯è¡¨å¤´
5. è¡¨å¤´è¡Œä¹‹åæ˜¯æ•°æ®è¡Œï¼Œæ•°æ®è¡Œé€šå¸¸æ²¡æœ‰èƒŒæ™¯è‰²æˆ–ä½¿ç”¨ä¸åŒçš„èƒŒæ™¯è‰²
6. **å¦‚æœå‘ç°ä¸­è‹±æ–‡å¯¹ç…§çš„æ ‡é¢˜è¡Œï¼ˆå¦‚"Name"å’Œ"ä¸­è‹±æ–‡å"ï¼‰ï¼Œåªä¿ç•™ä¸­æ–‡æ ‡é¢˜è¡Œçš„ç´¢å¼•ï¼Œè·³è¿‡è‹±æ–‡æ ‡é¢˜è¡Œ**
7. **å¿½ç•¥åŒ…å«"@@"ã€"="ç­‰å…¬å¼æ ‡è®°çš„è¡Œï¼Œè¿™äº›æ˜¯Excelå†…éƒ¨æ ‡è®°è¡Œ**
8. **ä¼˜å…ˆé€‰æ‹©åŒ…å«ä¸­æ–‡åˆ—åçš„è¡Œä½œä¸ºè¡¨å¤´ï¼Œè€Œä¸æ˜¯è‹±æ–‡åˆ—åçš„è¡Œ**

è¯·ä»”ç»†åˆ†ææ•°æ®å†…å®¹å’Œé¢œè‰²ä¿¡æ¯ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{{
  "reason": "åˆ¤æ–­ç†ç”±ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›è¡Œä½œä¸ºè¡¨å¤´ï¼Œä»¥åŠå¦‚ä½•è¯†åˆ«å’Œè¿‡æ»¤é‡å¤çš„ä¸­è‹±æ–‡æ ‡é¢˜è¡Œ",
  "header_rows": [è¡Œç´¢å¼•åˆ—è¡¨ï¼Œä»0å¼€å§‹],
}}

ç¤ºä¾‹1ï¼š
å¦‚æœç¬¬0è¡Œæ˜¯"è®¢å•ä¿¡æ¯"ï¼ˆæœ‰è“è‰²èƒŒæ™¯ï¼‰ï¼Œç¬¬1è¡Œæ˜¯"è¡Œ ID, è®¢å• ID, è®¢å•æ—¥æœŸ..."ï¼ˆæœ‰æµ…è“è‰²èƒŒæ™¯ï¼‰ï¼Œç¬¬2è¡Œå¼€å§‹æ˜¯æ•°æ®ï¼ˆæ— èƒŒæ™¯è‰²ï¼‰ï¼Œåˆ™è¿”å›ï¼š
{{
  "header_rows": [0, 1],
  "reason": "ç¬¬0-1è¡Œæœ‰èƒŒæ™¯è‰²ä¸”åŒ…å«è¡¨å¤´å…³é”®è¯ï¼Œç¬¬0è¡Œæ˜¯åˆ†ç±»æ ‡ç­¾ï¼Œç¬¬1è¡Œæ˜¯å…·ä½“åˆ—åã€‚ç¬¬2è¡Œå¼€å§‹æ— èƒŒæ™¯è‰²ä¸”å†…å®¹ä¸ºæ•°æ®å€¼"
}}

ç¤ºä¾‹2ï¼š
å¦‚æœç¬¬0-2è¡Œæ˜¯æ±‡æ€»ä¿¡æ¯ï¼ˆæ— èƒŒæ™¯è‰²æˆ–ä¸åŒèƒŒæ™¯è‰²ï¼‰ï¼Œç¬¬3è¡Œæ˜¯è¡¨å¤´ï¼ˆæœ‰èƒŒæ™¯è‰²ä¸”åŒ…å«IDã€æ—¥æœŸç­‰å…³é”®è¯ï¼‰ï¼Œåˆ™è¿”å›ï¼š
{{
  "header_rows": [3],
  "reason": "ç¬¬3è¡Œæœ‰ç‰¹æ®ŠèƒŒæ™¯è‰²ä¸”åŒ…å«IDã€æ—¥æœŸç­‰å…¸å‹è¡¨å¤´å…³é”®è¯ï¼Œå‰3è¡Œæ˜¯æ±‡æ€»ä¿¡æ¯"
}}

ç¤ºä¾‹3ï¼ˆé‡å¤æ ‡é¢˜è¡Œï¼‰ï¼š
å¦‚æœç¬¬2è¡Œæ˜¯"Onboarding_Date, Staff_ID, Department..."ï¼ˆè‹±æ–‡æ ‡é¢˜ï¼‰ï¼Œç¬¬3è¡Œæ˜¯"å…¥èŒæ—¥æœŸ, å‘˜å·¥ID, éƒ¨é—¨..."ï¼ˆä¸­æ–‡æ ‡é¢˜ï¼‰ï¼Œè¿™ä¸¤è¡Œè¡¨ç¤ºç›¸åŒçš„åˆ—ï¼Œåˆ™åªè¿”å›ï¼š
{{
  "header_rows": [3],
  "reason": "ç¬¬2è¡Œæ˜¯è‹±æ–‡æ ‡é¢˜ï¼Œç¬¬3è¡Œæ˜¯ä¸­æ–‡æ ‡é¢˜ï¼Œå®ƒä»¬è¡¨ç¤ºç›¸åŒçš„åˆ—ã€‚æ ¹æ®è§„åˆ™ï¼Œåªä¿ç•™ä¸­æ–‡æ ‡é¢˜è¡Œï¼ˆç¬¬3è¡Œï¼‰ï¼Œè·³è¿‡è‹±æ–‡æ ‡é¢˜è¡Œï¼ˆç¬¬2è¡Œï¼‰"
}}

ç°åœ¨è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼ˆå³ä¾§æ˜¾ç¤ºäº†æ¯è¡Œçš„é¢œè‰²åˆ†å¸ƒï¼‰ï¼š

{table_text}

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼š"""

        # è°ƒç”¨LLMï¼ˆéæµå¼ï¼‰
        request_params = {
            "messages": [ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)],
            "temperature": 0.1,
            "max_new_tokens": 1000,
            "context": ModelRequestContext(stream=False),
        }

        if self.model_name:
            request_params["model"] = self.model_name

        request = ModelRequest(**request_params)

        # è·å–å“åº”
        stream_response = self.llm_client.generate_stream(request)

        full_text = ""
        if inspect.isasyncgen(stream_response):

            async def collect_async():
                text = ""
                async for chunk in stream_response:
                    chunk_text = self._extract_chunk_text(chunk)
                    if chunk_text:
                        text = chunk_text
                return text

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                full_text = loop.run_until_complete(collect_async())
            finally:
                loop.close()
        elif inspect.isgenerator(stream_response):
            for chunk in stream_response:
                chunk_text = self._extract_chunk_text(chunk)
                if chunk_text:
                    full_text = chunk_text
        else:
            raise Exception(f"Unexpected response type: {type(stream_response)}")

        # è§£æJSONç»“æœ
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = full_text.find("{")
            end_idx = full_text.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = full_text[start_idx:end_idx]
                result = json.loads(json_str)

                header_rows = result.get("header_rows", [])
                reason = result.get("reason", "")

                logger.info(f"LLMåˆ¤æ–­ç†ç”±: {reason}")

                # éªŒè¯ç»“æœ
                if isinstance(header_rows, list) and all(
                    isinstance(x, int) for x in header_rows
                ):
                    # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                    valid_rows = [r for r in header_rows if 0 <= r < len(rows_data)]
                    if valid_rows:
                        return sorted(valid_rows)
                    else:
                        logger.warning(f"LLMè¿”å›çš„è¡¨å¤´è¡Œç´¢å¼•æ— æ•ˆ: {header_rows}")
                        return None
                else:
                    logger.warning(f"LLMè¿”å›çš„è¡¨å¤´è¡Œæ ¼å¼æ— æ•ˆ: {header_rows}")
                    return None
            else:
                logger.warning(f"æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSON: {full_text[:200]}")
                return None
        except json.JSONDecodeError as e:
            logger.warning(f"LLMè¾“å‡ºJSONè§£æå¤±è´¥: {e}, è¾“å‡º: {full_text[:200]}")
            return None

    def _merge_headers_by_color(self, color_info: Dict) -> List[str]:
        """
        åŸºäºé¢œè‰²ä¿¡æ¯åˆå¹¶è¡¨å¤´

        ç­–ç•¥ï¼š
        1. å¯¹äºæ¯ä¸ªè¡¨å¤´è¡Œï¼Œè¯†åˆ«åŒä¸€é¢œè‰²çš„åˆ—
        2. å¦‚æœåŒä¸€é¢œè‰²çš„åˆ—ä¸­åªæœ‰ä¸€ä¸ªå•å…ƒæ ¼æœ‰å€¼ï¼Œç”¨è¯¥å€¼å¡«å……å…¶ä»–åˆ—
        3. å°†å¤šè¡Œè¡¨å¤´åˆå¹¶ä¸ºå•è¡Œï¼Œç”¨"-"è¿æ¥

        Args:
            color_info: é¢œè‰²ä¿¡æ¯å­—å…¸

        Returns:
            åˆå¹¶åçš„è¡¨å¤´åˆ—è¡¨
        """
        rows_data = color_info["rows_data"]
        rows_colors = color_info["rows_colors"]
        header_rows = color_info["header_rows"]
        max_cols = color_info["max_cols"]

        # æå–è¡¨å¤´è¡Œçš„æ•°æ®å’Œé¢œè‰²
        header_data = [rows_data[i] for i in header_rows]
        header_colors = [rows_colors[i] for i in header_rows]

        # å¯¹æ¯ä¸€è¡Œè¡¨å¤´ï¼ŒæŒ‰é¢œè‰²å’Œä½ç½®è¿ç»­æ€§åˆ†ç»„
        filled_headers = []
        for row_idx, (row_data, row_colors) in enumerate(
            zip(header_data, header_colors)
        ):
            color_position_groups = []
            current_group = None
            current_color = None

            for col_idx, (value, color) in enumerate(zip(row_data, row_colors)):
                if color:
                    if (
                        color == current_color
                        and current_group
                        and col_idx == current_group[-1][0] + 1
                    ):
                        current_group.append((col_idx, value))
                    else:
                        if current_group:
                            color_position_groups.append((current_color, current_group))
                        current_group = [(col_idx, value)]
                        current_color = color
                else:
                    if current_group:
                        color_position_groups.append((current_color, current_group))
                        current_group = None
                        current_color = None

            if current_group:
                color_position_groups.append((current_color, current_group))

            filled_row = list(row_data)
            for color, cells in color_position_groups:
                non_empty_values = [
                    (idx, val) for idx, val in cells if val and str(val).strip()
                ]

                if len(non_empty_values) == 1:
                    fill_value = non_empty_values[0][1]
                    for col_idx, _ in cells:
                        filled_row[col_idx] = fill_value
                elif len(non_empty_values) > 1:
                    non_empty_indices = sorted([idx for idx, _ in non_empty_values])

                    for i, (val_idx, val) in enumerate(
                        sorted(non_empty_values, key=lambda x: x[0])
                    ):
                        start_idx = val_idx
                        if i < len(non_empty_indices) - 1:
                            end_idx = non_empty_indices[i + 1]
                        else:
                            end_idx = max([idx for idx, _ in cells]) + 1

                        cells_to_fill = [
                            (idx, v) for idx, v in cells if start_idx <= idx < end_idx
                        ]
                        for col_idx, _ in cells_to_fill:
                            filled_row[col_idx] = val

            filled_headers.append(filled_row)

        # åˆå¹¶å¤šçº§è¡¨å¤´
        combined_headers = []
        for col_idx in range(max_cols):
            # æ”¶é›†è¯¥åˆ—çš„æ‰€æœ‰éç©ºå€¼ï¼ˆä»ä¸Šå±‚åˆ°åº•å±‚ï¼‰
            col_values = []
            for row_idx in range(len(filled_headers)):
                val = filled_headers[row_idx][col_idx]
                if val and str(val).strip():
                    val_str = str(val).strip()
                    # å»é™¤æ¢è¡Œç¬¦ã€æ™®é€šç©ºæ ¼å’Œä¸é—´æ–­ç©ºæ ¼ï¼ˆåœ¨åˆå¹¶å‰å°±æ¸…ç†ï¼‰
                    val_str = (
                        val_str.replace("\n", "")
                        .replace("\r", "")
                        .replace("\t", "")
                        .replace(" ", "")
                        .replace("\u00a0", "")
                    )
                    # é¿å…é‡å¤å€¼
                    if not col_values or val_str != col_values[-1]:
                        col_values.append(val_str)

            # ç”¨"-"è¿æ¥å¤šçº§è¡¨å¤´
            if col_values:
                combined = "-".join(col_values)
            else:
                combined = f"Column_{col_idx}"

            combined_headers.append(combined)

        return combined_headers

    def _process_multi_level_header(
        self, df_raw: pd.DataFrame, excel_file_path: str, sheet_name: str = None
    ) -> pd.DataFrame:
        """å¤„ç†å¤šçº§è¡¨å¤´

        Args:
            df_raw: åŸå§‹DataFrame
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: sheetåç§°
        """

        header_rows, color_info = self._detect_header_rows_with_color(
            excel_file_path, sheet_name
        )

        if not header_rows:
            header_rows = [0]

        combined_headers = self._merge_headers_by_color(color_info)

        cleaned_headers = []
        for header in combined_headers:
            cleaned = str(header)
            parts = cleaned.split("-")
            valid_parts = [
                p
                for p in parts
                if "=" not in p
                and "@@" not in p
                and "@" not in p
                and p.strip()
                and p.strip() not in ["-", "_", ""]
            ]

            if valid_parts:
                cleaned = "-".join(valid_parts)
            else:
                cleaned = f"Column_{len(cleaned_headers)}"

            cleaned = (
                cleaned.replace("\n", "")
                .replace("\r", "")
                .replace("\t", "")
                .replace(" ", "")
                .replace("\u00a0", "")
            )
            cleaned = cleaned.replace("--", "-").replace("__", "_").strip("-_")

            if not cleaned or cleaned in ["-", "_"]:
                cleaned = f"Column_{len(cleaned_headers)}"

            cleaned_headers.append(cleaned)

        final_headers = []
        seen = {}
        for header in cleaned_headers:
            if header in seen:
                seen[header] += 1
                final_headers.append(f"{header}_{seen[header]}")
            else:
                seen[header] = 0
                final_headers.append(header)

        data_start_row = max(header_rows) + 1
        data_df = df_raw.iloc[data_start_row:].copy()

        if len(final_headers) > len(data_df.columns):
            final_headers = final_headers[: len(data_df.columns)]
        elif len(final_headers) < len(data_df.columns):
            for i in range(len(final_headers), len(data_df.columns)):
                final_headers.append(f"Column_{i}")

        data_df.columns = final_headers
        data_df = data_df.dropna(how="all").reset_index(drop=True)

        return data_df

    def _clean_excel_formula(self, text: str) -> str:
        """æ¸…ç†Excelå…¬å¼å’Œç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return text

        text_str = str(text)

        if text_str.startswith("="):
            import re

            quoted_texts = re.findall(r'["\']([^"\']+)["\']', text_str)
            if quoted_texts:
                text_str = "".join(quoted_texts)
            else:
                cleaned = re.sub(r"[=&()]", "", text_str)
                cleaned = re.sub(
                    r"CHAR\s*\(\s*\d+\s*\)", "", cleaned, flags=re.IGNORECASE
                )
                cleaned = re.sub(
                    r"CONCATENATE\s*\([^)]*\)", "", cleaned, flags=re.IGNORECASE
                )
                cleaned = re.sub(r"[^a-zA-Z0-9\u4e00-\u9fff_]", "", cleaned)
                if not cleaned:
                    return ""
                text_str = cleaned

        if "@@" in text_str or text_str.startswith("@"):
            import re

            text_str = re.sub(r"@@[^\u4e00-\u9fff-]*", "", text_str)
            text_str = text_str.replace("@", "")
            if not text_str.strip():
                return ""

        text_str = (
            text_str.replace("\n", "")
            .replace("\r", "")
            .replace("\t", "")
            .replace(" ", "")
        )
        return text_str

    def _remove_empty_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ é™¤å®Œå…¨ä¸ºç©ºçš„åˆ—"""
        df_cleaned = df.dropna(axis=1, how="all")

        empty_cols = [
            col
            for col in df_cleaned.columns
            if df_cleaned[col]
            .apply(lambda x: pd.isna(x) or (isinstance(x, str) and x.strip() == ""))
            .all()
        ]

        if empty_cols:
            df_cleaned = df_cleaned.drop(columns=empty_cols)

        return df_cleaned

    def _remove_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ é™¤åˆ—åå’Œæ•°æ®å€¼éƒ½å®Œå…¨é‡å¤çš„åˆ—"""
        columns_to_remove = []

        for i, col1 in enumerate(df.columns):
            if col1 in columns_to_remove:
                continue

            for col2 in df.columns[i + 1 :]:
                if col2 in columns_to_remove or col1 != col2:
                    continue

                try:
                    if df[col1].equals(df[col2]):
                        columns_to_remove.append(col2)
                except Exception:
                    try:
                        if (
                            df[col1]
                            .fillna("__NULL__")
                            .equals(df[col2].fillna("__NULL__"))
                        ):
                            columns_to_remove.append(col2)
                    except Exception:
                        pass

        if columns_to_remove:
            df_cleaned = df.drop(columns=columns_to_remove)
        else:
            df_cleaned = df

        return df_cleaned

    def _format_date_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ¼å¼åŒ–æ—¥æœŸåˆ—ä¸ºYYYY-MM-DDæ ¼å¼"""
        df_formatted = df.copy()

        for col in df_formatted.columns:
            if pd.api.types.is_datetime64_any_dtype(df_formatted[col]):
                df_formatted[col] = df_formatted[col].apply(
                    lambda x: x.strftime("%Y-%m-%d") if pd.notna(x) else None
                )
            elif df_formatted[col].dtype == "object":
                non_null_values = df_formatted[col].dropna()
                if len(non_null_values) > 0:
                    sample_val = non_null_values.iloc[0]
                    if isinstance(sample_val, (pd.Timestamp, datetime)):
                        df_formatted[col] = df_formatted[col].apply(
                            lambda x: x.strftime("%Y-%m-%d")
                            if pd.notna(x) and isinstance(x, (pd.Timestamp, datetime))
                            else x
                        )

        return df_formatted

    def _convert_column_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ¹æ®å­—æ®µçš„å®é™…å€¼è¿›è¡Œæ™ºèƒ½ç±»å‹è½¬æ¢

        è½¬æ¢ç­–ç•¥ï¼š
        1. å°è¯•è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
        2. å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼ˆæ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰
        3. å¦‚æœéƒ½å¤±è´¥ï¼Œä¿æŒå­—ç¬¦ä¸²ç±»å‹

        Args:
            df: åŸå§‹DataFrame

        Returns:
            è½¬æ¢åçš„DataFrame
        """
        df_converted = df.copy()

        for col in df_converted.columns:
            # è·³è¿‡å·²ç»æ˜¯æ•°å€¼ç±»å‹çš„åˆ—
            if df_converted[col].dtype in ["int64", "float64", "int32", "float32"]:
                continue

            # è·³è¿‡å·²ç»æ˜¯æ—¥æœŸç±»å‹çš„åˆ—
            if pd.api.types.is_datetime64_any_dtype(df_converted[col]):
                continue

            # åªå¤„ç†objectç±»å‹çš„åˆ—
            if df_converted[col].dtype == "object":
                non_null_data = df_converted[col].dropna()

                if len(non_null_data) == 0:
                    continue

                # ç­–ç•¥1: å°è¯•è½¬æ¢ä¸ºæ—¥æœŸç±»å‹
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®çœ‹èµ·æ¥åƒæ—¥æœŸï¼ˆè‡³å°‘30%çš„éç©ºå€¼èƒ½è§£æä¸ºæ—¥æœŸï¼‰
                date_success_count = 0
                try:
                    # å°è¯•è§£æå‰100ä¸ªéç©ºå€¼
                    sample_size = min(100, len(non_null_data))
                    sample_data = non_null_data.head(sample_size)

                    for val in sample_data:
                        if pd.notna(val):
                            try:
                                pd.to_datetime(str(val), errors="raise")
                                date_success_count += 1
                            except Exception:
                                pass

                    # å¦‚æœè¶…è¿‡30%çš„å€¼èƒ½è§£æä¸ºæ—¥æœŸï¼Œåˆ™è½¬æ¢æ•´ä¸ªåˆ—ä¸ºæ—¥æœŸ
                    if date_success_count > sample_size * 0.3:
                        try:
                            df_converted[col] = pd.to_datetime(
                                df_converted[col], errors="coerce"
                            )
                            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ YYYY-MM-DD
                            df_converted[col] = df_converted[col].apply(
                                lambda x: x.strftime("%Y-%m-%d")
                                if pd.notna(x)
                                else None
                            )
                            logger.debug(f"åˆ— '{col}' è½¬æ¢ä¸ºæ—¥æœŸç±»å‹")
                            continue
                        except Exception:
                            pass
                except Exception:
                    pass

                # ç­–ç•¥2: å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®çœ‹èµ·æ¥åƒæ•°å­—ï¼ˆè‡³å°‘50%çš„éç©ºå€¼èƒ½è§£æä¸ºæ•°å­—ï¼‰
                numeric_success_count = 0
                has_decimal = False

                try:
                    sample_size = min(100, len(non_null_data))
                    sample_data = non_null_data.head(sample_size)

                    for val in sample_data:
                        if pd.notna(val):
                            val_str = str(val).strip()
                            # ç§»é™¤å¸¸è§çš„æ•°å­—æ ¼å¼å­—ç¬¦ï¼ˆåƒä½åˆ†éš”ç¬¦ã€è´§å¸ç¬¦å·ç­‰ï¼‰
                            val_str = (
                                val_str.replace(",", "")
                                .replace("ï¿¥", "")
                                .replace("$", "")
                                .replace("â‚¬", "")
                                .replace(" ", "")
                            )

                            # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—
                            try:
                                float_val = float(val_str)
                                numeric_success_count += 1
                                # æ£€æŸ¥æ˜¯å¦æœ‰å°æ•°éƒ¨åˆ†
                                if "." in str(val) and float_val != int(float_val):
                                    has_decimal = True
                            except Exception:
                                pass

                    # å¦‚æœè¶…è¿‡50%çš„å€¼èƒ½è§£æä¸ºæ•°å­—ï¼Œåˆ™è½¬æ¢æ•´ä¸ªåˆ—ä¸ºæ•°å€¼ç±»å‹
                    if numeric_success_count > sample_size * 0.5:
                        try:
                            # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç§»é™¤æ ¼å¼å­—ç¬¦ï¼Œå†è½¬æ¢ä¸ºæ•°å€¼
                            df_converted[col] = df_converted[col].astype(str)
                            df_converted[col] = (
                                df_converted[col]
                                .str.replace(",", "")
                                .str.replace("ï¿¥", "")
                                .str.replace("$", "")
                                .str.replace("â‚¬", "")
                                .str.strip()
                            )
                            df_converted[col] = pd.to_numeric(
                                df_converted[col], errors="coerce"
                            )

                            # æ ¹æ®æ˜¯å¦æœ‰å°æ•°å†³å®šä½¿ç”¨æ•´æ•°è¿˜æ˜¯æµ®ç‚¹æ•°
                            if not has_decimal and df_converted[col].notna().any():
                                # æ£€æŸ¥æ‰€æœ‰éç©ºå€¼æ˜¯å¦éƒ½æ˜¯æ•´æ•°
                                all_integers = True
                                for val in df_converted[col].dropna():
                                    if pd.notna(val) and val != int(val):
                                        all_integers = False
                                        break

                                if all_integers:
                                    df_converted[col] = df_converted[col].astype(
                                        "Int64"
                                    )  # å¯ç©ºæ•´æ•°ç±»å‹
                                    logger.debug(f"åˆ— '{col}' è½¬æ¢ä¸ºæ•´æ•°ç±»å‹")
                                else:
                                    df_converted[col] = df_converted[col].astype(
                                        "float64"
                                    )
                                    logger.debug(f"åˆ— '{col}' è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹")
                            else:
                                df_converted[col] = df_converted[col].astype("float64")
                                logger.debug(f"åˆ— '{col}' è½¬æ¢ä¸ºæµ®ç‚¹æ•°ç±»å‹")

                            continue
                        except Exception as e:
                            logger.debug(f"åˆ— '{col}' è½¬æ¢ä¸ºæ•°å€¼ç±»å‹å¤±è´¥: {e}")
                            pass
                except Exception as e:
                    logger.debug(f"åˆ— '{col}' æ•°å€¼ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
                    pass

                # ç­–ç•¥3: ä¿æŒä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼ˆobjectï¼‰
                # ä¸åšä»»ä½•è½¬æ¢

        return df_converted

    def _format_numeric_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ ¼å¼åŒ–æ•°å€¼åˆ—ä¸ºä¸¤ä½å°æ•°

        Args:
            df: åŸå§‹DataFrame

        Returns:
            æ ¼å¼åŒ–åçš„DataFrameï¼ˆæ•°å€¼åˆ—ä¿ç•™ä¸¤ä½å°æ•°ï¼‰
        """
        df_formatted = df.copy()

        for col in df_formatted.columns:
            # åªå¤„ç†æ•°å€¼ç±»å‹çš„åˆ—
            if df_formatted[col].dtype in [
                "int64",
                "float64",
                "int32",
                "float32",
                "Int64",
            ]:
                try:
                    # å°†æ•°å€¼åˆ—è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¹¶ä¿ç•™ä¸¤ä½å°æ•°
                    # ä½¿ç”¨ apply ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ ¼å¼åŒ–ä¸ºä¸¤ä½å°æ•°ï¼ˆåŒ…æ‹¬æ•´æ•°ï¼‰
                    df_formatted[col] = pd.to_numeric(
                        df_formatted[col], errors="coerce"
                    ).apply(lambda x: round(float(x), 2) if pd.notna(x) else x)
                    # ç¡®ä¿æ•°æ®ç±»å‹ä¸º float64ï¼Œè¿™æ · DuckDB ä¼šæ­£ç¡®å­˜å‚¨ä¸ºæµ®ç‚¹æ•°
                    df_formatted[col] = df_formatted[col].astype("float64")
                    logger.debug(f"åˆ— '{col}' æ ¼å¼åŒ–ä¸ºä¸¤ä½å°æ•° (float64)")
                except Exception as e:
                    logger.warning(f"æ ¼å¼åŒ–åˆ— '{col}' å¤±è´¥: {e}")
                    # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œä¿æŒåŸæ ·
                    pass

        return df_formatted

    def _detect_header_rows_rule_based(self, df_raw: pd.DataFrame) -> List[int]:
        """åŸºäºè§„åˆ™çš„è¡¨å¤´è¡Œæ£€æµ‹"""

        header_keywords = [
            "id",
            "ID",
            "ç¼–å·",
            "åºå·",
            "è¡Œ",
            "è®¢å•",
            "æ—¥æœŸ",
            "date",
            "Date",
            "æ—¶é—´",
            "time",
            "Time",
            "åç§°",
            "name",
            "Name",
            "å®¢æˆ·",
            "äº§å“",
            "é‡‘é¢",
            "ä»·æ ¼",
            "price",
            "Price",
            "é”€å”®é¢",
            "åˆ©æ¶¦",
            "æ•°é‡",
            "quantity",
            "Quantity",
            "ç±»åˆ«",
            "category",
            "Category",
            "ç±»å‹",
            "type",
            "Type",
            "åŒºåŸŸ",
            "region",
            "Region",
            "åŸå¸‚",
            "city",
            "City",
            "ä¿¡æ¯",
            "info",
            "Info",
            "æ•°æ®",
            "data",
            "Data",
        ]

        max_check_rows = min(20, len(df_raw))
        candidate_rows = []

        for i in range(max_check_rows):
            row = df_raw.iloc[i]
            non_null_count = row.notna().sum()
            row_text = " ".join([str(val) for val in row if pd.notna(val)])
            keyword_matches = sum(
                1 for keyword in header_keywords if keyword.lower() in row_text.lower()
            )
            score = non_null_count + keyword_matches * 2

            if score > 0:
                candidate_rows.append((i, score, non_null_count, keyword_matches))

        if not candidate_rows:
            return [0]

        candidate_rows.sort(key=lambda x: x[1], reverse=True)
        main_header_row = candidate_rows[0][0]
        header_rows = [main_header_row]

        for offset in range(1, min(4, main_header_row + 1)):
            check_row = main_header_row - offset
            if check_row >= 0:
                row = df_raw.iloc[check_row]
                if row.notna().sum() >= 2:
                    header_rows.insert(0, check_row)
                else:
                    break

        return sorted(header_rows)

    def _merge_multiple_sheets(
        self,
        sheets_data: List[Tuple[str, pd.DataFrame]],
        source_column_name: str = "æ•°æ®ç±»å‹",
    ) -> pd.DataFrame:
        """
        åˆå¹¶å¤šä¸ªsheetçš„æ•°æ®ï¼Œæ·»åŠ æ¥æºæ ‡è¯†åˆ—

        Args:
            sheets_data: [(sheet_name, dataframe), ...] åˆ—è¡¨
            source_column_name: æ¥æºåˆ—çš„åˆ—åï¼Œé»˜è®¤ä¸º"æ•°æ®ç±»å‹"

        Returns:
            åˆå¹¶åçš„DataFrame
        """
        if not sheets_data:
            raise ValueError("sheets_dataä¸èƒ½ä¸ºç©º")

        if len(sheets_data) == 1:
            # åªæœ‰ä¸€ä¸ªsheetï¼Œç›´æ¥æ·»åŠ æ¥æºåˆ—
            sheet_name, df = sheets_data[0]
            df_copy = df.copy()
            df_copy[source_column_name] = sheet_name
            return df_copy

        # å¤šä¸ªsheetçš„æƒ…å†µ
        merged_dfs = []

        # æ”¶é›†æ‰€æœ‰åˆ—åï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰
        all_columns = []
        seen_columns = set()
        for sheet_name, df in sheets_data:
            for col in df.columns:
                if col not in seen_columns:
                    all_columns.append(col)
                    seen_columns.add(col)

        logger.info(f"åˆå¹¶{len(sheets_data)}ä¸ªsheetï¼Œå…±{len(all_columns)}ä¸ªå”¯ä¸€åˆ—")

        # å¯¹æ¯ä¸ªsheetè¿›è¡Œåˆ—å¯¹é½
        for sheet_name, df in sheets_data:
            df_copy = df.copy()

            # æ·»åŠ ç¼ºå¤±çš„åˆ—ï¼ˆå¡«å……ä¸ºNoneï¼‰
            for col in all_columns:
                if col not in df_copy.columns:
                    df_copy[col] = None

            # æŒ‰ç»Ÿä¸€çš„åˆ—é¡ºåºé‡æ–°æ’åˆ—
            df_copy = df_copy[all_columns]

            # æ·»åŠ æ¥æºåˆ—
            df_copy[source_column_name] = sheet_name

            merged_dfs.append(df_copy)
            logger.debug(f"Sheet '{sheet_name}': {len(df)}è¡Œ -> å¯¹é½å{len(df_copy)}è¡Œ")

        # åˆå¹¶æ‰€æœ‰DataFrame
        merged_df = pd.concat(merged_dfs, ignore_index=True)
        logger.info(f"åˆå¹¶å®Œæˆï¼šæ€»è¡Œæ•° {len(merged_df)}")

        return merged_df

    def process_excel(
        self,
        excel_file_path: str,
        table_name: str = None,
        force_reimport: bool = False,
        original_filename: str = None,
        conv_uid: str = None,
        sheet_names: List[str] = None,
        merge_sheets: bool = False,
        source_column_name: str = "æ•°æ®ç±»å‹",
    ) -> Dict:
        """å¤„ç†Excelæ–‡ä»¶ï¼Œè‡ªåŠ¨æ³¨å†Œåˆ°æ•°æ®æº

        Args:
            excel_file_path: Excelæ–‡ä»¶è·¯å¾„
            table_name: è¡¨åï¼ˆå¯é€‰ï¼‰
            force_reimport: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥
            original_filename: åŸå§‹æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            conv_uid: ä¼šè¯IDï¼ˆå¯é€‰ï¼‰
            sheet_names: è¦å¤„ç†çš„sheetåç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†æ‰€æœ‰sheet
            merge_sheets: æ˜¯å¦åˆå¹¶å¤šä¸ªsheetï¼ˆå¦‚æœä¸ºTrueï¼Œå°†å¤šä¸ªsheetåˆå¹¶ä¸ºä¸€å¼ è¡¨ï¼‰
            source_column_name: åˆå¹¶æ—¶æ·»åŠ çš„æ¥æºåˆ—åï¼Œé»˜è®¤ä¸º"æ•°æ®ç±»å‹"
        """
        if original_filename is None:
            original_filename = Path(excel_file_path).name

        # è¯»å–Excelè·å–sheetä¿¡æ¯
        excel_file = pd.ExcelFile(excel_file_path)
        all_sheet_names = excel_file.sheet_names

        # ç¡®å®šè¦å¤„ç†çš„sheet
        if sheet_names is None:
            target_sheets = all_sheet_names
        else:
            # éªŒè¯æŒ‡å®šçš„sheetæ˜¯å¦å­˜åœ¨
            target_sheets = []
            for name in sheet_names:
                if name in all_sheet_names:
                    target_sheets.append(name)
                else:
                    logger.warning(f"Sheet '{name}' ä¸å­˜åœ¨ï¼Œè·³è¿‡")

            if not target_sheets:
                raise ValueError(f"æŒ‡å®šçš„sheetéƒ½ä¸å­˜åœ¨ã€‚å¯ç”¨çš„sheet: {all_sheet_names}")

        # ä½¿ç”¨æ–‡ä»¶çº§åˆ«çš„å“ˆå¸Œï¼ˆåŒ…å«sheetä¿¡æ¯ï¼‰
        content_hash = self.cache_manager.calculate_file_hash(
            excel_file_path, target_sheets if merge_sheets else None
        )
        logger.debug(f"æ–‡ä»¶å“ˆå¸Œ: {content_hash[:16]}... (æ–‡ä»¶: {original_filename})")

        # æ£€æŸ¥ç¼“å­˜ï¼ˆåœ¨è¯»å–Excelå’Œå¤„ç†è¡¨å¤´ä¹‹å‰ï¼‰
        if not force_reimport:
            cached_info = self.cache_manager.get_cached_info(content_hash)
            if cached_info and os.path.exists(cached_info["db_path"]):
                # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›ï¼ˆæ— éœ€è¯»å–Excelå’Œå¤„ç†è¡¨å¤´ï¼‰
                cached_schema_json = cached_info.get("data_schema_json")

                # ä¸ºäº†è¿”å›top_10_rowsï¼Œéœ€è¦ä»æ•°æ®åº“è¯»å–
                try:
                    import duckdb

                    conn = duckdb.connect(cached_info["db_path"])
                    # è·å–åˆ—å
                    columns_result = conn.execute(
                        f"DESCRIBE {cached_info['table_name']}"
                    ).fetchall()
                    columns = [col[0] for col in columns_result]

                    # è·å–å‰10è¡Œæ•°æ®
                    rows = conn.execute(
                        f"SELECT * FROM {cached_info['table_name']} LIMIT 10"
                    ).fetchall()
                    conn.close()

                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    top_10_rows = [dict(zip(columns, row)) for row in rows]
                    top_10_rows = self._convert_to_json_serializable(top_10_rows)
                except Exception as e:
                    logger.warning(f"ä»æ•°æ®åº“è¯»å–top_10_rowså¤±è´¥: {e}ï¼Œå°†è¿”å›ç©ºåˆ—è¡¨")
                    top_10_rows = []

                return {
                    "status": "cached",
                    "message": "ä½¿ç”¨ç¼“å­˜æ•°æ®",
                    "content_hash": content_hash,
                    "db_name": cached_info["db_name"],
                    "db_path": cached_info["db_path"],
                    "table_name": cached_info["table_name"],
                    "row_count": cached_info["row_count"],
                    "column_count": cached_info["column_count"],
                    "columns_info": cached_info["columns_info"],
                    "summary_prompt": cached_info["summary_prompt"],
                    "data_schema_json": cached_schema_json,
                    "top_10_rows": top_10_rows,
                    "access_count": cached_info["access_count"],
                    "last_accessed": cached_info["last_accessed"],
                    "conv_uid": conv_uid,
                }

        # æ²¡æœ‰ç¼“å­˜æˆ–å¼ºåˆ¶é‡æ–°å¯¼å…¥ï¼Œéœ€è¦å®Œæ•´å¤„ç†
        logger.info(f"ğŸ“ å¼€å§‹å¤„ç†Excelæ–‡ä»¶ï¼ˆéœ€è¦è¯†åˆ«è¡¨å¤´ï¼‰: {original_filename}")

        # å¤„ç†å¤šä¸ªsheet
        if merge_sheets and len(target_sheets) > 1:
            # åˆå¹¶å¤šä¸ªsheetçš„åœºæ™¯
            logger.info(f"ğŸ”„ åˆå¹¶ {len(target_sheets)} ä¸ªsheet...")
            sheets_data = []

            for sheet_name in target_sheets:
                logger.info(f"  å¤„ç† sheet: {sheet_name}")
                df_raw = pd.read_excel(
                    excel_file_path, sheet_name=sheet_name, header=None
                )
                df_processed = self._process_multi_level_header(
                    df_raw, excel_file_path, sheet_name
                )
                sheets_data.append((sheet_name, df_processed))

            # åˆå¹¶æ‰€æœ‰sheet
            df = self._merge_multiple_sheets(sheets_data, source_column_name)
        else:
            # åªå¤„ç†ç¬¬ä¸€ä¸ªsheetï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            target_sheet = target_sheets[0]
            logger.info(f"ğŸ“„ å¤„ç†å•ä¸ªsheet: {target_sheet}")
            df_raw = pd.read_excel(
                excel_file_path, sheet_name=target_sheet, header=None
            )
            df = self._process_multi_level_header(df_raw, excel_file_path, target_sheet)

        if table_name is None:
            base_name = Path(original_filename).stem
            base_name = "".join(
                c if c.isalnum() or c == "_" else "_" for c in base_name
            )
            if base_name and base_name[0].isdigit():
                base_name = f"tbl_{base_name}"
            if not base_name or len(base_name) < 2:
                base_name = f"excel_table_{content_hash[:8]}"
            table_name = base_name

        db_name = f"excel_{content_hash[:8]}"
        db_filename = f"{db_name}.duckdb"
        db_path = str(self.db_storage_dir / db_filename)

        df = self._remove_empty_columns(df)
        df = self._remove_duplicate_columns(df)
        df = self._format_date_columns(df)
        df = self._convert_column_types(df)  # æ™ºèƒ½ç±»å‹è½¬æ¢
        df = self._format_numeric_columns(df)  # æ ¼å¼åŒ–æ•°å€¼åˆ—ä¸ºä¸¤ä½å°æ•°

        df.columns = [
            str(col)
            .replace(" ", "")
            .replace("\u00a0", "")
            .replace("\n", "")
            .replace("\r", "")
            .replace("\t", "")
            for col in df.columns
        ]

        # æ¸…ç†åå†æ¬¡å»é‡åˆ—å,é˜²æ­¢DuckDBæŠ¥duplicate column nameé”™è¯¯
        final_columns = []
        seen_columns = {}
        for col in df.columns:
            if col in seen_columns:
                seen_columns[col] += 1
                final_columns.append(f"{col}_{seen_columns[col]}")
            else:
                seen_columns[col] = 0
                final_columns.append(col)
        df.columns = final_columns

        # ç›´æ¥ä½¿ç”¨DuckDBä¿å­˜æ•°æ®ï¼ˆè·³è¿‡SQLiteï¼‰
        import duckdb

        conn = None
        try:
            conn = duckdb.connect(db_path)
            # å°†DataFrameæ³¨å†Œä¸ºä¸´æ—¶è§†å›¾
            conn.register("temp_df", df)
            
            # è¯†åˆ«æ•°å€¼åˆ—ï¼Œå¹¶åœ¨åˆ›å»ºè¡¨æ—¶ä½¿ç”¨ ROUND å‡½æ•°ç¡®ä¿ä¿ç•™ä¸¤ä½å°æ•°
            numeric_columns = []
            for col in df.columns:
                if df[col].dtype in ["int64", "float64", "int32", "float32", "Int64"]:
                    numeric_columns.append(col)
            
            # æ„å»º SELECT è¯­å¥ï¼Œå¯¹æ•°å€¼åˆ—åº”ç”¨ ROUND å‡½æ•°
            if numeric_columns:
                select_parts = []
                for col in df.columns:
                    # ä½¿ç”¨åŒå¼•å·è½¬ä¹‰åˆ—åï¼Œé˜²æ­¢ç‰¹æ®Šå­—ç¬¦é—®é¢˜
                    col_quoted = f'"{col}"'
                    if col in numeric_columns:
                        # å¯¹æ•°å€¼åˆ—ä½¿ç”¨ ROUND å‡½æ•°ä¿ç•™ä¸¤ä½å°æ•°
                        select_parts.append(f"ROUND(CAST({col_quoted} AS DOUBLE), 2) AS {col_quoted}")
                    else:
                        select_parts.append(col_quoted)
                select_sql = ", ".join(select_parts)
                table_name_quoted = f'"{table_name}"'
                conn.execute(f"CREATE TABLE {table_name_quoted} AS SELECT {select_sql} FROM temp_df")
            else:
                # å¦‚æœæ²¡æœ‰æ•°å€¼åˆ—ï¼Œç›´æ¥åˆ›å»ºè¡¨
                table_name_quoted = f'"{table_name}"'
                conn.execute(f"CREATE TABLE {table_name_quoted} AS SELECT * FROM temp_df")
            
            # DuckDB ä¼šè‡ªåŠ¨æäº¤ï¼Œä½†æ˜¾å¼å…³é—­è¿æ¥ç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
            conn.close()
            conn = None
            logger.info(
                f"âœ… æ•°æ®å·²ä¿å­˜åˆ°DuckDB: {db_path} (è¡¨: {table_name}, è¡Œæ•°: {len(df)})"
            )
        except Exception as e:
            if conn:
                try:
                    conn.close()
                except Exception:
                    pass
            logger.error(f"æ•°æ®å†™å…¥DuckDBå¤±è´¥: {e}")
            print(f"âŒ DEBUG: ä¿å­˜å¤±è´¥: {e}")
            raise Exception(f"Excelæ•°æ®è½¬æ¢ä¸ºæ•°æ®åº“å¤±è´¥: {e}")

        # è·å–åˆ—ä¿¡æ¯
        conn = duckdb.connect(db_path)
        try:
            columns_result = conn.execute(f"DESCRIBE {table_name}").fetchall()
            columns_info = [
                {"name": col[0], "type": col[1], "dtype": str(df[col[0]].dtype)}
                for col in columns_result
            ]
        finally:
            conn.close()

        schema_understanding_json = self._generate_schema_understanding_with_llm(
            df, table_name
        )
        summary_prompt = self._format_schema_as_prompt(
            schema_understanding_json, df, table_name
        )

        self.cache_manager.save_cache_info(
            content_hash=content_hash,
            original_filename=original_filename,
            table_name=table_name,
            db_name=db_name,
            db_path=db_path,
            df=df,
            summary_prompt=summary_prompt,
            data_schema_json=schema_understanding_json,
        )

        self._register_to_dbgpt(db_name, db_path, table_name)

        top_10_rows_raw = df.head(10).values.tolist()
        top_10_rows = self._convert_to_json_serializable(top_10_rows_raw)

        return {
            "status": "imported",
            "message": "æˆåŠŸå¯¼å…¥æ–°æ•°æ®",
            "content_hash": content_hash,
            "db_name": db_name,
            "db_path": db_path,
            "table_name": table_name,
            "row_count": len(df),
            "column_count": len(df.columns),
            "columns_info": columns_info,
            "summary_prompt": summary_prompt,
            "data_schema_json": schema_understanding_json,
            "top_10_rows": top_10_rows,
            "conv_uid": conv_uid,
        }

    def _generate_schema_understanding_with_llm(
        self, df: pd.DataFrame, table_name: str
    ) -> str:
        """ä½¿ç”¨LLMç”ŸæˆSchemaç†è§£JSON"""
        er_info = self._prepare_er_info(df, table_name)
        numeric_stats = self._prepare_numeric_stats(df)
        categorical_distribution = self._prepare_categorical_distribution(df)

        prompt = self._build_schema_understanding_prompt(
            table_name=table_name,
            er_info=er_info,
            numeric_stats=numeric_stats,
            categorical_distribution=categorical_distribution,
            sample_data=df.head(3).to_dict("records"),
        )

        # è°ƒç”¨LLMç”Ÿæˆç®€åŒ–çš„Schema JSONï¼ˆåªåŒ…å«ä¸šåŠ¡ç†è§£å­—æ®µï¼‰
        simplified_json = self._call_llm_for_schema(prompt)

        # é€šè¿‡ä»£ç è¡¥å……æŠ€æœ¯æ€§å­—æ®µï¼Œç”Ÿæˆå®Œæ•´çš„Schema JSON
        enriched_json = self._enrich_schema_json(simplified_json, df, table_name)

        return enriched_json

    def _prepare_er_info(self, df: pd.DataFrame, table_name: str) -> str:
        """å‡†å¤‡ERä¿¡æ¯ï¼ˆè¡¨ç»“æ„ï¼‰"""
        er_lines = [f"è¡¨å: {table_name}"]
        er_lines.append(f"è¡Œæ•°: {len(df)}")
        er_lines.append(f"åˆ—æ•°: {len(df.columns)}")
        er_lines.append("\nå­—æ®µåˆ—è¡¨:")

        for col in df.columns:
            dtype = str(df[col].dtype)
            null_count = df[col].isnull().sum()
            null_pct = (null_count / len(df)) * 100
            er_lines.append(f"  - {col} ({dtype}, ç¼ºå¤±ç‡: {null_pct:.1f}%)")

        return "\n".join(er_lines)

    def _prepare_numeric_stats(self, df: pd.DataFrame) -> str:
        """å‡†å¤‡æ•°å€¼åˆ—çš„æè¿°ç»Ÿè®¡"""
        numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()

        if not numeric_cols:
            return "æ— æ•°å€¼åˆ—"

        stats_lines = ["æ•°å€¼åˆ—æè¿°ç»Ÿè®¡:"]
        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                stats_lines.append(f"\n  {col}:")
                stats_lines.append(f"    æœ€å°å€¼: {col_data.min():.2f}")
                stats_lines.append(f"    æœ€å¤§å€¼: {col_data.max():.2f}")
                stats_lines.append(f"    å¹³å‡å€¼: {col_data.mean():.2f}")
                stats_lines.append(f"    ä¸­ä½æ•°: {col_data.median():.2f}")
                stats_lines.append(f"    æ ‡å‡†å·®: {col_data.std():.2f}")

        return "\n".join(stats_lines)

    def _prepare_categorical_distribution(self, df: pd.DataFrame) -> str:
        """å‡†å¤‡åˆ†ç±»åˆ—çš„å”¯ä¸€å€¼åˆ†å¸ƒ"""
        categorical_cols = df.select_dtypes(
            include=["object", "category"]
        ).columns.tolist()

        if not categorical_cols:
            return "æ— åˆ†ç±»åˆ—"

        dist_lines = ["åˆ†ç±»åˆ—å”¯ä¸€å€¼åˆ†å¸ƒ:"]
        for col in categorical_cols:
            unique_vals = df[col].dropna().unique()
            unique_count = len(unique_vals)

            dist_lines.append(f"\n  {col} (å”¯ä¸€å€¼æ•°é‡: {unique_count}):")

            if unique_count <= 20:
                # æ˜¾ç¤ºæ‰€æœ‰å”¯ä¸€å€¼
                value_counts = df[col].value_counts()
                for val, count in value_counts.head(20).items():
                    dist_lines.append(
                        f"    - '{val}': {count}æ¡ ({count / len(df) * 100:.1f}%)"
                    )
            else:
                # åªæ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§çš„å€¼
                value_counts = df[col].value_counts()
                dist_lines.append("    å‰10ä¸ªæœ€å¸¸è§å€¼:")
                for val, count in value_counts.head(10).items():
                    dist_lines.append(
                        f"    - '{val}': {count}æ¡ ({count / len(df) * 100:.1f}%)"
                    )

        return "\n".join(dist_lines)

    def _build_schema_understanding_prompt(
        self,
        table_name: str,
        er_info: str,
        numeric_stats: str,
        categorical_distribution: str,
        sample_data: list,
    ) -> str:
        """æ„å»ºSchemaç†è§£Promptï¼ˆç®€åŒ–ç‰ˆï¼Œåªç”Ÿæˆå¿…è¦çš„ä¸šåŠ¡ç†è§£å­—æ®µï¼‰"""

        # è½¬æ¢sample_dataä¸­çš„ç‰¹æ®Šç±»å‹ï¼ˆå¦‚Timestampï¼‰ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼
        def convert_to_serializable(obj):
            """é€’å½’è½¬æ¢å¯¹è±¡ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼"""
            if isinstance(obj, dict):
                return {k: convert_to_serializable(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_to_serializable(item) for item in obj]
            elif pd.isna(obj):
                return None
            elif hasattr(obj, "isoformat"):  # datetime, date, time, Timestamp
                return obj.isoformat()
            elif isinstance(obj, (int, float, str, bool, type(None))):
                return obj
            else:
                return str(obj)

        serializable_sample_data = convert_to_serializable(sample_data)
        sample_data_str = json.dumps(
            serializable_sample_data, ensure_ascii=False, indent=2
        )

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æ•°æ®è¡¨çš„ç»“æ„å’Œè¯­ä¹‰ï¼Œç”ŸæˆSchemaç†è§£çš„JSONã€‚

=== æ•°æ®è¡¨ERä¿¡æ¯ ===
{er_info}

=== æ•°å€¼åˆ—æè¿°ç»Ÿè®¡ ===
{numeric_stats}

=== åˆ†ç±»åˆ—å”¯ä¸€å€¼åˆ†å¸ƒ ===
{categorical_distribution}

=== æ•°æ®ç¤ºä¾‹ï¼ˆå‰3è¡Œï¼‰ ===
{sample_data_str}

è¯·ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„JSONæ ¼å¼ï¼ŒåªåŒ…å«éœ€è¦ä¸šåŠ¡ç†è§£çš„æ ¸å¿ƒä¿¡æ¯ï¼š

1. **table_description**: è¡¨çš„æ•´ä½“æè¿°ï¼Œè¯´æ˜è¿™æ˜¯ä»€ä¹ˆæ•°æ®ï¼Œé€‚åˆåšä»€ä¹ˆåˆ†æ
2. **columns**: æ¯ä¸ªå­—æ®µçš„ä¸šåŠ¡ç†è§£ä¿¡æ¯ï¼ˆåªéœ€è¦ä»¥ä¸‹å­—æ®µï¼‰ï¼š
   - column_name: å­—æ®µåï¼ˆå¿…é¡»ä½¿ç”¨å®Œæ•´çš„å­—æ®µåï¼Œä¸èƒ½åˆ å‡ï¼‰
   - semantic_type: è¯­ä¹‰ç±»å‹ï¼ˆå¦‚ï¼šæ—¶é—´ç»´åº¦ã€åœ°åŸŸç»´åº¦ã€æ•°å€¼æŒ‡æ ‡ã€åˆ†ç±»ç»´åº¦ã€æ ‡è¯†å­—æ®µç­‰ï¼‰
   - description: å­—æ®µçš„ä¸šåŠ¡å«ä¹‰å’Œç”¨é€”æè¿°

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆåªåŒ…å«ä¸Šè¿°å­—æ®µï¼‰ï¼š

```json
{{
  "table_description": "è¡¨çš„æ•´ä½“æè¿°...",
  "columns": [
    {{
      "column_name": "å®Œæ•´çš„å­—æ®µå",
      "semantic_type": "æ—¶é—´ç»´åº¦/åœ°åŸŸç»´åº¦/æ•°å€¼æŒ‡æ ‡/åˆ†ç±»ç»´åº¦/æ ‡è¯†å­—æ®µ",
      "description": "è¯¦ç»†çš„ä¸šåŠ¡å«ä¹‰æè¿°"
    }}
  ]
}}
```

æ³¨æ„ï¼š
1. æ·±å…¥ç†è§£å­—æ®µçš„ä¸šåŠ¡å«ä¹‰ï¼Œä¸è¦åªæ˜¯ç®€å•é‡å¤å­—æ®µå
2. semantic_typeè¦å‡†ç¡®ï¼Œè¿™å¯¹åç»­åˆ†æéå¸¸é‡è¦
3. column_nameå¿…é¡»ä¸æ•°æ®è¡¨ä¸­çš„å­—æ®µåå®Œå…¨ä¸€è‡´

è¯·ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼š
"""
        return prompt

    def _extract_chunk_text(self, chunk) -> str:
        """ç»Ÿä¸€çš„chunkæ–‡æœ¬æå–æ–¹æ³•"""
        try:
            if hasattr(chunk, "text"):
                return chunk.text
            elif hasattr(chunk, "content"):
                if hasattr(chunk.content, "get_text"):
                    try:
                        return chunk.content.get_text()
                    except Exception:
                        pass
                elif isinstance(chunk.content, str):
                    return chunk.content
        except Exception as e:
            logger.debug(f"æå–chunkæ–‡æœ¬å¤±è´¥: {e}")
        return ""

    def _enrich_schema_json(
        self, simplified_json: str, df: pd.DataFrame, table_name: str
    ) -> str:
        """é€šè¿‡ä»£ç è¡¥å……æŠ€æœ¯æ€§å­—æ®µï¼Œç”Ÿæˆå®Œæ•´çš„Schema JSON"""
        try:
            schema = json.loads(simplified_json)
        except json.JSONDecodeError as e:
            logger.error(f"è§£æç®€åŒ–JSONå¤±è´¥: {e}")
            raise

        # æ„å»ºå­—æ®µåæ˜ å°„
        llm_map = {
            col.get("column_name"): col
            for col in schema.get("columns", [])
            if col.get("column_name")
        }

        # æ„å»ºå®Œæ•´çš„columnsåˆ—è¡¨
        enriched_columns = []
        for col_name in df.columns:
            col_data = df[col_name]
            dtype = str(col_data.dtype)
            llm_info = llm_map.get(col_name, {})

            col_info = {
                "column_name": col_name,
                "data_type": dtype,
                "semantic_type": llm_info.get("semantic_type", "æœªçŸ¥"),
                "description": llm_info.get("description", f"{col_name}å­—æ®µ"),
                "is_key_field": self._is_potential_key_field(col_name, col_data),
            }

            # åˆ¤æ–­å­—æ®µç±»å‹ï¼ˆä¼˜å…ˆæ ¹æ®è¯­ä¹‰ç±»å‹åˆ¤æ–­ï¼Œé¿å…æ•°æ®ç±»å‹è¯¯åˆ¤ï¼‰
            semantic_type = col_info["semantic_type"]
            is_numeric_by_dtype = dtype in ["int64", "float64", "int32", "float32"]
            is_numeric_by_semantic = "æŒ‡æ ‡" in semantic_type or "æ•°å€¼" in semantic_type

            # æ•°å€¼å­—æ®µï¼šä¼˜å…ˆæ ¹æ®è¯­ä¹‰ç±»å‹åˆ¤æ–­ï¼Œå¦‚æœè¯­ä¹‰ç±»å‹æ˜¯æ•°å€¼æŒ‡æ ‡ï¼Œå³ä½¿data_typeæ˜¯objectä¹ŸæŒ‰æ•°å€¼å¤„ç†
            if is_numeric_by_dtype or is_numeric_by_semantic:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                if dtype in ["object", "category"]:
                    try:
                        numeric_data = pd.to_numeric(col_data, errors="coerce").dropna()
                    except Exception:
                        numeric_data = col_data.dropna()
                else:
                    numeric_data = col_data.dropna()

                if len(numeric_data) > 0:
                    try:
                        min_val = numeric_data.min()
                        max_val = numeric_data.max()
                        mean_val = numeric_data.mean()
                        median_val = numeric_data.median()
                        col_info["statistics_summary"] = (
                            f"èŒƒå›´: [{min_val:.2f}, {max_val:.2f}], "
                            f"å‡å€¼: {mean_val:.2f}, ä¸­ä½æ•°: {median_val:.2f}"
                        )
                    except Exception:
                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä¸æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                        pass

            # åˆ†ç±»å­—æ®µï¼šåªåˆ—å‡ºå‡ºç°æ¬¡æ•°æœ€é«˜çš„5ä¸ªå€¼ï¼Œå¹¶åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            # æ’é™¤æ•°å€¼æŒ‡æ ‡å­—æ®µï¼ˆå³ä½¿data_typeæ˜¯objectï¼‰
            elif (
                dtype in ["object", "category"]
                and "æ—¶é—´" not in semantic_type
                and "æ—¥æœŸ" not in semantic_type
                and "æŒ‡æ ‡" not in semantic_type
                and "æ•°å€¼" not in semantic_type
            ):
                value_counts = col_data.value_counts()
                total_unique = len(col_data.dropna().unique())

                # åªå–å‡ºç°æ¬¡æ•°æœ€é«˜çš„5ä¸ªå€¼
                top_5_values = value_counts.head(5)
                col_info["unique_values_top5"] = [
                    str(v) for v in top_5_values.index.tolist()
                ]

                # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯åˆ° value_distributionï¼ˆåŒ…å«å‡ºç°æ¬¡æ•°ï¼‰
                top_5_list = [
                    f"{str(v)}({count}æ¬¡)" for v, count in top_5_values.items()
                ]
                col_info["value_distribution"] = (
                    f"å…±{total_unique}ä¸ªå”¯ä¸€å€¼ï¼Œå‡ºç°æ¬¡æ•°å‰5: {', '.join(top_5_list)}"
                )

            enriched_columns.append(col_info)

        return json.dumps(
            {
                "table_name": table_name,
                "table_description": schema.get("table_description", ""),
                "columns": enriched_columns,
            },
            ensure_ascii=False,
            indent=2,
        )

    def _call_llm_for_schema(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”ŸæˆSchema JSON"""
        try:
            import asyncio
            import logging

            from dbgpt.core import ModelMessage, ModelMessageRoleType, ModelRequest

            worker_logger = logging.getLogger(
                "dbgpt.model.cluster.worker.default_worker"
            )
            original_level = worker_logger.level

            try:
                worker_logger.setLevel(logging.ERROR)

                request_params = {
                    "messages": [
                        ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)
                    ],
                    "temperature": 0.1,
                    "max_new_tokens": 20480,
                }

                if self.model_name:
                    request_params["model"] = self.model_name

                request = ModelRequest(**request_params)

                if hasattr(self.llm_client, "generate_stream"):
                    import inspect

                    stream_response = self.llm_client.generate_stream(request)

                    full_text = ""
                    if inspect.isasyncgen(stream_response):

                        async def collect_async():
                            text = ""
                            async for chunk in stream_response:
                                chunk_text = self._extract_chunk_text(chunk)
                                if chunk_text:
                                    text = chunk_text
                            return text

                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                        try:
                            full_text = loop.run_until_complete(collect_async())
                        finally:
                            loop.close()
                    elif inspect.isgenerator(stream_response):
                        for chunk in stream_response:
                            chunk_text = self._extract_chunk_text(chunk)
                            if chunk_text:
                                full_text = chunk_text
                    else:
                        raise Exception(
                            f"Unexpected response type: {type(stream_response)}"
                        )

                    class FakeResponse:
                        def __init__(self, text):
                            self.text = text

                    response = FakeResponse(full_text)

                else:
                    raise Exception("LLMå®¢æˆ·ç«¯æ²¡æœ‰generate_streamæ–¹æ³•")

            finally:
                worker_logger.setLevel(original_level)

            if response and hasattr(response, "text") and response.text:
                text = response.text.strip()
                json_str = None

                if "```json" in text.lower():
                    start_idx = text.lower().find("```json")
                    if start_idx >= 0:
                        content_start = text.find("\n", start_idx) + 1
                        if content_start > 0:
                            end_idx = text.find("```", content_start)
                            if end_idx > content_start:
                                json_str = text[content_start:end_idx].strip()

                if not json_str and "```" in text:
                    start_idx = text.find("```")
                    content_start = text.find("\n", start_idx) + 1
                    if content_start > 0:
                        end_idx = text.find("```", content_start)
                        if end_idx > content_start:
                            json_str = text[content_start:end_idx].strip()

                if not json_str:
                    start = text.find("{")
                    end = text.rfind("}") + 1
                    if start >= 0 and end > start:
                        json_str = text[start:end].strip()

                if not json_str:
                    raise Exception("æ— æ³•æå–JSONå†…å®¹")

                try:
                    parsed = json.loads(json_str)
                    return json_str
                except json.JSONDecodeError as e:
                    logger.error(f"JSONæ ¼å¼é”™è¯¯: {e}")
                    raise
            else:
                raise Exception("LLMè¿”å›ç©ºç»“æœ")

        except Exception as e:
            logger.error(f"è°ƒç”¨LLMå¤±è´¥: {e}")
            raise

    def _format_schema_as_prompt(
        self, schema_json: str, df: pd.DataFrame, table_name: str
    ) -> str:
        """
        å°†Schema JSONæ ¼å¼åŒ–ä¸ºæ–‡æœ¬prompt
        ç”¨äºåç»­çš„queryæ”¹å†™å’ŒSQLç”Ÿæˆ
        """
        try:
            schema = json.loads(schema_json)
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›ç®€å•æ ¼å¼
            return f"æ•°æ®è¡¨: {table_name}\næ•°æ®è§„æ¨¡: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—"

        # æ„å»ºæ˜“è¯»çš„æ–‡æœ¬æ ¼å¼
        lines = []
        lines.append("=== æ•°æ®è¡¨Schemaç†è§£ ===")
        lines.append(f"è¡¨å: {schema.get('table_name', table_name)}")
        lines.append(f"è¡¨æè¿°: {schema.get('table_description', '')}")
        lines.append("")

        lines.append("=== å­—æ®µè¯¦ç»†ä¿¡æ¯ ===")
        for col in schema.get("columns", []):
            lines.append(f"\nå­—æ®µ: {col.get('column_name')}")
            lines.append(f"  ç±»å‹: {col.get('data_type')}")
            lines.append(f"  è¯­ä¹‰: {col.get('semantic_type')}")
            lines.append(f"  æè¿°: {col.get('description')}")

            if "unique_values_top5" in col:
                unique_vals = col["unique_values_top5"]
                lines.append(
                    f"  å‡ºç°æ¬¡æ•°å‰5çš„å€¼: {', '.join([str(v) for v in unique_vals])}"
                )

            if "statistics_summary" in col:
                lines.append(f"  ç»Ÿè®¡: {col['statistics_summary']}")

        return "\n".join(lines)

    def _get_sample_values(self, col_data: pd.Series, n: int = 3) -> list:
        """
        è·å–åˆ—çš„ç¤ºä¾‹å€¼
        """
        non_null_values = col_data.dropna().unique()
        if len(non_null_values) > 0:
            sample = non_null_values[:n].tolist()
            return [str(v) for v in sample]
        return []

    def _is_potential_key_field(self, col_name: str, col_data: pd.Series) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯æ½œåœ¨çš„å…³é”®å­—æ®µ
        """
        col_name_lower = col_name.lower()

        # åŸºäºåˆ—ååˆ¤æ–­
        if any(keyword in col_name_lower for keyword in ["id", "ç¼–å·", "key", "code"]):
            return True

        # åŸºäºå”¯ä¸€æ€§åˆ¤æ–­
        if len(col_data.dropna().unique()) == len(col_data.dropna()):
            return True

        return False

    def _convert_to_json_serializable(self, obj):
        """
        é€’å½’è½¬æ¢å¯¹è±¡ä¸ºå¯JSONåºåˆ—åŒ–çš„æ ¼å¼

        Args:
            obj: è¦è½¬æ¢çš„å¯¹è±¡

        Returns:
            å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡
        """
        from datetime import date, datetime

        if isinstance(obj, (datetime, date, pd.Timestamp)):
            # å¦‚æœæ˜¯æ—¥æœŸç±»å‹ï¼Œæ£€æŸ¥æ—¶é—´éƒ¨åˆ†æ˜¯å¦ä¸º00:00:00ï¼Œå¦‚æœæ˜¯åˆ™åªè¿”å›æ—¥æœŸéƒ¨åˆ†
            if isinstance(obj, pd.Timestamp):
                if obj.hour == 0 and obj.minute == 0 and obj.second == 0:
                    return obj.strftime("%Y-%m-%d")
                else:
                    return obj.isoformat()
            elif isinstance(obj, datetime):
                if obj.hour == 0 and obj.minute == 0 and obj.second == 0:
                    return obj.strftime("%Y-%m-%d")
                else:
                    return obj.isoformat()
            elif isinstance(obj, date):
                return obj.strftime("%Y-%m-%d")
            else:
                return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: self._convert_to_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._convert_to_json_serializable(item) for item in obj]
        elif pd.isna(obj):
            return None
        elif isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        else:
            return str(obj)

    def _register_to_dbgpt(self, db_name: str, db_path: str, table_name: str):
        """æ³¨å†Œåˆ° DB-GPT æ•°æ®æºç®¡ç†å™¨"""
        try:
            logger.info(
                f"SQLiteæ•°æ®åº“å·²åˆ›å»º: {db_name}, è·¯å¾„: {db_path}, è¡¨å: {table_name}"
            )
            return
        except Exception as e:
            logger.warning(f"æ³¨å†Œåˆ° DB-GPT å¤±è´¥: {e}")

    def update_summary_prompt(self, content_hash: str, summary_prompt: str):
        """
        æ›´æ–°æ•°æ®ç†è§£æç¤ºè¯

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼
            summary_prompt: æ–°çš„æ•°æ®ç†è§£æç¤ºè¯
        """
        self.cache_manager.update_summary_prompt(content_hash, summary_prompt)

    def get_excel_info(self, content_hash: str) -> Optional[Dict]:
        """
        è·å– Excel ä¿¡æ¯

        Args:
            content_hash: å†…å®¹å“ˆå¸Œå€¼

        Returns:
            Excel ä¿¡æ¯å­—å…¸
        """
        return self.cache_manager.get_cached_info(content_hash)
