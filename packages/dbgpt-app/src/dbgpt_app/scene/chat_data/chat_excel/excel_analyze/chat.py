import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Type, Union, Optional

from dbgpt import SystemApp
from dbgpt.agent.util.api_call import ApiCall
from dbgpt.configs.model_config import DATA_DIR
from dbgpt.core import (
    ModelOutput,
    ModelRequest,
    ModelMessage,
    ModelMessageRoleType,
    ChatPromptTemplate,
    SystemPromptTemplate,
    HumanPromptTemplate,
    MessagesPlaceholder,
)
from dbgpt.core.interface.file import _SCHEMA, FileStorageClient
from dbgpt.util.executor_utils import blocking_func_to_async
from dbgpt.util.json_utils import EnhancedJSONEncoder
from dbgpt.util.tracer import root_tracer, trace
from dbgpt_app.scene import BaseChat, ChatScene
from dbgpt_app.scene.base_chat import ChatParam
from dbgpt_app.scene.chat_data.chat_excel.config import ChatExcelConfig
from dbgpt_app.scene.chat_data.chat_excel.excel_learning.chat import ExcelLearning
from dbgpt_app.scene.chat_data.chat_excel.excel_reader import ExcelReader
from dbgpt_app.scene.chat_data.chat_excel.excel_schema_db import ExcelSchemaDao
from dbgpt_app.scene.chat_data.chat_excel.excel_analyze.language_detector import (
    detect_language,
)

logger = logging.getLogger(__name__)


class ChatExcel(BaseChat):
    """a Excel analyzer to analyze Excel Data"""

    chat_scene: str = ChatScene.ChatExcel.value()

    @classmethod
    def param_class(cls) -> Type[ChatExcelConfig]:
        return ChatExcelConfig

    def __init__(self, chat_param: ChatParam, system_app: SystemApp):
        """Chat Excel Module Initialization
        Args:
           - chat_param: Dict
            - chat_session_id: (str) chat session_id
            - current_user_input: (str) current user input
            - model_name:(str) llm model name
            - select_param:(str) file path
        """
        self.fs_client = FileStorageClient.get_instance(system_app)
        self.select_param = chat_param.select_param
        if not self.select_param:
            raise ValueError("Please upload the Excel document you want to talk toï¼")
        self.model_name = chat_param.model_name
        self.curr_config = chat_param.real_app_config(ChatExcelConfig)
        self.chat_param = chat_param
        self._bucket = "dbgpt_app_file"

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„DuckDBæ•°æ®åº“è·¯å¾„
        use_existing_db = False
        duckdb_path = None
        duckdb_table_name = None  # ä¿å­˜DuckDBä¸­çš„å®é™…è¡¨å

        # âœ… è°ƒè¯•ï¼šæ‰“å°select_paramçš„ç±»å‹å’Œå†…å®¹
        logger.info(f"ğŸ” select_paramç±»å‹: {type(self.select_param)}")
        logger.info(
            f"ğŸ” select_paramå†…å®¹: {self.select_param if isinstance(self.select_param, dict) else str(self.select_param)[:200]}"
        )

        # âœ… ä¿®å¤ï¼šå¦‚æœselect_paramæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè§£æä¸ºå­—å…¸
        select_param_dict = self.select_param
        if isinstance(self.select_param, str):
            try:
                import json

                select_param_dict = json.loads(self.select_param)
                logger.info(f"âœ… æˆåŠŸè§£æselect_paramå­—ç¬¦ä¸²ä¸ºå­—å…¸")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ è§£æselect_paramå¤±è´¥: {e}")
                select_param_dict = {}

        if isinstance(select_param_dict, dict):
            # å¦‚æœæœ‰db_pathï¼Œè¯´æ˜excel_auto_registerå·²ç»å¤„ç†è¿‡äº†
            duckdb_path = select_param_dict.get("db_path")
            duckdb_table_name = select_param_dict.get("table_name")  # è·å–å®é™…è¡¨å
            self._content_hash = select_param_dict.get(
                "content_hash"
            )  # ä¿å­˜ content_hash ç”¨äºæ›´æ–°é¢†åŸŸçŸ¥è¯†
            logger.info(f"ğŸ” db_path: {duckdb_path}")
            logger.info(f"ğŸ” table_name: {duckdb_table_name}")
            logger.info(
                f"ğŸ” content_hash: {self._content_hash[:16] if self._content_hash else 'None'}..."
            )

            # å¦‚æœæœ‰ content_hashï¼Œä»æ•°æ®åº“é‡æ–°åŠ è½½æœ€æ–°çš„ data_schema_json
            if self._content_hash:
                try:
                    import sqlite3
                    from pathlib import Path

                    current_file = Path(__file__)
                    project_root = (
                        current_file.parent.parent.parent.parent.parent.parent.parent.parent.parent
                    )
                    cache_dir = (
                        project_root / "packages" / "pilot" / "data" / "excel_cache"
                    )
                    meta_db_path = cache_dir / "excel_metadata.db"

                    if meta_db_path.exists():
                        conn = sqlite3.connect(str(meta_db_path))
                        cursor = conn.cursor()
                        cursor.execute(
                            """
                            SELECT data_schema_json 
                            FROM excel_metadata 
                            WHERE content_hash = ?
                        """,
                            (self._content_hash,),
                        )
                        result = cursor.fetchone()
                        conn.close()

                        if result and result[0]:
                            select_param_dict["data_schema_json"] = result[0]
                            if isinstance(self.select_param, str):
                                self.select_param = json.dumps(
                                    select_param_dict, ensure_ascii=False
                                )
                            else:
                                self.select_param = select_param_dict
                except Exception as e:
                    logger.warning(f"ä»æ•°æ®åº“é‡æ–°åŠ è½½ data_schema_json å¤±è´¥: {e}")

            if duckdb_path and os.path.exists(duckdb_path):
                use_existing_db = True
                logger.info(f"âœ… æ£€æµ‹åˆ°å·²å­˜åœ¨çš„DuckDBæ•°æ®åº“: {duckdb_path}")
                logger.info(f"   DuckDBè¡¨å: {duckdb_table_name}")
            else:
                if duckdb_path:
                    logger.warning(f"âš ï¸ db_pathå­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨: {duckdb_path}")
                else:
                    logger.warning(f"âš ï¸ select_paramä¸­æ²¡æœ‰db_pathå­—æ®µ")
        else:
            logger.warning(f"âš ï¸ select_paramä¸æ˜¯å­—å…¸ç±»å‹ï¼Œä½¿ç”¨ä¼ ç»ŸExcelå¯¼å…¥")

        file_path, file_name, database_file_path, database_file_id = self._resolve_path(
            select_param_dict,  # âœ… ä½¿ç”¨è§£æåçš„å­—å…¸
            chat_param.chat_session_id,
            self.fs_client,
            self._bucket,
            duckdb_path=duckdb_path,  # ä¼ é€’DuckDBè·¯å¾„
        )

        # å¦‚æœæœ‰DuckDBæ•°æ®åº“ï¼Œç›´æ¥ä½¿ç”¨DuckDBè¿æ¥
        if use_existing_db and duckdb_path:
            # ä½¿ç”¨DuckDBç¼“å­˜æ—¶ï¼Œç›´æ¥ä½¿ç”¨å®é™…è¡¨åï¼Œæ— éœ€åˆ›å»ºæ–°è¡¨
            self._curr_table = duckdb_table_name if duckdb_table_name else "data_analysis_table"
            self.excel_reader = self._create_reader_from_duckdb(
                chat_param.chat_session_id,
                duckdb_path,
                file_name,
                duckdb_table_name,  # ä¼ é€’DuckDBä¸­çš„å®é™…è¡¨å
            )
            logger.info(f"âœ… ä½¿ç”¨DuckDBç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨è¡¨å: {self._curr_table}")
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šä»Excelæ–‡ä»¶å¯¼å…¥åˆ°DuckDB
            self._curr_table = "data_analysis_table"
            self.excel_reader = ExcelReader(
                chat_param.chat_session_id,
                file_path,
                file_name,
                read_type="direct",
                database_name=database_file_path,
                table_name=self._curr_table,
                duckdb_extensions_dir=self.curr_config.duckdb_extensions_dir,
                force_install=self.curr_config.force_install,
            )

        self._file_name = file_name
        self._database_file_path = database_file_path
        self._database_file_id = database_file_id
        self._query_rewrite_result = None  # ä¿å­˜Queryæ”¹å†™ç»“æœ
        self._last_sql_error = None  # ä¿å­˜æœ€åä¸€æ¬¡SQLæ‰§è¡Œé”™è¯¯

        self.api_call = ApiCall()
        super().__init__(chat_param=chat_param, system_app=system_app)

    def _create_reader_from_duckdb(
        self,
        conv_uid: str,
        duckdb_path: str,
        file_name: str,
        duckdb_table_name: str = None,
    ):
        """
        ä»DuckDBæ•°æ®åº“åˆ›å»ºExcelReaderï¼ˆç›´æ¥ä½¿ç”¨DuckDBè¿æ¥ï¼‰

        Args:
            conv_uid: ä¼šè¯ID
            duckdb_path: DuckDBæ•°æ®åº“æ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
            duckdb_table_name: DuckDBä¸­çš„å®é™…è¡¨åï¼ˆå¦‚æœä¸ºNoneï¼Œä¼šå°è¯•è‡ªåŠ¨æ£€æµ‹ï¼‰
        """
        import duckdb

        # ç›´æ¥è¿æ¥DuckDBæ•°æ®åº“æ–‡ä»¶ï¼ˆåªè¯»æ¨¡å¼ï¼‰
        db_conn = duckdb.connect(database=duckdb_path, read_only=True)

        try:
            # å¦‚æœæ²¡æœ‰æä¾›è¡¨åï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
            if not duckdb_table_name:
                logger.info("æœªæä¾›DuckDBè¡¨åï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹...")
                tables_result = db_conn.execute(
                    "SELECT table_name FROM information_schema.tables WHERE table_schema = 'main'"
                ).fetchall()
                if tables_result:
                    duckdb_table_name = tables_result[0][0]
                    logger.info(f"è‡ªåŠ¨æ£€æµ‹åˆ°è¡¨å: {duckdb_table_name}")
                else:
                    raise ValueError(f"åœ¨DuckDBæ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä»»ä½•è¡¨: {duckdb_path}")

            # ç›´æ¥ä½¿ç”¨DuckDBä¸­çš„è¡¨ï¼ˆæ— éœ€å¯¼å…¥ï¼‰
            logger.info(
                f"âœ… ç›´æ¥ä½¿ç”¨DuckDBè¡¨ '{duckdb_table_name}'ï¼ˆæ— éœ€å¯¼å…¥ï¼‰"
            )

            # âœ… éªŒè¯è¡¨ç»“æ„
            try:
                # è·å–åˆ—å
                columns_info = db_conn.execute(
                    f"""
                    SELECT column_name, data_type 
                    FROM information_schema.columns 
                    WHERE table_name = '{duckdb_table_name}' AND table_schema = 'main'
                    ORDER BY ordinal_position
                """
                ).fetchall()

                column_names = [col[0] for col in columns_info]
                logger.info(f"DuckDBè¡¨ '{duckdb_table_name}' çš„åˆ—å: {column_names}")

                # è·å–è¡Œæ•°
                row_count = db_conn.execute(
                    f"SELECT COUNT(*) FROM {duckdb_table_name}"
                ).fetchone()[0]
                logger.info(f"DuckDBè¡¨ '{duckdb_table_name}' çš„è¡Œæ•°: {row_count}")

                # è·å–å‰3è¡Œæ•°æ®ç”¨äºéªŒè¯
                sample_data = db_conn.execute(
                    f"SELECT * FROM {duckdb_table_name} LIMIT 3"
                ).fetchall()
                logger.info(
                    f"DuckDBè¡¨ '{duckdb_table_name}' çš„å‰3è¡Œ: {sample_data[:2]}"
                )  # åªæ‰“å°å‰2è¡Œé¿å…æ—¥å¿—è¿‡é•¿

            except Exception as e:
                logger.error(f"éªŒè¯è¡¨ç»“æ„æ—¶å‡ºé”™: {e}")

            # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ExcelReaderå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨DuckDBè¿æ¥
            reader = object.__new__(ExcelReader)
            reader.conv_uid = conv_uid
            reader.db = db_conn
            # ä½¿ç”¨DuckDBç¼“å­˜æ—¶ï¼Œtemp_table_nameå’Œtable_nameéƒ½è®¾ç½®ä¸ºå®é™…è¡¨å
            reader.temp_table_name = duckdb_table_name  # è®¾ç½®ä¸ºå®é™…è¡¨åï¼Œä¾›ExcelLearningä½¿ç”¨
            reader.table_name = duckdb_table_name  # ç›´æ¥ä½¿ç”¨DuckDBä¸­çš„è¡¨å
            reader.excel_file_name = file_name

            return reader

        except Exception as e:
            logger.error(f"ä»DuckDBè¯»å–æ•°æ®å¤±è´¥: {e}")
            db_conn.close()
            raise

    def _resolve_path(
        self,
        file_param: Any,
        conv_uid: str,
        fs_client: FileStorageClient,
        bucket: str,
        duckdb_path: str = None,
    ) -> Union[str, str, str]:
        if isinstance(file_param, str) and os.path.isabs(file_param):
            file_path = file_param
            file_name = os.path.basename(file_param)
        else:
            if isinstance(file_param, dict):
                file_path = file_param.get("file_path", None)
                if not file_path:
                    raise ValueError("Not find file path!")
                else:
                    file_name = os.path.basename(file_path.replace(f"{conv_uid}_", ""))

            else:
                temp_obj = json.loads(file_param)
                file_path = temp_obj.get("file_path")
                if not file_path:
                    raise ValueError("Not find file path!")
                file_name = os.path.basename(file_path.replace(f"{conv_uid}_", ""))

        # å¦‚æœæœ‰DuckDBè·¯å¾„ï¼Œç›´æ¥ä½¿ç”¨å®ƒä½œä¸ºdatabase_file_path
        if duckdb_path and os.path.exists(duckdb_path):
            database_file_path = duckdb_path
            database_file_id = None
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„DuckDBæ•°æ®åº“: {duckdb_path}")
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šä½¿ç”¨DuckDB
            database_root_path = os.path.join(DATA_DIR, "_chat_excel_tmp")
            os.makedirs(database_root_path, exist_ok=True)
            database_file_path = os.path.join(
                database_root_path, f"_chat_excel_{file_name}.duckdb"
            )
            database_file_id = None

        if file_path.startswith(_SCHEMA):
            file_path, file_meta = fs_client.download_file(file_path, dest_dir=DATA_DIR)
            file_name = os.path.basename(file_path)

            if not duckdb_path:  # åªåœ¨æ²¡æœ‰DuckDBè·¯å¾„æ—¶æ‰åˆ›å»ºæ–°çš„DuckDB
                database_file_path = os.path.join(
                    database_root_path, f"_chat_excel_{file_name}.duckdb"
                )
                database_file_id = f"{file_meta.file_id}_{conv_uid}"
                db_files = fs_client.list_files(
                    bucket,
                    filters={
                        "file_id": database_file_id,
                    },
                )
                if db_files:
                    logger.info("Database file exists in file storage. Downloading...")
                    fs_client.download_file(db_files[0].uri, database_file_path)
                    logger.info(f"Database file downloaded to {database_file_path}")

        return file_path, file_name, database_file_path, database_file_id

    @trace()
    async def generate_input_values(self) -> Dict:
        # é˜²æ­¢é‡å¤æ‰§è¡Œï¼šå¦‚æœå·²ç»ç”Ÿæˆè¿‡ input_valuesï¼Œç›´æ¥è¿”å›ç¼“å­˜
        if (
            hasattr(self, "_cached_input_values")
            and self._cached_input_values is not None
        ):
            return self._cached_input_values

        # ç¡®ä¿ data_analysis_table å­˜åœ¨ï¼ˆç‰¹åˆ«æ˜¯åœ¨æœ‰å†å²æ¶ˆæ¯æ—¶ï¼Œprepare()ä¼šè¢«è·³è¿‡ï¼‰
        await self._ensure_data_analysis_table_exists()

        # ===== æ–°å¢ï¼šæ£€æµ‹ç”¨æˆ·è¾“å…¥è¯­è¨€å¹¶åŠ¨æ€é€‰æ‹© prompt =====
        user_input = self.current_user_input.last_text
        detected_language = detect_language(user_input)

        # ä¿å­˜æ£€æµ‹åˆ°çš„è¯­è¨€ï¼Œä¾› _build_model_request ä½¿ç”¨
        self._detected_language = detected_language

        # åŠ¨æ€å¯¼å…¥ prompt æ¨¡æ¿
        from dbgpt_app.scene.chat_data.chat_excel.excel_analyze.prompt import (
            get_prompt_templates_by_language,
        )

        prompt_templates = get_prompt_templates_by_language(detected_language)

        table_schema = await blocking_func_to_async(
            self._executor, self.excel_reader.get_create_table_sql, self._curr_table
        )
        # table_summary = await blocking_func_to_async(
        #     self._executor, self.excel_reader.get_summary, self._curr_table
        # )
        colunms, datas = await blocking_func_to_async(
            self._executor, self.excel_reader.get_sample_data, self._curr_table
        )

        # è·å–æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
        data_time_range = await blocking_func_to_async(
            self._executor, self._get_data_time_range, self._curr_table
        )

        # === æ–°å¢ï¼šQueryæ”¹å†™æµç¨‹ ===
        query_rewrite_info = ""
        analysis_context = ""
        relevant_columns_info = ""  # æ–°å¢ï¼šç›¸å…³åˆ—ä¿¡æ¯

        # ğŸ”§ ä¿®å¤ï¼šå¦‚æœselect_paramæ˜¯JSONå­—ç¬¦ä¸²ï¼Œå…ˆè§£æä¸ºå­—å…¸
        select_param_dict = self.select_param
        if isinstance(self.select_param, str):
            try:
                import json

                select_param_dict = json.loads(self.select_param)
                logger.info(f"æˆåŠŸå°†select_paramä»JSONå­—ç¬¦ä¸²è§£æä¸ºå­—å…¸")
            except Exception as e:
                logger.warning(f"âš ï¸ è§£æselect_param JSONå¤±è´¥: {e}")
                select_param_dict = None

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„data_schema_json
        if select_param_dict and isinstance(select_param_dict, dict):
            data_schema_json = select_param_dict.get("data_schema_json")

            if data_schema_json:
                logger.info(f"âœ… æ£€æµ‹åˆ°data_schema_jsonï¼Œå¼€å§‹Queryæ”¹å†™å’Œåˆ—æ£€ç´¢")
                try:
                    # è°ƒç”¨Queryæ”¹å†™Agent
                    from dbgpt_app.scene.chat_data.chat_excel.query_rewrite import (
                        QueryRewriteAgent,
                    )

                    # è·å–æ¨¡å‹åç§°
                    model_name = self.llm_model

                    rewrite_agent = QueryRewriteAgent(self.llm_client, model_name)

                    # è·å–å†å²å¯¹è¯ï¼ˆç”¨äºç†è§£è¿½é—®å’ŒæŒ‡ä»£ï¼‰
                    chat_history = []
                    if hasattr(self, "history_messages") and self.history_messages:
                        # æŒ‰è½®æ¬¡ç»„ç»‡å†å²æ¶ˆæ¯ï¼Œåˆå¹¶åŒä¸€è½®çš„å¤šæ¡åŠ©æ‰‹æ¶ˆæ¯
                        current_round_messages = []
                        last_role = None

                        for msg in self.history_messages:
                            if not hasattr(msg, "content"):
                                continue

                            role = getattr(msg, "role", "user")
                            content = msg.content

                            # æå–æ–‡æœ¬å†…å®¹
                            if hasattr(content, "get_text"):
                                try:
                                    content = content.get_text()
                                except:
                                    content = str(content)
                            elif isinstance(content, list):
                                # å¤„ç† MediaContent åˆ—è¡¨
                                text_parts = []
                                for item in content:
                                    if hasattr(item, "object") and hasattr(
                                        item.object, "data"
                                    ):
                                        text_parts.append(str(item.object.data))
                                    else:
                                        text_parts.append(str(item))
                                content = " ".join(text_parts)
                            else:
                                content = str(content)

                            # æ¸…ç†å†…å®¹ï¼šç§»é™¤æ•°æ®ç»“æœï¼Œåªä¿ç•™ SQL å’Œå¼•å¯¼æ–‡æœ¬
                            content = self._clean_history_content(content)

                            # å¦‚æœè§’è‰²ç›¸åŒï¼Œåˆå¹¶å†…å®¹ï¼›å¦åˆ™å¼€å§‹æ–°çš„æ¶ˆæ¯
                            if role == last_role and current_round_messages:
                                # åˆå¹¶åŒè§’è‰²çš„è¿ç»­æ¶ˆæ¯
                                current_round_messages[-1]["content"] += "\n" + content
                            else:
                                # æ–°è§’è‰²ï¼Œæ·»åŠ æ–°æ¶ˆæ¯
                                current_round_messages.append(
                                    {"role": role, "content": content}
                                )
                                last_role = role

                        chat_history = current_round_messages

                    logger.info(
                        f"ğŸ“œ ä¼ é€’å†å²å¯¹è¯ç»™Queryæ”¹å†™Agentï¼Œå…±{len(chat_history)}æ¡æ¶ˆæ¯ï¼ˆå·²åˆå¹¶åŒè½®æ¶ˆæ¯ï¼‰"
                    )

                    rewrite_result = await blocking_func_to_async(
                        self._executor,
                        rewrite_agent.rewrite_query,
                        self.current_user_input.last_text,
                        data_schema_json,
                        table_schema,
                        chat_history,  # ä¼ å…¥å†å²å¯¹è¯
                    )

                    # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
                    if rewrite_result and rewrite_result.get("rewritten_query"):
                        query_rewrite_info = f"""


ç”¨æˆ·çš„é—®é¢˜ï¼š{rewrite_result['rewritten_query']}

ç›¸å…³å­—æ®µï¼š
{self._format_relevant_columns(rewrite_result.get('relevant_columns', []))}

åˆ†æå»ºè®®ï¼š
{self._format_analysis_suggestions(rewrite_result.get('analysis_suggestions', []))}

åˆ†æé€»è¾‘ï¼š
{rewrite_result.get('analysis_logic', '')}

æ¥ä¸‹æ¥è¯·æŒ‰ç…§æ ¼å¼è¦æ±‚ç”Ÿæˆsqlè¯­å¥è¿›è¡ŒæŸ¥è¯¢ã€‚
"""
                        # ä¿å­˜æ”¹å†™ç»“æœä¾›åç»­ä½¿ç”¨
                        self._query_rewrite_result = rewrite_result

                        logger.info(f"âœ… Queryæ”¹å†™æˆåŠŸ")
                        logger.info(f"æ”¹å†™åé—®é¢˜: {rewrite_result['rewritten_query']}")

                        # æ£€æŸ¥æ˜¯å¦æœ‰æå–åˆ°çš„é¢†åŸŸçŸ¥è¯†
                        extracted_knowledge = rewrite_result.get("_extracted_knowledge")
                        if extracted_knowledge:
                            await self._save_domain_knowledge(
                                extracted_knowledge, data_schema_json
                            )

                        # === æ–°å¢ï¼šä»æ”¹å†™ç»“æœä¸­æå–ç›¸å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯ ===
                        try:
                            # è§£ædata_schema_json
                            import json

                            schema_obj = (
                                json.loads(data_schema_json)
                                if isinstance(data_schema_json, str)
                                else data_schema_json
                            )
                            all_columns = schema_obj.get("columns", [])

                            # ä»æ”¹å†™ç»“æœä¸­è·å–ç›¸å…³åˆ—å
                            relevant_col_names = [
                                col.get("column_name", "")
                                for col in rewrite_result.get("relevant_columns", [])
                            ]

                            # ä»schemaä¸­æå–è¿™äº›åˆ—çš„å®Œæ•´ä¿¡æ¯
                            relevant_columns_details = []
                            for col_name in relevant_col_names:
                                for col_info in all_columns:
                                    if col_info.get("column_name") == col_name:
                                        relevant_columns_details.append(col_info)
                                        break

                            # æ ¼å¼åŒ–ä¸ºpromptæ–‡æœ¬
                            if relevant_columns_details:
                                relevant_columns_info = (
                                    self._format_relevant_columns_for_prompt(
                                        relevant_columns_details
                                    )
                                )
                                logger.info(
                                    f"âœ… æˆåŠŸæå– {len(relevant_columns_details)} ä¸ªç›¸å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯"
                                )
                                logger.info(
                                    f"ç›¸å…³åˆ—: {[col['column_name'] for col in relevant_columns_details]}"
                                )
                            else:
                                relevant_columns_info = "æœªæ‰¾åˆ°ç›¸å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯ã€‚"
                                logger.warning(f"âš ï¸ æœªèƒ½ä»schemaä¸­æ‰¾åˆ°ç›¸å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯")

                        except Exception as col_err:
                            logger.warning(f"æå–åˆ—è¯¦ç»†ä¿¡æ¯å¤±è´¥: {col_err}")
                            relevant_columns_info = ""

                except Exception as e:
                    logger.warning(f"Queryæ”¹å†™å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜: {e}")
                    self._query_rewrite_result = None
            else:
                logger.warning(f"âš ï¸ data_schema_jsonä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œè·³è¿‡Queryæ”¹å†™å’Œåˆ—æ£€ç´¢")
        else:
            logger.warning(f"âš ï¸ select_paramä¸æ˜¯å­—å…¸æˆ–ä¸ºç©ºï¼Œè·³è¿‡Queryæ”¹å†™å’Œåˆ—æ£€ç´¢")

        # ===== ä» prompt.py å¯¼å…¥è§„åˆ™å’Œç¤ºä¾‹å— =====
        from dbgpt_app.scene.chat_data.chat_excel.excel_analyze.prompt import (
            _DUCKDB_RULES,
            _ANALYSIS_CONSTRAINTS_TEMPLATE,
            _EXAMPLES,
        )

        # æ ¼å¼åŒ–çº¦æŸæ¡ä»¶ï¼ˆå¡«å…¥table_nameï¼‰
        analysis_constraints = _ANALYSIS_CONSTRAINTS_TEMPLATE.format(
            table_name=self._curr_table, display_type=self._generate_numbered_list()
        )

        input_values = {
            "user_input": self.current_user_input.last_text,
            "table_name": self._curr_table,
            "display_type": self._generate_numbered_list(),
            # "table_summary": table_summary,
            "table_schema": table_schema,
            "data_example": json.dumps(
                datas, cls=EnhancedJSONEncoder, ensure_ascii=False
            ),
            "query_rewrite_info": query_rewrite_info,  # Queryæ”¹å†™ä¿¡æ¯
            "relevant_columns_info": relevant_columns_info,  # æ–°å¢ï¼šç›¸å…³åˆ—ä¿¡æ¯
            "data_time_range": data_time_range or "",  # å§‹ç»ˆæä¾›ï¼Œé¿å…KeyError
            # ===== ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„è§„åˆ™ã€çº¦æŸå’Œç¤ºä¾‹å— =====
            "duckdb_syntax_rules": prompt_templates["duckdb_rules"],
            "analysis_constraints": analysis_constraints,
            "examples": prompt_templates["examples"],
        }

        # ğŸ”§ ç¼“å­˜ input_valuesï¼Œé¿å…é‡å¤æ‰§è¡Œ Query æ”¹å†™
        self._cached_input_values = input_values

        return input_values

    async def _build_model_request(self) -> ModelRequest:
        """
        é‡å†™çˆ¶ç±»æ–¹æ³•ï¼ŒåŠ¨æ€æ›¿æ¢ System Prompt å’Œ User Prompt Template ä»¥æ”¯æŒå¤šè¯­è¨€
        """
        # è·å–æ£€æµ‹åˆ°çš„è¯­è¨€ï¼ˆå¦‚æœæœªæ£€æµ‹åˆ™ä½¿ç”¨é»˜è®¤å€¼ "zh"ï¼‰
        detected_language = getattr(self, "_detected_language", "zh")

        # åŠ¨æ€è·å–å¯¹åº”è¯­è¨€çš„ prompt æ¨¡æ¿
        from dbgpt_app.scene.chat_data.chat_excel.excel_analyze.prompt import (
            get_prompt_templates_by_language,
        )

        prompt_templates = get_prompt_templates_by_language(detected_language)

        # åˆ›å»ºæ–°çš„ ChatPromptTemplateï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„è¯­è¨€å¯¹åº”çš„æ¨¡æ¿
        dynamic_prompt = ChatPromptTemplate(
            messages=[
                SystemPromptTemplate.from_template(prompt_templates["system_prompt"]),
                MessagesPlaceholder(variable_name="chat_history"),
                HumanPromptTemplate.from_template(
                    prompt_templates["user_prompt_template"]
                ),
            ]
        )

        # ä¸´æ—¶æ›¿æ¢ prompt_template çš„ prompt
        original_prompt = self.prompt_template.prompt
        self.prompt_template.prompt = dynamic_prompt

        try:
            # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ„å»º ModelRequest
            model_request = await super()._build_model_request()
            return model_request
        finally:
            # æ¢å¤åŸå§‹ promptï¼ˆé˜²æ­¢å½±å“å…¶ä»–è¯·æ±‚ï¼‰
            self.prompt_template.prompt = original_prompt

    async def _save_domain_knowledge(self, knowledge: dict, current_schema_json: str):
        """
        ä¿å­˜é¢†åŸŸçŸ¥è¯†åˆ° excel_metadata.db çš„ data_schema_json ä¸­

        Args:
            knowledge: æå–åˆ°çš„çŸ¥è¯†ï¼Œæ ¼å¼ {'column_name': 'å­—æ®µå', 'knowledge': 'çŸ¥è¯†å†…å®¹'}
            current_schema_json: å½“å‰çš„ schema JSON å­—ç¬¦ä¸²
        """
        try:
            import json
            import sqlite3
            from datetime import datetime

            column_name = knowledge.get("column_name")
            knowledge_text = knowledge.get("knowledge")

            if not column_name or not knowledge_text:
                logger.warning("é¢†åŸŸçŸ¥è¯†æ ¼å¼ä¸å®Œæ•´ï¼Œè·³è¿‡ä¿å­˜")
                return

            # è§£æå½“å‰ schema
            schema_obj = (
                json.loads(current_schema_json)
                if isinstance(current_schema_json, str)
                else current_schema_json
            )
            columns = schema_obj.get("columns", [])

            # æ‰¾åˆ°å¯¹åº”çš„å­—æ®µå¹¶æ·»åŠ  domain_knowledge
            knowledge_saved = False
            for col in columns:
                if col.get("column_name") == column_name:
                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒçš„çŸ¥è¯†
                    existing_knowledge = col.get("domain_knowledge", "")
                    if knowledge_text in existing_knowledge:
                        return

                    # æ·»åŠ æˆ–è¿½åŠ çŸ¥è¯†
                    if existing_knowledge:
                        col["domain_knowledge"] = (
                            f"{existing_knowledge}\nâ€¢ {knowledge_text}"
                        )
                    else:
                        col["domain_knowledge"] = knowledge_text

                    knowledge_saved = True
                    break

            if not knowledge_saved:
                logger.warning(f"æœªæ‰¾åˆ°å­—æ®µ {column_name}ï¼Œæ— æ³•ä¿å­˜çŸ¥è¯†")
                return

            # æ›´æ–°åˆ°æ•°æ®åº“
            # è·å–æ•°æ®åº“è·¯å¾„ï¼ˆä¸ ExcelAutoRegisterService ä¿æŒä¸€è‡´ï¼‰
            # ä»å½“å‰æ–‡ä»¶å¾€ä¸Š9å±‚åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•ï¼Œç„¶åè¿›å…¥ packages/pilot/data/excel_cache
            current_file = Path(__file__)
            # chat.py -> excel_analyze -> chat_excel -> chat_data -> scene -> dbgpt_app -> src -> dbgpt-app -> packages -> é¡¹ç›®æ ¹ç›®å½•
            project_root = (
                current_file.parent.parent.parent.parent.parent.parent.parent.parent.parent
            )
            cache_dir = project_root / "packages" / "pilot" / "data" / "excel_cache"
            meta_db_path = cache_dir / "excel_metadata.db"

            if not meta_db_path.exists():
                logger.warning(f"å…ƒæ•°æ®æ•°æ®åº“ä¸å­˜åœ¨: {meta_db_path}")
                return

            # è·å–å½“å‰ä¼šè¯çš„ content_hash
            content_hash = getattr(self, "_content_hash", None)
            if not content_hash:
                logger.warning("æ— æ³•è·å– content_hashï¼Œæ— æ³•æ›´æ–°æ•°æ®åº“")
                return

            # æ›´æ–°æ•°æ®åº“
            conn = sqlite3.connect(str(meta_db_path))
            cursor = conn.cursor()

            updated_schema_json = json.dumps(schema_obj, ensure_ascii=False, indent=2)

            cursor.execute(
                """
                UPDATE excel_metadata
                SET data_schema_json = ?,
                    last_accessed = ?
                WHERE content_hash = ?
            """,
                (updated_schema_json, datetime.now().isoformat(), content_hash),
            )

            affected_rows = cursor.rowcount
            conn.commit()
            conn.close()

            # æ›´æ–°å½“å‰ä¼šè¯çš„ data_schema_jsonï¼Œä½¿æ–°çŸ¥è¯†ç«‹å³ç”Ÿæ•ˆ
            if hasattr(self, "select_param"):
                if isinstance(self.select_param, dict):
                    self.select_param["data_schema_json"] = updated_schema_json
                elif isinstance(self.select_param, str):
                    try:
                        param_dict = json.loads(self.select_param)
                        param_dict["data_schema_json"] = updated_schema_json
                        self.select_param = json.dumps(param_dict, ensure_ascii=False)
                    except Exception as e:
                        logger.warning(f"æ›´æ–° select_param å­—ç¬¦ä¸²å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"ä¿å­˜é¢†åŸŸçŸ¥è¯†å¤±è´¥: {e}", exc_info=True)

    def _clean_history_content(self, content: str) -> str:
        """
        æ¸…ç†å†å²å¯¹è¯å†…å®¹ï¼Œç§»é™¤æ•°æ®ç»“æœï¼Œåªä¿ç•™ SQL å’Œå¼•å¯¼æ–‡æœ¬

        Args:
            content: åŸå§‹å†…å®¹

        Returns:
            æ¸…ç†åçš„å†…å®¹
        """
        import re

        # ç§»é™¤ <chart-view> æ ‡ç­¾åŠå…¶å†…å®¹ï¼ˆåŒ…å«å¤§é‡æ•°æ®ï¼‰
        content = re.sub(
            r"<chart-view[^>]*>.*?</chart-view>", "", content, flags=re.DOTALL
        )

        # ç§»é™¤è¿‡é•¿çš„æ•°æ®åˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ JSON æ•°ç»„ï¼‰
        # ä¿ç•™ <api-call> ä¸­çš„ SQLï¼Œä½†ç§»é™¤æ‰§è¡Œç»“æœ
        lines = content.split("\n")
        cleaned_lines = []
        skip_data = False

        for line in lines:
            # æ£€æµ‹æ•°æ®å¼€å§‹çš„æ ‡å¿—
            if any(marker in line for marker in ["[{", '{"data":', '"rows":']):
                # å¦‚æœè¿™è¡Œå¾ˆé•¿ï¼Œå¯èƒ½æ˜¯æ•°æ®ï¼Œè·³è¿‡
                if len(line) > 200:
                    skip_data = True
                    continue

            # æ£€æµ‹æ•°æ®ç»“æŸ
            if skip_data and ("}]" in line or line.strip() == "}"):
                skip_data = False
                continue

            if not skip_data:
                cleaned_lines.append(line)

        content = "\n".join(cleaned_lines)

        # é™åˆ¶æ€»é•¿åº¦ï¼ˆä¿ç•™ SQL å’Œå¼•å¯¼æ–‡æœ¬ï¼‰
        if len(content) > 1000:
            # å°è¯•ä¿ç•™ <api-call> éƒ¨åˆ†
            api_call_match = re.search(r"<api-call>.*?</api-call>", content, re.DOTALL)
            if api_call_match:
                # ä¿ç•™å¼•å¯¼æ–‡æœ¬ + SQL
                before_api = content[: api_call_match.start()].strip()
                if len(before_api) > 200:
                    before_api = before_api[:200] + "..."
                content = before_api + "\n" + api_call_match.group(0)
            else:
                # æ²¡æœ‰ api-callï¼Œç›´æ¥æˆªæ–­
                content = content[:1000] + "..."

        return content.strip()

    def _format_relevant_columns(self, columns: List[Dict]) -> str:
        """æ ¼å¼åŒ–ç›¸å…³åˆ—ä¿¡æ¯"""
        if not columns:
            return "æœªæŒ‡å®š"

        formatted = []
        for col in columns:
            col_name = col.get("column_name", "")
            usage = col.get("usage", "")
            formatted.append(f"  â€¢ {col_name}: {usage}")

        return "\n".join(formatted)

    def _format_analysis_suggestions(self, suggestions: List[str]) -> str:
        """æ ¼å¼åŒ–åˆ†æå»ºè®®"""
        if not suggestions:
            return "æ— "

        formatted = []
        for i, suggestion in enumerate(suggestions, 1):
            formatted.append(f"  {i}. {suggestion}")

        return "\n".join(formatted)

    def _format_relevant_columns_for_prompt(self, columns: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–ç›¸å…³åˆ—çš„è¯¦ç»†ä¿¡æ¯ï¼Œç”¨äºæ³¨å…¥åˆ°prompt

        Args:
            columns: åˆ—è¯¦ç»†ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯schema_jsonä¸­çš„columnå¯¹è±¡

        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not columns:
            # è·å–è¯­è¨€
            detected_language = getattr(self, "_detected_language", "zh")
            is_english = detected_language == "en"
            return (
                "No relevant column information found."
                if is_english
                else "æœªæ‰¾åˆ°ç›¸å…³åˆ—ä¿¡æ¯ã€‚"
            )

        # è·å–è¯­è¨€å¹¶é€‰æ‹©æ ‡é¢˜
        detected_language = getattr(self, "_detected_language", "zh")
        is_english = detected_language == "en"

        header = "Key fields to focus on:" if is_english else "ä½ åº”è¯¥é‡ç‚¹å…³æ³¨çš„å­—æ®µä¸ºï¼š"
        formatted_parts = [header]

        for col in columns:
            col_name = col.get("column_name", "")
            data_type = col.get("data_type", "")
            description = col.get("description", "")
            semantic_type = col.get("semantic_type", "")
            analysis_usage = col.get("analysis_usage", [])
            domain_knowledge = col.get("domain_knowledge", "")

            col_text = f"  â€¢ {col_name}"
            if data_type:
                label = "Data type" if is_english else "æ•°æ®ç±»å‹"
                col_text += f"\n    {label}: {data_type}"
            if semantic_type:
                label = "Semantic type" if is_english else "è¯­ä¹‰ç±»å‹"
                col_text += f"\n    {label}: {semantic_type}"
            if description:
                label = "Description" if is_english else "æè¿°"
                col_text += f"\n    {label}: {description}"

            # å¦‚æœæœ‰é¢†åŸŸçŸ¥è¯†ï¼Œä¼˜å…ˆæ˜¾ç¤ºï¼ˆæ”¾åœ¨æè¿°åé¢ï¼‰
            if domain_knowledge:
                label = "**Key Knowledge**" if is_english else "**å…³é”®çŸ¥è¯†**"
                col_text += f"\n    {label}: {domain_knowledge}"

            if analysis_usage:
                label = "Analysis usage" if is_english else "åˆ†æç”¨é€”"
                col_text += f"\n    {label}: {', '.join(analysis_usage)}"

            # å¦‚æœæœ‰ç»Ÿè®¡ä¿¡æ¯ï¼Œä¹Ÿæ·»åŠ è¿›æ¥
            if "statistics_summary" in col:
                label = "Statistics" if is_english else "ç»Ÿè®¡ä¿¡æ¯"
                col_text += f"\n    {label}: {col['statistics_summary']}"

            # å¦‚æœæœ‰å”¯ä¸€å€¼ï¼Œä¹Ÿæ·»åŠ è¿›æ¥ï¼ˆé™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼‰
            if "unique_values_top5" in col:
                unique_vals = col["unique_values_top5"]
                label = "Possible values" if is_english else "å¯é€‰å€¼"
                label_partial = (
                    "Possible values (partial)" if is_english else "å¯é€‰å€¼(éƒ¨åˆ†)"
                )
                if len(unique_vals) <= 10:
                    col_text += f"\n    {label}: {', '.join(map(str, unique_vals))}"
                else:
                    col_text += f"\n    {label_partial}: {', '.join(map(str, unique_vals[:10]))}..."

            formatted_parts.append(col_text)

        return "\n\n".join(formatted_parts)

    def _get_data_time_range(self, table_name: str) -> str:
        """
        è·å–æ•°æ®çš„æ—¶é—´èŒƒå›´ï¼Œå¸®åŠ©LLMç†è§£å¯ç”¨çš„æ•°æ®å‘¨æœŸ
        """
        try:
            # æŸ¥æ‰¾å¯èƒ½çš„æ—¥æœŸåˆ—
            columns_result = self.excel_reader.db.sql(
                f"DESCRIBE {table_name}"
            ).fetchall()
            date_columns = []

            for col_info in columns_result:
                col_name = col_info[0]
                col_type = col_info[1].upper()
                if "DATE" in col_type or "TIME" in col_type:
                    date_columns.append(col_name)

            if not date_columns:
                return ""

            # å¯¹ç¬¬ä¸€ä¸ªæ—¥æœŸåˆ—è·å–æ—¶é—´èŒƒå›´
            date_col = date_columns[0]
            query = f'SELECT MIN("{date_col}") as min_date, MAX("{date_col}") as max_date FROM "{table_name}"'
            result = self.excel_reader.db.sql(query).fetchone()

            if result and result[0] and result[1]:
                min_date = result[0]
                max_date = result[1]

                # æ ¼å¼åŒ–æ—¥æœŸ
                if isinstance(min_date, str):
                    min_date = min_date[:10]  # å–å‰10ä¸ªå­—ç¬¦ YYYY-MM-DD
                else:
                    min_date = str(min_date)[:10]

                if isinstance(max_date, str):
                    max_date = max_date[:10]
                else:
                    max_date = str(max_date)[:10]

                time_range = f"\n\næ•°æ®æ—¶é—´èŒƒå›´ï¼š{min_date} è‡³ {max_date}"
                time_range += (
                    f"\nï¼ˆæ³¨æ„ï¼šè¿›è¡ŒåŒæ¯”åˆ†ææ—¶ï¼Œè¯·ç¡®ä¿SQLæŸ¥è¯¢åŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰"
                )
                return time_range

        except Exception as e:
            logger.warning(f"è·å–æ•°æ®æ—¶é—´èŒƒå›´å¤±è´¥: {e}")

        return ""

    async def prepare(self):
        logger.info(f"{self.chat_mode} prepare start!")
        if self.has_history_messages():
            return None

        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ summary_promptï¼ˆè·³è¿‡ LLM ç”Ÿæˆï¼‰
        # âœ… ä¿®å¤ï¼šè§£æselect_paramå­—ç¬¦ä¸²
        select_param_dict = self.select_param
        if isinstance(self.select_param, str):
            try:
                import json

                select_param_dict = json.loads(self.select_param)
            except:
                select_param_dict = {}

        if select_param_dict and isinstance(select_param_dict, dict):
            summary_prompt = select_param_dict.get("summary_prompt")

            if summary_prompt and isinstance(summary_prompt, str):
                logger.info(f"âœ… æ£€æµ‹åˆ°ç¼“å­˜çš„ Data Summaryï¼Œè·³è¿‡ LLM ç”Ÿæˆ")
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨DuckDBç¼“å­˜ï¼ˆè¡¨åä¸æ˜¯é»˜è®¤çš„ data_analysis_tableï¼‰
                if self._curr_table != "data_analysis_table":
                    # ä½¿ç”¨DuckDBç¼“å­˜ï¼Œè¡¨å·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º
                    logger.info(f"âœ… ä½¿ç”¨DuckDBç¼“å­˜ï¼Œè¡¨ {self._curr_table} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                else:
                    # ä¼ ç»Ÿæ–¹å¼ï¼Œéœ€è¦åˆ›å»º data_analysis_table
                    try:
                        await blocking_func_to_async(
                            self._executor, self._create_simple_data_analysis_table
                        )
                        logger.info(f"âœ… ä½¿ç”¨ç®€åŒ–æ–¹å¼åˆ›å»ºäº† data_analysis_table")
                    except Exception as e:
                        logger.warning(f"ä½¿ç”¨ç¼“å­˜åˆ›å»ºè¡¨å¤±è´¥: {e}, å°†é‡æ–°ç”Ÿæˆ")
                        # ç»§ç»­æ‰§è¡Œåç»­çš„LLMç”Ÿæˆæµç¨‹
                        pass

                # ç”Ÿæˆå¹¶ä¿å­˜ Excel åŸºæœ¬ä¿¡æ¯ï¼ˆå³ä½¿ä½¿ç”¨ç¼“å­˜ï¼‰
                await self._generate_and_save_excel_info(None)

                # ç”ŸæˆåŒ…å« Excel åŸºæœ¬ä¿¡æ¯çš„å±•ç¤ºæ¶ˆæ¯
                excel_info_message = await self._format_excel_info_message()

                # å¦‚æœæœ‰ Excel åŸºæœ¬ä¿¡æ¯ï¼Œè¿”å›å±•ç¤ºæ¶ˆæ¯
                if excel_info_message:
                    return ModelOutput(
                        error_code=0, text=excel_info_message, finish_reason="stop"
                    )

                # è¿”å›ç®€åŒ–æ¶ˆæ¯
                return ModelOutput(
                    error_code=0,
                    text="æ•°æ®åˆ†æç»“æ„å·²åŠ è½½ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰",
                    finish_reason="stop",
                )

        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™è°ƒç”¨ LLM ç”Ÿæˆ
        logger.info(f"âš ï¸ æœªæ£€æµ‹åˆ°ç¼“å­˜ï¼Œå°†è°ƒç”¨ LLM ç”Ÿæˆ Data Summary")
        chat_param = ChatParam(
            chat_session_id=self.chat_session_id,
            current_user_input="["
            + self.excel_reader.excel_file_name
            + "]"
            + " Analyzeï¼",
            select_param=self.select_param,
            chat_mode=ChatScene.ExcelLearning,
            model_name=self.model_name,
            user_name=self.chat_param.user_name,
            sys_code=self.chat_param.sys_code,
        )
        if self._chat_param.temperature is not None:
            chat_param.temperature = self._chat_param.temperature
        if self._chat_param.max_new_tokens is not None:
            chat_param.max_new_tokens = self._chat_param.max_new_tokens
        learn_chat = ExcelLearning(
            chat_param,
            system_app=self.system_app,
            parent_mode=self.chat_mode,
            excel_reader=self.excel_reader,
        )
        result = await learn_chat.nostream_call()

        if (
            os.path.exists(self._database_file_path)
            and self._database_file_id is not None
        ):
            await blocking_func_to_async(self._executor, self.excel_reader.close)
            await blocking_func_to_async(
                self._executor,
                self.fs_client.upload_file,
                self._bucket,
                self._database_file_path,
                file_id=self._database_file_id,
            )

        # ç”Ÿæˆå¹¶ä¿å­˜ Excel åŸºæœ¬ä¿¡æ¯
        await self._generate_and_save_excel_info(result)

        # ç”ŸæˆåŒ…å« Excel åŸºæœ¬ä¿¡æ¯çš„å±•ç¤ºæ¶ˆæ¯
        excel_info_message = await self._format_excel_info_message()

        # å¦‚æœæœ‰ Excel åŸºæœ¬ä¿¡æ¯ï¼Œä¿®æ”¹è¿”å›æ¶ˆæ¯
        if excel_info_message:
            return ModelOutput(
                error_code=0, text=excel_info_message, finish_reason="stop"
            )

        return result

    def _create_simple_data_analysis_table(self):
        """åˆ›å»ºç®€åŒ–ç‰ˆçš„ data_analysis_tableï¼ˆä»ç°æœ‰è¡¨å¤åˆ¶ï¼‰"""
        try:
            # æ£€æŸ¥ data_analysis_table æ˜¯å¦å·²å­˜åœ¨
            tables = self.excel_reader.db.sql("SHOW TABLES").fetchall()
            table_names = [t[0] for t in tables]

            if self._curr_table in table_names:
                logger.info(f"âœ… {self._curr_table} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return

            # ä¼˜å…ˆä½¿ç”¨ excel_reader çš„å®é™…è¡¨åï¼ˆDuckDB ç¼“å­˜åœºæ™¯ï¼‰
            source_table = None
            if hasattr(self.excel_reader, 'table_name') and self.excel_reader.table_name:
                source_table = self.excel_reader.table_name
                if source_table in table_names and source_table != self._curr_table:
                    logger.info(f"ä½¿ç”¨å®é™…è¡¨å {source_table} åˆ›å»º {self._curr_table}")
                    sql = f"CREATE TABLE {self._curr_table} AS SELECT * FROM {source_table};"
                    self.excel_reader.db.sql(sql)
                    logger.info(f"âœ… Created {self._curr_table} from {source_table}")
                    return

            # å¦‚æœæ²¡æœ‰å®é™…è¡¨åï¼Œå°è¯•ä½¿ç”¨ temp_tableï¼ˆä¼ ç»Ÿåœºæ™¯ï¼‰
            if "temp_table" in table_names:
                logger.info(f"ä½¿ç”¨ temp_table åˆ›å»º {self._curr_table}")
                sql = f"CREATE TABLE {self._curr_table} AS SELECT * FROM temp_table;"
                self.excel_reader.db.sql(sql)
                logger.info(f"âœ… Created {self._curr_table} from temp_table")
                return

            # å¦‚æœéƒ½æ²¡æœ‰ï¼ŒæŠ¥é”™
            logger.error(f"âš ï¸ æ‰¾ä¸åˆ°æºè¡¨ï¼štemp_table ä¸å­˜åœ¨ï¼Œä¸” excel_reader.table_name ({getattr(self.excel_reader, 'table_name', None)}) ä¹Ÿä¸å­˜åœ¨")
            raise ValueError(f"æ‰¾ä¸åˆ°å¯ç”¨çš„æºè¡¨æ¥åˆ›å»º {self._curr_table}")
        except Exception as e:
            logger.error(f"Failed to create {self._curr_table}: {e}")
            raise

    async def _ensure_data_analysis_table_exists(self):
        """ç¡®ä¿ data_analysis_table å­˜åœ¨ï¼ˆåœ¨ generate_input_values ä¹‹å‰è°ƒç”¨ï¼‰"""
        try:
            tables = await blocking_func_to_async(
                self._executor,
                lambda: self.excel_reader.db.sql("SHOW TABLES").fetchall(),
            )
            table_names = [t[0] for t in tables]

            if self._curr_table not in table_names:
                logger.info(f"âš ï¸ {self._curr_table} ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                await blocking_func_to_async(
                    self._executor, self._create_simple_data_analysis_table
                )
            else:
                logger.info(f"âœ… {self._curr_table} å·²å­˜åœ¨")
        except Exception as e:
            logger.error(f"ç¡®ä¿è¡¨å­˜åœ¨æ—¶å¤±è´¥: {e}")
            raise

    async def _generate_and_save_excel_info(self, learning_result: ModelOutput = None):
        """ç”Ÿæˆå¹¶ä¿å­˜ Excel åŸºæœ¬ä¿¡æ¯åˆ°æ•°æ®åº“"""
        try:
            # è·å–å‰åè¡Œæ•°æ®
            columns, top_10_rows = await blocking_func_to_async(
                self._executor,
                self.excel_reader.get_sample_data,
                self._curr_table,
                10,  # è·å–å‰10è¡Œ
            )

            # è·å–è¡Œåˆ—æ•°
            row_count, column_count = await blocking_func_to_async(
                self._executor, self._get_table_stats, self._curr_table
            )

            # è·å–æ•°æ®æè¿°ï¼ˆä» learning_result æˆ–ä»ç¼“å­˜ï¼‰
            data_description = None
            data_schema_json = None

            if learning_result and learning_result.has_text:
                # ä» learning_result ä¸­æå–æ•°æ®æè¿°
                data_description = learning_result.text
            else:
                # å°è¯•ä» select_param ä¸­è·å–ç¼“å­˜çš„æè¿°
                select_param_dict = self.select_param
                if isinstance(self.select_param, str):
                    try:
                        select_param_dict = json.loads(self.select_param)
                    except:
                        select_param_dict = {}

                if isinstance(select_param_dict, dict):
                    data_description = select_param_dict.get("summary_prompt")
                    data_schema_json = select_param_dict.get("data_schema_json")

            # ç”Ÿæˆæ¨èé—®é¢˜ï¼šä¼˜å…ˆä» data_schema_json ä¸­æå–
            suggested_questions = []
            if data_schema_json:
                try:
                    schema = json.loads(data_schema_json)
                    suggested_questions = schema.get("suggested_questions", [])
                    if suggested_questions:
                        logger.info(
                            f"âœ… ä» data_schema_json ä¸­æå–äº† {len(suggested_questions)} ä¸ªæ¨èé—®é¢˜"
                        )
                except Exception as e:
                    logger.warning(f"ä» data_schema_json æå–æ¨èé—®é¢˜å¤±è´¥: {e}")

            # å¦‚æœæ²¡æœ‰æå–åˆ°æ¨èé—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜
            if not suggested_questions:
                logger.info("data_schema_json ä¸­æ²¡æœ‰æ¨èé—®é¢˜ï¼Œä½¿ç”¨é»˜è®¤é—®é¢˜")
                suggested_questions = self._get_default_suggested_questions()

            # ä¿å­˜åˆ°æ•°æ®åº“
            schema_dao = ExcelSchemaDao()
            await blocking_func_to_async(
                self._executor,
                schema_dao.save_or_update,
                conv_uid=self.chat_session_id,
                file_name=self._file_name,
                table_name=self._curr_table,
                row_count=row_count,
                column_count=column_count,
                top_10_rows=top_10_rows,
                data_description=data_description,
                data_schema_json=data_schema_json,
                suggested_questions=suggested_questions,
                file_path=self._database_file_path,
                db_path=self._database_file_path,
                user_id=(
                    self.chat_param.user_id
                    if hasattr(self.chat_param, "user_id")
                    else None
                ),
                user_name=self.chat_param.user_name,
                sys_code=self.chat_param.sys_code,
            )

            logger.info(f"âœ… Excel åŸºæœ¬ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¹¶ä¿å­˜ Excel åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()

    def _get_table_stats(self, table_name: str) -> tuple:
        """è·å–è¡¨çš„è¡Œåˆ—æ•°"""
        try:
            # è·å–è¡Œæ•°
            row_result = self.excel_reader.db.sql(
                f"SELECT COUNT(*) FROM {table_name}"
            ).fetchone()
            row_count = row_result[0] if row_result else 0

            # è·å–åˆ—æ•°
            columns, _ = self.excel_reader.get_columns(table_name)
            column_count = len(columns) if columns else 0

            return row_count, column_count
        except Exception as e:
            logger.error(f"è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return 0, 0

    def _get_default_suggested_questions(self) -> List[str]:
        """ç”Ÿæˆé»˜è®¤æ¨èé—®é¢˜ï¼ˆå½“ data_schema_json ä¸­æ²¡æœ‰æ¨èé—®é¢˜æ—¶ä½¿ç”¨ï¼‰"""
        return [
            "æŸ¥çœ‹æ•°æ®çš„åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯",
            "åˆ†ææ•°æ®çš„åˆ†å¸ƒæƒ…å†µ",
            "æ‰¾å‡ºæ•°æ®ä¸­çš„å¼‚å¸¸å€¼",
            "åˆ†ææ•°æ®çš„è¶‹åŠ¿å˜åŒ–",
            "å¯¹æ¯”ä¸åŒç»´åº¦çš„æ•°æ®",
        ]

    async def _format_excel_info_message(self) -> str:
        """æ ¼å¼åŒ– Excel åŸºæœ¬ä¿¡æ¯å±•ç¤ºæ¶ˆæ¯"""
        try:
            schema_dao = ExcelSchemaDao()
            schema_entity = await blocking_func_to_async(
                self._executor, schema_dao.get_by_conv_uid, self.chat_session_id
            )

            if not schema_entity:
                return None

            schema_dict = schema_dao.to_dict(schema_entity)

            # æ„å»ºå±•ç¤ºæ¶ˆæ¯
            message_parts = []
            message_parts.append("## ğŸ“Š Excel æ•°æ®åŸºæœ¬ä¿¡æ¯\n\n")

            # åŸºæœ¬ä¿¡æ¯
            message_parts.append(f"**æ–‡ä»¶å**: {schema_dict['file_name']}\n")
            message_parts.append(
                f"**æ•°æ®è§„æ¨¡**: {schema_dict['row_count']} è¡Œ Ã— {schema_dict['column_count']} åˆ—\n\n"
            )

            # å‰åè¡Œæ•°æ®
            if schema_dict.get("top_10_rows"):
                message_parts.append("### ğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰\n\n")
                message_parts.append("```\n")
                # æ ¼å¼åŒ–è¡¨æ ¼æ˜¾ç¤º
                top_rows = schema_dict["top_10_rows"]
                if top_rows and len(top_rows) > 0:
                    # æ˜¾ç¤ºè¡¨å¤´
                    if len(top_rows) > 0:
                        headers = [str(item) for item in top_rows[0]]
                        message_parts.append(" | ".join(headers[:10]))  # æœ€å¤šæ˜¾ç¤º10åˆ—
                        message_parts.append("\n")
                        message_parts.append(
                            " | ".join(["---"] * min(len(headers), 10))
                        )
                        message_parts.append("\n")
                        # æ˜¾ç¤ºæ•°æ®è¡Œ
                        for row in top_rows[1:11]:  # æœ€å¤šæ˜¾ç¤º10è¡Œ
                            row_data = [
                                str(item)[:30] for item in row[:10]
                            ]  # æ¯åˆ—æœ€å¤š30å­—ç¬¦
                            message_parts.append(" | ".join(row_data))
                            message_parts.append("\n")
                message_parts.append("```\n\n")

            # æ•°æ®æè¿°
            if schema_dict.get("data_description"):
                message_parts.append("### ğŸ“ æ•°æ®æè¿°\n\n")
                message_parts.append(
                    f"{schema_dict['data_description'][:500]}...\n\n"
                )  # æœ€å¤šæ˜¾ç¤º500å­—ç¬¦

            # æ¨èé—®é¢˜
            if schema_dict.get("suggested_questions"):
                message_parts.append("### ğŸ’¡ æ¨èé—®é¢˜\n\n")
                for i, question in enumerate(schema_dict["suggested_questions"][:8], 1):
                    message_parts.append(f"{i}. {question}\n")
                message_parts.append("\n")

            message_parts.append("---\n")
            message_parts.append(
                "ğŸ’¬ æ‚¨å¯ä»¥åŸºäºä»¥ä¸Šä¿¡æ¯å¼€å§‹æ•°æ®åˆ†æï¼Œæˆ–ç›´æ¥æå‡ºæ‚¨çš„é—®é¢˜ã€‚\n"
            )

            return "".join(message_parts)
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ– Excel åŸºæœ¬ä¿¡æ¯æ¶ˆæ¯å¤±è´¥: {e}")
            return None

    def stream_plugin_call(self, text):
        with root_tracer.start_span(
            "ChatExcel.stream_plugin_call.run_display_sql", metadata={"text": text}
        ):
            result = self.api_call.display_sql_llmvis(
                text,
                self.excel_reader.get_df_by_sql_ex,
            )
            return result


    async def stream_call(self, text_output: bool = True, incremental: bool = False):
        """
        é‡å†™stream_callæ–¹æ³•ï¼Œåˆ†é˜¶æ®µæµå¼è¾“å‡ºä¸­é—´ç»“æœï¼Œæå‡ç”¨æˆ·ä½“éªŒ

        è¾“å‡ºé˜¶æ®µï¼š
        1. Queryæ”¹å†™ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        2. SQLç”Ÿæˆç»“æœ
        3. æœ€ç»ˆæ€»ç»“å’Œå›¾è¡¨

        æ”¯æŒSQLé”™è¯¯è‡ªåŠ¨ä¿®å¤ï¼šå¦‚æœSQLæ‰§è¡Œå¤±è´¥ï¼Œä¼šè‡ªåŠ¨é‡è¯•ä¸€æ¬¡
        """
        input_values = await self.generate_input_values()

        # ===== é˜¶æ®µ1ï¼šè¾“å‡ºQueryæ”¹å†™ç»“æœ =====
        if self._query_rewrite_result:
            thinking_stage1 = self._format_query_rewrite_thinking(
                self._query_rewrite_result
            )
            if thinking_stage1:
                # åŒ…è£…æˆ vis-thinking æ ¼å¼
                from dbgpt.vis.tags.vis_thinking import VisThinking

                vis_thinking_output = VisThinking().sync_display(
                    content=thinking_stage1
                )
                if text_output:
                    yield vis_thinking_output
                else:
                    # ç›´æ¥ä½œä¸º text è¾“å‡ºï¼Œå‰ç«¯ä¼šè¯†åˆ« vis-thinking æ ¼å¼
                    stage1_output = ModelOutput.build(
                        text=vis_thinking_output, error_code=0, finish_reason="continue"
                    )
                    yield stage1_output

        # è°ƒç”¨çˆ¶ç±»çš„_build_model_requestè·å–payload
        # æ³¨æ„ï¼šè¿™é‡Œä¼šå†æ¬¡è°ƒç”¨ generate_input_valuesï¼Œä½†ç”±äºå·²ç»æ‰§è¡Œè¿‡ï¼Œä¼šå¾ˆå¿«
        payload = await self._build_model_request()
        logger.info(f"payload request: \n{payload}")

        # åˆå§‹åŒ–é”™è¯¯è·Ÿè¸ª
        self._last_sql_error = None

        # ä½¿ç”¨éæµå¼è°ƒç”¨ï¼Œç›´æ¥è·å–å®Œæ•´ç»“æœï¼ˆé¿å…æµå¼è¾“å‡ºæ—¥å¿—ï¼‰
        full_output = await self.call_llm_operator(payload)

        # ===== é˜¶æ®µ2ï¼šæ‰§è¡ŒSQLå¹¶ç”Ÿæˆæœ€ç»ˆæ€»ç»“ =====
        if full_output:
            try:
                ai_response_text, view_message = await self._handle_final_output(
                    full_output, incremental=incremental
                )

                # é˜¶æ®µ3ï¼šè¾“å‡ºæœ€ç»ˆç»“æœï¼ˆè¿½åŠ åœ¨ä¹‹å‰çš„æ€è€ƒè¿‡ç¨‹åé¢ï¼‰
                # æ„å»ºå®Œæ•´çš„è¾“å‡ºï¼šæ€è€ƒè¿‡ç¨‹ + æœ€ç»ˆç»“æœ
                final_output_parts = []

                # åªè¿½åŠ é˜¶æ®µ1çš„æ€è€ƒè¿‡ç¨‹ï¼ˆé—®é¢˜ç†è§£ä¸åˆ†æï¼‰
                if self._query_rewrite_result:
                    thinking_stage1 = self._format_query_rewrite_thinking(
                        self._query_rewrite_result
                    )
                    if thinking_stage1:
                        from dbgpt.vis.tags.vis_thinking import VisThinking

                        vis_thinking_output = VisThinking().sync_display(
                            content=thinking_stage1
                        )
                        final_output_parts.append(vis_thinking_output)

                # è¿½åŠ æœ€ç»ˆç»“æœ
                final_output_parts.append(view_message)

                # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†
                complete_output = "\n\n".join(final_output_parts)

                if text_output:
                    yield complete_output
                else:
                    yield ModelOutput.build(
                        complete_output,
                        "",
                        error_code=full_output.error_code if full_output else 0,
                        finish_reason=(
                            full_output.finish_reason if full_output else "stop"
                        ),
                    )
            except Exception as e:
                logger.error(f"å¤„ç†è¾“å‡ºæ—¶å‡ºé”™: {e}")
                # SQLæ‰§è¡Œå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                error_msg = f"æ•°æ®æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"
                if text_output:
                    yield error_msg
                else:
                    yield ModelOutput.build(
                        error_msg,
                        "",
                        error_code=1,
                        finish_reason="error",
                    )
        else:
            # full_outputä¸ºNoneï¼Œè¿”å›é”™è¯¯
            error_msg = "ç”ŸæˆSQLå¤±è´¥ï¼Œè¯·é‡è¯•"
            if text_output:
                yield error_msg
            else:
                yield ModelOutput.build(
                    error_msg,
                    "",
                    error_code=1,
                    finish_reason="error",
                )

    async def _handle_final_output(
        self,
        final_output: ModelOutput,
        incremental: bool = False,
        check_error: bool = True,
    ):
        text_msg = final_output.text if final_output.has_text else ""
        view_msg = self.stream_plugin_call(text_msg)

        # âš ï¸ å…³é”®ä¿®æ”¹ï¼šå…ˆæ£€æŸ¥SQLé”™è¯¯ï¼Œå†å†³å®šæ˜¯å¦ç”Ÿæˆæ€»ç»“
        # å¦‚æœæœ‰SQLé”™è¯¯ï¼Œ_last_sql_errorä¼šåœ¨stream_plugin_callä¸­è¢«è®¾ç½®
        # check_error=Falseæ—¶è·³è¿‡æ£€æŸ¥(ç”¨äºé‡è¯•åçš„æ‰§è¡Œ)
        if check_error and self._last_sql_error:
            # æœ‰é”™è¯¯ï¼Œä¸ç”Ÿæˆæ€»ç»“ï¼Œç›´æ¥è¿”å›ï¼ˆè®©ä¸Šå±‚å¤„ç†é‡è¯•ï¼‰
            logger.warning(f"SQLæ‰§è¡Œå¤±è´¥ï¼Œè·³è¿‡æ€»ç»“ç”Ÿæˆ: {self._last_sql_error[:100]}")
            view_msg = final_output.gen_text_with_thinking(new_text=view_msg)
            # âš ï¸ åˆå¹¶ thinking å’Œ text ä½œä¸ºå®Œæ•´çš„ AI å›ç­”
            ai_full_response = self._combine_thinking_and_text(final_output, view_msg)
            return ai_full_response, view_msg

        # æ²¡æœ‰é”™è¯¯ï¼Œå°è¯•ç”Ÿæˆè‡ªç„¶è¯­è¨€æ€»ç»“
        summary_text = await self._generate_result_summary(text_msg, view_msg)

        # å¦‚æœæˆåŠŸç”Ÿæˆæ€»ç»“ï¼Œæ›¿æ¢æ‰åŸæ¥çš„å¼•å¯¼æ€§æ–‡æœ¬
        if summary_text:
            # ä»view_msgä¸­æå–æ‰€æœ‰çš„chart-view
            import re

            chart_pattern = r"(<chart-view.*?</chart-view>)"
            chart_matches = re.findall(chart_pattern, view_msg, re.DOTALL)

            if chart_matches:
                # ä¿ç•™æ‰€æœ‰chart-viewï¼Œç”¨æ¢è¡Œåˆ†éš”
                all_chart_views = "\n\n".join(chart_matches)
                # ç”¨æ€»ç»“æ›¿æ¢æ‰åŸæ¥çš„å¼•å¯¼æ€§æ–‡æœ¬
                view_msg = f"{summary_text}\n\n{all_chart_views}"
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°chart-viewï¼Œå°±åœ¨å¼€å¤´æ·»åŠ æ€»ç»“
                view_msg = f"{summary_text}\n\n{view_msg}"

        view_msg = final_output.gen_text_with_thinking(new_text=view_msg)
        # âš ï¸ åˆå¹¶ thinking å’Œ text ä½œä¸ºå®Œæ•´çš„ AI å›ç­”
        ai_full_response = self._combine_thinking_and_text(final_output, view_msg)
        return ai_full_response, view_msg

    def _combine_thinking_and_text(
        self, final_output: ModelOutput, view_msg: str
    ) -> str:
        """
        åˆå¹¶ thinking å’Œ text éƒ¨åˆ†ï¼Œä½œä¸ºå®Œæ•´çš„ AI å›ç­”

        Args:
            final_output: LLM è¾“å‡º
            view_msg: å¤„ç†åçš„è§†å›¾æ¶ˆæ¯ï¼ˆå·²åŒ…å« thinking + textï¼‰

        Returns:
            å®Œæ•´çš„ AI å›ç­”ï¼ˆthinking + textï¼‰
        """
        # å®‰å…¨åœ°è·å– thinking å±æ€§ï¼ˆæœ‰äº›æ¨¡å‹å¯èƒ½æ²¡æœ‰è¿™ä¸ªå±æ€§ï¼‰
        thinking_text = getattr(final_output, "thinking", None)

        # å¦‚æœæœ‰ thinkingï¼Œæ‹¼æ¥ thinking å’Œ text
        if thinking_text:
            text_content = final_output.text if final_output.has_text else ""
            # ç”¨æ¢è¡Œåˆ†éš” thinking å’Œ text
            return f"{thinking_text}\n{text_content}".strip()
        else:
            # æ²¡æœ‰ thinkingï¼Œç›´æ¥è¿”å› text
            return final_output.text if final_output.has_text else ""

    def _format_query_rewrite_thinking(self, rewrite_result: dict) -> str:
        """
        æ ¼å¼åŒ–Queryæ”¹å†™ç»“æœä¸ºthinkingæ ¼å¼ï¼Œç”¨äºæµå¼è¾“å‡º

        Args:
            rewrite_result: Queryæ”¹å†™ç»“æœ

        Returns:
            æ ¼å¼åŒ–åçš„thinkingæ–‡æœ¬
        """
        try:
            if not rewrite_result:
                return ""

            # è·å–æ£€æµ‹åˆ°çš„è¯­è¨€ï¼ˆé»˜è®¤ä¸ºä¸­æ–‡ï¼‰
            detected_language = getattr(self, "_detected_language", "zh")
            is_english = detected_language == "en"

            # æ ¹æ®è¯­è¨€é€‰æ‹©æ ‡é¢˜å’Œæ ‡ç­¾
            if is_english:
                title = "Question Understanding & Analysis\n\n"
                label_question = "1. Understood Question: "
                label_columns = "\n2. Relevant Fields:\n"
                label_suggestions = "\n3. Analysis Approach:\n"
                separator = ": "
            else:
                title = "é—®é¢˜ç†è§£ä¸åˆ†æ\n\n"
                label_question = "1.ç†è§£çš„é—®é¢˜ï¼š"
                label_columns = "\n2.éœ€è¦å…³æ³¨çš„å­—æ®µï¼š\n"
                label_suggestions = "\n3.åˆ†ææ€è·¯ï¼š\n"
                separator = "ï¼š"

            thinking_parts = []
            thinking_parts.append(title)

            # æ”¹å†™åçš„é—®é¢˜
            rewritten_query = rewrite_result.get("rewritten_query", "")
            if rewritten_query:
                thinking_parts.append(f"{label_question}{rewritten_query}\n")

            # ç›¸å…³å­—æ®µ
            relevant_columns = rewrite_result.get("relevant_columns", [])
            if relevant_columns:
                thinking_parts.append(label_columns)
                for col in relevant_columns[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                    col_name = col.get("column_name", "")
                    usage = col.get("usage", "")
                    if col_name:
                        thinking_parts.append(f"  â€¢ {col_name}")
                        if usage:
                            thinking_parts.append(f"{separator}{usage}")
                        thinking_parts.append("\n")

            # åˆ†æå»ºè®®
            analysis_suggestions = rewrite_result.get("analysis_suggestions", [])
            if analysis_suggestions:
                thinking_parts.append(label_suggestions)
                for i, suggestion in enumerate(
                    analysis_suggestions[:5], 1
                ):  # æœ€å¤šæ˜¾ç¤º5æ¡
                    thinking_parts.append(f"  â€¢ {suggestion}\n")

            return "".join(thinking_parts)

        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–Queryæ”¹å†™thinkingå¤±è´¥: {e}")
            return ""

    async def _generate_result_summary(self, original_text: str, view_msg: str) -> str:
        """
        æ ¹æ®SQLæ‰§è¡Œç»“æœç”Ÿæˆè‡ªç„¶è¯­è¨€æ€»ç»“

        Args:
            original_text: LLMç”Ÿæˆçš„åŸå§‹æ–‡æœ¬ï¼ˆåŒ…å«å¼•å¯¼æ€§æ–‡æœ¬ï¼‰
            view_msg: åŒ…å«æŸ¥è¯¢ç»“æœçš„å®Œæ•´æ¶ˆæ¯

        Returns:
            è‡ªç„¶è¯­è¨€æ€»ç»“ï¼Œå¦‚æœç”Ÿæˆå¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            import re
            import json

            # ä»view_msgä¸­æå–æ‰€æœ‰çš„chart-viewå†…å®¹
            chart_pattern = r'<chart-view content="([^"]+)">'
            matches = re.findall(chart_pattern, view_msg)

            if not matches:
                logger.info("æœªæ‰¾åˆ°chart-viewå†…å®¹ï¼Œè·³è¿‡æ€»ç»“ç”Ÿæˆ")
                return ""

            # æ”¶é›†æ‰€æœ‰SQLå’ŒæŸ¥è¯¢ç»“æœ
            all_sql_results = []
            import html

            for match_str in matches:
                # è§£ç HTMLå®ä½“
                content_str = html.unescape(match_str)
                content_data = json.loads(content_str)

                # è·å–SQLå’ŒæŸ¥è¯¢ç»“æœ
                sql = content_data.get("sql", "").strip()
                query_data = content_data.get("data", [])

                if query_data:
                    all_sql_results.append({"sql": sql, "result": query_data})

            if not all_sql_results:
                logger.info("æ‰€æœ‰æŸ¥è¯¢ç»“æœä¸ºç©ºï¼Œè·³è¿‡æ€»ç»“ç”Ÿæˆ")
                return ""

            # æ„å»ºæ€»ç»“æç¤ºè¯ï¼ŒåŒ…å«å†å²å¯¹è¯ã€æ‰€æœ‰SQLå’Œç»“æœ

            # 1. æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆæ¸…ç†åçš„ç‰ˆæœ¬ï¼‰
            history_context = ""
            if self.history_messages and len(self.history_messages) > 0:
                history_context = "\n=== å†å²å¯¹è¯ ===\n"
                for msg in self.history_messages[-6:]:  # åªå–æœ€è¿‘3è½®ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰
                    if not hasattr(msg, "content"):
                        continue

                    role = getattr(msg, "role", "user")
                    content = msg.content

                    # æå–æ–‡æœ¬å†…å®¹
                    if hasattr(content, "get_text"):
                        try:
                            content = content.get_text()
                        except:
                            content = str(content)
                    elif isinstance(content, list):
                        text_parts = []
                        for item in content:
                            if hasattr(item, "object") and hasattr(item.object, "data"):
                                text_parts.append(str(item.object.data))
                            else:
                                text_parts.append(str(item))
                        content = " ".join(text_parts)
                    else:
                        content = str(content)

                    # æ¸…ç†å†…å®¹ï¼šç§»é™¤æ•°æ®ç»“æœ
                    content = self._clean_history_content(content)

                    # è·å–è¯­è¨€å¹¶æ ¼å¼åŒ–è§’è‰²æ˜¾ç¤º
                    detected_language = getattr(self, "_detected_language", "zh")
                    is_english = detected_language == "en"
                    role_display = (
                        "User"
                        if role == "human"
                        else (
                            "Assistant"
                            if is_english
                            else "ç”¨æˆ·" if role == "human" else "åŠ©æ‰‹"
                        )
                    )
                    history_context += f"{role_display}: {content}\n\n"

            # è·å–æ£€æµ‹åˆ°çš„è¯­è¨€
            detected_language = getattr(self, "_detected_language", "zh")
            is_english = detected_language == "en"

            # 2. æ„å»ºSQLæ‰§è¡Œç»“æœï¼ˆå¤šè¯­è¨€ï¼‰
            sql_results_text = ""
            for i, sql_result in enumerate(all_sql_results, 1):
                sql_label = f"Executed SQL {i}" if is_english else f"æ‰§è¡Œçš„SQL {i}"
                result_label = f"Query Result {i}" if is_english else f"æŸ¥è¯¢ç»“æœ {i}"
                sql_results_text += f"\n{sql_label}ï¼š\n{sql_result['sql']}\n\n"
                sql_results_text += f"{result_label}ï¼š\n{json.dumps(sql_result['result'], ensure_ascii=False, indent=2)}\n"

            # 3. æ„å»ºå®Œæ•´çš„æ€»ç»“æç¤ºè¯ï¼ˆå¤šè¯­è¨€ï¼‰
            if is_english:
                summary_prompt = f"""{history_context}
=== User's Current Question ===
{self.current_user_input.last_text}
{sql_results_text}
**IMPORTANT - Language Requirement**:
- The user's question is in ENGLISH
- You MUST respond in ENGLISH
- Your answer MUST be in ENGLISH, not Chinese
- Based on the conversation history and all the SQL query results above, summarize and answer the user's current question in one sentence in ENGLISH.
- If the current question is a follow-up or continuation of previous topics, reflect continuity and contextual relationship in your summary.
- Use ENGLISH language style consistent with the user's question.

Answer:"""
            else:
                summary_prompt = f"""{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{self.current_user_input.last_text}
{sql_results_text}
**é‡è¦ - è¯­è¨€è¦æ±‚**ï¼š
- ç”¨æˆ·çš„é—®é¢˜æ˜¯**ä¸­æ–‡**
- ä½ å¿…é¡»ç”¨**ä¸­æ–‡**å›ç­”
- è¯·æ ¹æ®å†å²å¯¹è¯å’Œä¸Šè¿°æ‰€æœ‰SQLæŸ¥è¯¢ç»“æœï¼Œç”¨ä¸€å¥è¯æ€»ç»“å¹¶å®Œæ•´å›ç­”ç”¨æˆ·çš„å½“å‰é—®é¢˜ã€‚
- å¦‚æœå½“å‰é—®é¢˜æ˜¯è¿½é—®æˆ–å»¶ç»­ä¹‹å‰çš„è¯é¢˜ï¼Œè¯·åœ¨æ€»ç»“ä¸­ä½“ç°å‡ºè¿è´¯æ€§å’Œä¸Šä¸‹æ–‡å…³ç³»ã€‚
- è¯­è¨€é£æ ¼å’Œç”¨æˆ·é—®é¢˜ä¸€è‡´ï¼Œä½¿ç”¨**ä¸­æ–‡**ã€‚

å›ç­”ï¼š"""

            # æ„å»ºModelRequest - ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«å¿…è¦å‚æ•°
            summary_request = ModelRequest(
                model=self.llm_model,
                messages=[
                    ModelMessage(
                        role=ModelMessageRoleType.HUMAN, content=summary_prompt
                    )
                ],
                temperature=0.3,
                max_new_tokens=500,  # å¢åŠ tokenæ•°é‡ï¼Œå› ä¸ºå¯èƒ½éœ€è¦æ€»ç»“å¤šä¸ªç»“æœ
            )

            # ä½¿ç”¨llm_clientç”Ÿæˆæ€»ç»“
            if self.llm_client:
                summary_output = await self.llm_client.generate(summary_request)
                if summary_output and summary_output.text:
                    summary_text = summary_output.text.strip()
                    logger.info(f"ç”Ÿæˆç»“æœæ€»ç»“: {summary_text}")
                    return summary_text
            else:
                logger.warning("llm_clientæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“")

            return ""

        except Exception as e:
            logger.warning(f"ç”Ÿæˆç»“æœæ€»ç»“å¤±è´¥: {e}", exc_info=True)
            return ""
