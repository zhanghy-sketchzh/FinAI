"""
Queryæ”¹å†™Agent - å‚è€ƒformat_sql/backend/agents/query_rewrite_assistant.py
è´Ÿè´£æ ¹æ®æ•°æ®ç†è§£ä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç›¸å…³åˆ—å’Œåˆ†æå»ºè®®
"""
# ruff: noqa: E501

import json
import logging
from typing import AsyncIterator, Dict, List, Union

logger = logging.getLogger(__name__)


class JSONParseError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šJSONè§£æå¤±è´¥"""

    pass


class InvalidColumnError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šå­—æ®µåä¸å­˜åœ¨"""

    def __init__(self, message: str, invalid_columns: list):
        super().__init__(message)
        self.invalid_columns = invalid_columns


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€

    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬

    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€
    """
    if not text or not text.strip():
        return "en"

    import re

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_pattern = re.compile(r"[\u4e00-\u9fff]+")
    chinese_chars = chinese_pattern.findall(text)
    chinese_count = sum(len(match) for match in chinese_chars)

    # è®¡ç®—æ€»å­—ç¬¦æ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    total_chars = len(re.sub(r'[\s\.,;:!?\'"\-_()\[\]{}]', "", text))

    if total_chars == 0:
        return "en"

    # è®¡ç®—ä¸­æ–‡å æ¯”
    chinese_ratio = chinese_count / total_chars if total_chars > 0 else 0

    # å¦‚æœä¸­æ–‡å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡è¾“å…¥
    return "zh" if chinese_ratio > 0.3 else "en"


class QueryRewriteAgent:
    """
    Queryæ”¹å†™Agent - ç®€åŒ–ç‰ˆæœ¬
    åŠŸèƒ½ï¼š
    1. æ ¹æ®æ•°æ®å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„æé—®
    2. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—
    3. æä¾›åˆ†æå»ºè®®å’Œé€»è¾‘æ”¯æ’‘
    """

    def __init__(self, llm_client=None, model_name=None):
        """
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›é»˜è®¤ç»“æœï¼‰
            model_name: æ¨¡å‹åç§°
        """
        self.llm_client = llm_client
        self.model_name = model_name

    def _extract_valid_column_names(self, table_schema_json: str) -> set:
        """
        ä»schemaä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„å­—æ®µå
        
        Returns:
            æœ‰æ•ˆå­—æ®µåçš„é›†åˆ
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            if not isinstance(schema_obj, dict):
                return set()

            columns = schema_obj.get("columns", [])
            return {col.get("column_name", "") for col in columns if col.get("column_name")}
        except Exception as e:
            logger.warning(f"æå–å­—æ®µåå¤±è´¥: {e}")
            return set()

    def _simplify_schema_for_rewrite(self, table_schema_json: str) -> str:
        """
        ç²¾ç®€schemaç”¨äºqueryæ”¹å†™ï¼Œåªä¿ç•™å¿…è¦å­—æ®µä¿¡æ¯
        ç§»é™¤ suggested_questions_zhã€suggested_questions_en ç­‰ä¸å¿…è¦å­—æ®µ
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            if not isinstance(schema_obj, dict):
                return table_schema_json

            # åªä¿ç•™columnså­—æ®µï¼Œç§»é™¤å»ºè®®é—®é¢˜ç­‰
            simplified = {}
            if "columns" in schema_obj:
                # ç²¾ç®€æ¯ä¸ªåˆ—çš„ä¿¡æ¯
                simplified_columns = []
                for col in schema_obj["columns"]:
                    simplified_col = {
                        "column_name": col.get("column_name", ""),
                        "data_type": col.get("data_type", ""),
                    }
                    # ä¿ç•™å…³é”®å­—æ®µæ ‡è®°
                    if col.get("is_key_field"):
                        simplified_col["is_key_field"] = True
                    # ä¿ç•™ä¸šåŠ¡çŸ¥è¯†
                    if col.get("domain_knowledge"):
                        simplified_col["domain_knowledge"] = col["domain_knowledge"]
                    # ä¿ç•™åˆ†ç±»å­—æ®µçš„å¯é€‰å€¼ï¼ˆç²¾ç®€ç‰ˆï¼‰
                    if col.get("unique_values_top20"):
                        values = col["unique_values_top20"]
                        # æœ€å¤šä¿ç•™10ä¸ªå€¼
                        simplified_col["possible_values"] = values[:10] if len(values) > 10 else values
                    # ä¿ç•™æ•°å€¼ç»Ÿè®¡æ‘˜è¦
                    if col.get("statistics_summary"):
                        simplified_col["stats"] = col["statistics_summary"]
                    simplified_columns.append(simplified_col)
                simplified["columns"] = simplified_columns

            # ä¿ç•™è¡¨åç­‰åŸºæœ¬ä¿¡æ¯
            if "table_name" in schema_obj:
                simplified["table_name"] = schema_obj["table_name"]

            return json.dumps(simplified, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ç²¾ç®€schemaå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹schema")
            return table_schema_json if isinstance(table_schema_json, str) else json.dumps(table_schema_json, ensure_ascii=False)

    async def rewrite_query_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        æµå¼æ”¹å†™ç”¨æˆ·query
        
        å…ˆæµå¼è¾“å‡ºLLMçš„åŸå§‹è¾“å‡ºï¼ˆæ–‡æœ¬å’ŒJSONï¼‰ï¼Œç„¶åè¾“å‡ºè§£æåçš„ç»“æœ
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®è¡Œï¼Œæ ¼å¼ä¸º [(columns, data_rows)] æˆ–ç›´æ¥çš„è¡Œåˆ—è¡¨
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunk
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœï¼ˆå½“æµå¼è¾“å‡ºå®Œæˆæ—¶ï¼‰
        """
        logger.info(f"Queryæ”¹å†™ï¼ˆæµå¼ï¼‰ - åŸå§‹é—®é¢˜: {user_query}")

        # å°è¯•ä½¿ç”¨LLMæ”¹å†™
        if self.llm_client:
            try:
                logger.info("å°è¯•ä½¿ç”¨LLMè¿›è¡ŒQueryæ”¹å†™ï¼ˆæµå¼ï¼‰...")
                full_text = ""
                async for chunk in self._llm_based_rewrite_stream(
                    user_query,
                    table_schema_json,
                    table_description,
                    chat_history=chat_history,
                    sample_rows=sample_rows,
                ):
                    if isinstance(chunk, str):
                        # æµå¼è¾“å‡ºåŸå§‹æ–‡æœ¬chunk
                        full_text = chunk
                        yield chunk
                    elif isinstance(chunk, dict):
                        # è¿”å›æœ€ç»ˆè§£æç»“æœ
                        logger.info(f"âœ… LLMæ”¹å†™æˆåŠŸï¼ˆæµå¼ï¼‰ - æ”¹å†™ç»“æœ: {chunk.get('rewritten_query', '')}")
                        yield chunk
                        return
            except JSONParseError as e:
                logger.error(f"âŒ LLMæ”¹å†™å¤±è´¥ï¼ˆJSONè§£æé”™è¯¯ï¼‰: {e}")
                logger.warning("âš ï¸ ä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
        else:
            logger.info("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™")

        # Fallback: åŸºäºè§„åˆ™çš„ç®€å•æ”¹å†™
        result = self._rule_based_rewrite(user_query, table_schema_json)
        logger.info(f"è§„åˆ™æ”¹å†™ - æ”¹å†™ç»“æœ: {result['rewritten_query']}")
        yield result

    
    
    def _rule_based_rewrite(self, user_query: str, table_schema_json: str) -> Dict:
        """
        åŸºäºè§„åˆ™çš„Queryæ”¹å†™ï¼ˆä¸ä¾èµ–LLMï¼Œå¿«é€Ÿå¯é ï¼‰
        """
        try:
            # è§„åˆ™æ”¹å†™ä¸è¿›è¡Œç›¸å…³æ€§åˆ¤æ–­ï¼Œç»Ÿä¸€è¿”å› is_relevant=True
            # ç›¸å…³æ€§åˆ¤æ–­äº¤ç»™LLMå¤„ç†ï¼ˆæ›´å‡†ç¡®ï¼‰
            
            # è§£æschema JSON
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            # è·å–åˆ—ä¿¡æ¯
            schema_data = (
                schema_obj.get("columns", []) if isinstance(schema_obj, dict) else []
            )

            relevant_columns = []
            analysis_suggestions = []

            # åˆ†æç”¨æˆ·queryä¸­æåˆ°çš„å…³é”®è¯
            query_lower = user_query.lower()

            # æ£€æµ‹æ—¶é—´ç›¸å…³çš„åˆ†æ
            if any(
                keyword in query_lower for keyword in ["åŒæ¯”", "ç¯æ¯”", "yoy", "mom"]
            ):
                # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
                date_cols = [
                    col
                    for col in schema_data
                    if any(
                        kw in col.get("column_name", "").lower()
                        for kw in ["date", "æ—¥æœŸ", "time", "æ—¶é—´"]
                    )
                ]
                if date_cols:
                    relevant_columns.append(
                        {
                            "column_name": date_cols[0]["column_name"],
                            "usage": "æ—¶é—´ç­›é€‰å’Œåˆ†ç»„æ¡ä»¶ï¼Œéœ€è¦åŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®",
                        }
                    )

                analysis_suggestions.append(
                    "åŒæ¯”åˆ†æéœ€è¦å»å¹´åŒæœŸæ•°æ®ï¼Œç¡®ä¿WHEREæ¡ä»¶åŒ…å«è‡³å°‘ä¸¤å¹´çš„æ•°æ®"
                )
                analysis_suggestions.append(
                    "ç¯æ¯”åˆ†æéœ€è¦ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®ï¼Œä½¿ç”¨LAGçª—å£å‡½æ•°"
                )
                analysis_suggestions.append(
                    "æ—¶é—´èŒƒå›´ç¤ºä¾‹ï¼šWHERE è®¢å•æ—¥æœŸ >= '2021-01-01' AND è®¢å•æ—¥æœŸ < '2023-01-01'"
                )

            # æ£€æµ‹åœ°åŸŸåˆ†æ
            if any(
                keyword in user_query
                for keyword in [
                    "åŒºåŸŸ",
                    "åœ°åŒº",
                    "ååŒ—",
                    "åä¸œ",
                    "åå—",
                    "ä¸œåŒ—",
                    "è¥¿åŒ—",
                    "è¥¿å—",
                    "ä¸­å—",
                ]
            ):
                region_cols = [
                    col for col in schema_data if "åŒºåŸŸ" in col.get("column_name", "")
                ]
                if region_cols:
                    relevant_columns.append(
                        {
                            "column_name": region_cols[0]["column_name"],
                            "usage": "åœ°åŸŸç­›é€‰æˆ–åˆ†ç»„æ¡ä»¶",
                        }
                    )
                    analysis_suggestions.append("ä½¿ç”¨'åŒºåŸŸ'å­—æ®µè¿›è¡Œç­›é€‰æˆ–åˆ†ç»„")

            # æ£€æµ‹æŒ‡æ ‡åˆ†æ
            metric_keywords = {
                "åˆ©æ¶¦": "profit",
                "é”€å”®é¢": "sales",
                "é”€é‡": "quantity",
                "æ•°é‡": "quantity",
            }

            for cn_keyword, en_keyword in metric_keywords.items():
                if cn_keyword in user_query:
                    metric_cols = [
                        col
                        for col in schema_data
                        if cn_keyword in col.get("column_name", "")
                    ]
                    if metric_cols:
                        relevant_columns.append(
                            {
                                "column_name": metric_cols[0]["column_name"],
                                "usage": "èšåˆæŒ‡æ ‡ï¼Œéœ€è¦è¿›è¡ŒSUM/AVGç­‰èšåˆè¿ç®—",
                            }
                        )
                        analysis_suggestions.append(
                            f"å¯¹'{cn_keyword}'å­—æ®µè¿›è¡Œèšåˆè®¡ç®—ï¼ˆSUMæ±‚å’Œï¼‰"
                        )

            # æ„å»ºæ”¹å†™åçš„query
            rewritten_query = self._enhance_query(
                user_query, relevant_columns, analysis_suggestions
            )

            # æ„å»ºåˆ†æé€»è¾‘
            analysis_logic = self._build_analysis_logic(user_query, relevant_columns)

            return {
                "original_query": user_query,
                "is_relevant": True,
                "rewritten_query": rewritten_query,
                "relevant_columns": relevant_columns,
                "analysis_suggestions": analysis_suggestions,
                "analysis_logic": analysis_logic,
            }

        except Exception as e:
            logger.error(f"è§„åˆ™æ”¹å†™å¤±è´¥: {e}", exc_info=True)
            return self._default_result(user_query)

    def _enhance_query(
        self, user_query: str, relevant_columns: List[Dict], suggestions: List[str]
    ) -> str:
        """
        å¢å¼ºç”¨æˆ·query
        """
        if not relevant_columns:
            return user_query

        # æ·»åŠ æ˜ç¡®çš„å­—æ®µå¼•ç”¨
        col_names = [col["column_name"] for col in relevant_columns]
        enhanced = user_query

        # å¦‚æœæ˜¯ç®€çŸ­çš„queryï¼Œæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
        if len(user_query) < 20:
            enhanced = f"åŸºäºæ•°æ®è¡¨ï¼Œä½¿ç”¨å­—æ®µã€{', '.join(col_names)}ã€‘æ¥{user_query}"

        return enhanced

    def _build_analysis_logic(
        self, user_query: str, relevant_columns: List[Dict]
    ) -> str:
        """
        æ„å»ºåˆ†æé€»è¾‘
        """
        logic_parts = []

        # ç­›é€‰æ¡ä»¶
        filter_cols = [
            col for col in relevant_columns if "ç­›é€‰" in col.get("usage", "")
        ]
        if filter_cols:
            logic_parts.append(
                f"1. ç­›é€‰æ¡ä»¶ï¼š{', '.join([c['column_name'] for c in filter_cols])}"
            )

        # åˆ†ç»„ç»´åº¦
        group_cols = [col for col in relevant_columns if "åˆ†ç»„" in col.get("usage", "")]
        if group_cols:
            logic_parts.append(
                f"2. åˆ†ç»„ç»´åº¦ï¼š{', '.join([c['column_name'] for c in group_cols])}"
            )

        # èšåˆæŒ‡æ ‡
        agg_cols = [col for col in relevant_columns if "èšåˆ" in col.get("usage", "")]
        if agg_cols:
            logic_parts.append(
                f"3. èšåˆæŒ‡æ ‡ï¼š{', '.join([c['column_name'] for c in agg_cols])}"
            )

        if logic_parts:
            return "\n".join(logic_parts)
        else:
            return "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ ‡å‡†çš„æ•°æ®æŸ¥è¯¢å’Œåˆ†æ"

    def _format_sample_rows(self, sample_rows: list) -> str:
        """
        æ ¼å¼åŒ–ä»å¤–éƒ¨ä¼ å…¥çš„æ ·æœ¬æ•°æ®ï¼ˆåˆ—ååªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        
        Args:
            sample_rows: æ ¼å¼ä¸º (columns, data_rows) çš„å…ƒç»„ï¼Œ
                        å…¶ä¸­ columns æ˜¯åˆ—ååˆ—è¡¨ï¼Œdata_rows æ˜¯æ•°æ®è¡Œåˆ—è¡¨
        """
        try:
            if not sample_rows:
                return ""
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ (columns, data_rows) æ ¼å¼
            if isinstance(sample_rows, tuple) and len(sample_rows) == 2:
                columns, data_rows = sample_rows
                if not data_rows or not columns:
                    return ""
                
                # å…ˆæ˜¾ç¤ºåˆ—å
                lines = [f"åˆ—å: {json.dumps(list(columns), ensure_ascii=False)}"]
                
                # åªå–å‰2è¡Œï¼Œåªæ˜¾ç¤ºå€¼
                for i, row in enumerate(data_rows[:2], 1):
                    values = []
                    for v in row:
                        if v is None or (isinstance(v, float) and str(v) == 'nan'):
                            values.append(None)
                        elif hasattr(v, 'strftime'):
                            values.append(v.strftime("%Y-%m-%d"))
                        elif isinstance(v, (int, float, bool)):
                            values.append(v)
                        else:
                            values.append(str(v))
                    lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                return "\n".join(lines)
            
            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼ˆç›´æ¥çš„è¡Œåˆ—è¡¨ï¼Œæ¯è¡Œæ˜¯dictï¼‰ï¼Œæå–å…±ç”¨åˆ—å
            if isinstance(sample_rows, list) and len(sample_rows) > 0:
                first_row = sample_rows[0]
                if isinstance(first_row, dict):
                    columns = list(first_row.keys())
                    lines = [f"åˆ—å: {json.dumps(columns, ensure_ascii=False)}"]
                    for i, row in enumerate(sample_rows[:2], 1):
                        values = [row.get(col) for col in columns]
                        lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                    return "\n".join(lines)
                else:
                    # édictæ ¼å¼ï¼Œç›´æ¥è¾“å‡º
                    lines = []
                    for i, row in enumerate(sample_rows[:2], 1):
                        lines.append(f"è¡Œ{i}: {str(row)}")
                    return "\n".join(lines)
            
            return ""
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
            return ""

    def _extract_sample_rows(self, table_schema_json: str) -> str:
        """
        ä»schemaä¸­æå–æ ·æœ¬æ•°æ®å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆåˆ—ååªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            sample_rows = schema_obj.get("sample_rows", [])
            if not sample_rows:
                return ""

            # æå–åˆ—åï¼ˆä»ç¬¬ä¸€è¡Œçš„keysï¼‰
            first_row = sample_rows[0]
            if isinstance(first_row, dict):
                columns = list(first_row.keys())
                lines = [f"åˆ—å: {json.dumps(columns, ensure_ascii=False)}"]
                for i, row in enumerate(sample_rows[:2], 1):
                    values = [row.get(col) for col in columns]
                    lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                return "\n".join(lines)
            else:
                # édictæ ¼å¼
                lines = []
                for i, row in enumerate(sample_rows[:2], 1):
                    lines.append(f"è¡Œ{i}: {json.dumps(row, ensure_ascii=False)}")
                return "\n".join(lines)
        except Exception as e:
            logger.warning(f"æå–æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
            return ""

    def _build_rewrite_prompt(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
    ) -> str:
        """
        æ„å»ºæ”¹å†™prompt
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®ï¼Œæ ¼å¼ä¸º (columns, data_rows) æˆ–ç›´æ¥ä»å‚æ•°ä¼ å…¥
        """
        # ç²¾ç®€schemaï¼Œç§»é™¤å»ºè®®é—®é¢˜ç­‰ä¸å¿…è¦ä¿¡æ¯
        simplified_schema = self._simplify_schema_for_rewrite(table_schema_json)
        
        # æå–æ ·æœ¬æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ sample_rowsï¼Œå…¶æ¬¡ä» schema ä¸­æå–
        sample_rows_str = ""
        if sample_rows:
            sample_rows_str = self._format_sample_rows(sample_rows)
        if not sample_rows_str:
            sample_rows_str = self._extract_sample_rows(table_schema_json)
        
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n=== å†å²å¯¹è¯ä¸Šä¸‹æ–‡ ===\n"
            # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰ï¼Œé¿å…promptè¿‡é•¿
            recent_history = (
                chat_history[-6:] if len(chat_history) > 6 else chat_history
            )

            for msg in recent_history:
                role = msg.get("role", "user") if isinstance(msg, dict) else "user"
                content = (
                    str(msg.get("content", "")) if isinstance(msg, dict) else str(msg)
                )

                # æ ¹æ®å®é™…è§’è‰²æ˜¾ç¤º
                if "human" in role.lower() or "user" in role.lower():
                    role_display = "ç”¨æˆ·"
                elif "ai" in role.lower() or "assistant" in role.lower():
                    role_display = "åŠ©æ‰‹"
                else:
                    role_display = role

                history_context += f"\n{role_display}: {content}\n"

        # æ£€æµ‹ç”¨æˆ·è¾“å…¥è¯­è¨€
        user_language = detect_language(user_query)
        logger.info(f"ğŸŒ Queryæ”¹å†™ - æ£€æµ‹åˆ°ç”¨æˆ·è¾“å…¥è¯­è¨€: {user_language}")

        # æ ¹æ®è¯­è¨€é€‰æ‹©prompt
        if user_language == "zh":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆ†æé—®é¢˜ï¼Œä½ éœ€è¦ï¼š
1. æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå›ç­”ï¼Œå……åˆ†ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜
2. æ ¹æ®æ•°æ®è¡¨çš„å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„é—®é¢˜
3. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—ï¼ˆåŒ…æ‹¬ç­›é€‰æ¡ä»¶åˆ—ã€åˆ†ç»„ç»´åº¦åˆ—ã€èšåˆæŒ‡æ ‡åˆ—ï¼‰
4. æä¾›3-5æ¡åˆ†æå»ºè®®ï¼Œè¯´æ˜å¦‚ä½•åˆ†æè¿™ä¸ªé—®é¢˜
5. ç»™å‡ºæ¸…æ™°çš„åˆ†æé€»è¾‘
6. å¦‚æœç”¨æˆ·åœ¨å¯¹è¯æˆ–å½“å‰å†å²é—®ç­”ä¸Šä¸‹æ–‡ä¸­çº æ­£æˆ–è¡¥å……äº†å­—æ®µçš„ä½¿ç”¨æ–¹æ³•ã€ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰å…³é”®çŸ¥è¯†ï¼Œè¯·æå–å¹¶è®°å½•ä½œä¸ºdomain_knowledgeå­—æ®µ

=== æ•°æ®è¡¨å­—æ®µä¿¡æ¯ ===
{simplified_schema}

**æ³¨æ„**ï¼šå­—æ®µä¿¡æ¯ä¸­å¯èƒ½åŒ…å« `domain_knowledge` å­—æ®µï¼Œè¿™æ˜¯ä¹‹å‰ä»ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ åˆ°çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä½¿ç”¨ã€‚

=== çœŸå®æ•°æ®æ ·æœ¬ï¼ˆ2è¡Œï¼‰ ===
{sample_rows_str if sample_rows_str else "æš‚æ— æ ·æœ¬æ•°æ®"}

=== æ•°æ®è¡¨æè¿° ===
{table_description}

{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{user_query}

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "is_relevant": true,  // å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸æ•°æ®è¡¨åˆ†æç›¸å…³ã€‚å¦‚æœæ˜¯é—²èŠï¼ˆå¦‚"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"ã€"ä½ åƒé¥­äº†å—"ï¼‰åˆ™ä¸ºfalse
  "rewritten_query": "æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼Œæ˜ç¡®æŒ‡å‡ºéœ€è¦åˆ†æçš„ç»´åº¦å’ŒæŒ‡æ ‡",
  "relevant_columns": [
    {{
      "column_name": "åˆ—å",
      "usage": "ç”¨é€”è¯´æ˜ï¼ˆå¦‚ï¼šç­›é€‰æ¡ä»¶/åˆ†ç»„ç»´åº¦/èšåˆæŒ‡æ ‡ï¼‰"
    }}
  ],
  "analysis_suggestions": [
    "å»ºè®®1ï¼šå…·ä½“çš„åˆ†ææ­¥éª¤æˆ–æ³¨æ„äº‹é¡¹",
    "å»ºè®®2ï¼š...",
    "å»ºè®®3ï¼š..."
    ...
  ],
  "analysis_logic": "åˆ†æé€»è¾‘çš„è¯¦ç»†è¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š1) éœ€è¦ç­›é€‰å“ªäº›æ•°æ® 2) æŒ‰ä»€ä¹ˆç»´åº¦åˆ†ç»„ 3) è®¡ç®—å“ªäº›æŒ‡æ ‡ 4) å¦‚ä½•æ’åºæˆ–å¯¹æ¯”",
  "domain_knowledge": {{
    "column_name": "å­—æ®µåï¼ˆå¦‚æœç”¨æˆ·çº æ­£æˆ–è¡¥å……äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•ï¼‰",
    "knowledge": "ç”¨æˆ·è¡¥å……çš„ä¸šåŠ¡çŸ¥è¯†æˆ–æ•°æ®å¤„ç†æŠ€å·§ï¼ˆä¾‹å¦‚ï¼š'è¯¥å­—æ®µæ ¼å¼ä¸ºH1,H2ï¼Œé€—å·å‰æ˜¯H1ç»©æ•ˆï¼Œé€—å·åæ˜¯H2ç»©æ•ˆï¼Œéœ€è¦ç”¨SPLIT_PARTå‡½æ•°åˆ†å‰²'ï¼‰"
  }}
}}


**å…³äº is_relevant çš„è¯´æ˜**ï¼š
- åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸å½“å‰æ•°æ®è¡¨çš„åˆ†æç›¸å…³
- å¦‚æœæ˜¯æ•°æ®åˆ†æé—®é¢˜ï¼ˆå¦‚"é”€å”®é¢æ˜¯å¤šå°‘"ã€"åˆ©æ¶¦æ’å"ã€"åŒæ¯”å¢é•¿"ç­‰ï¼‰ï¼Œè®¾ä¸º true
- å¦‚æœæ˜¯é—²èŠæˆ–ä¸æ•°æ®è¡¨æ— å…³çš„é—®é¢˜ï¼ˆå¦‚"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"ã€"ä½ åƒé¥­äº†å—"ã€"è®²ä¸ªç¬‘è¯"ç­‰ï¼‰ï¼Œè®¾ä¸º false
- å½“ is_relevant ä¸º false æ—¶ï¼Œå…¶ä»–å­—æ®µå¯ä»¥ç®€åŒ–æˆ–çœç•¥

**å…³äº domain_knowledge çš„è¯´æ˜**ï¼š
- åªæœ‰å½“ç”¨æˆ·æ˜ç¡®çº æ­£ã€è¡¥å……æˆ–è¯´æ˜äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•æ—¶æ‰éœ€è¦å¡«å†™
- å¦‚æœç”¨æˆ·åªæ˜¯æ™®é€šæé—®ï¼Œä¸éœ€è¦å¡«å†™æ­¤å­—æ®µï¼ˆå¯ä»¥çœç•¥æˆ–è®¾ä¸º nullï¼‰
- çŸ¥è¯†åº”è¯¥æ˜¯å¯å¤ç”¨çš„ã€å¯¹æœªæ¥åˆ†ææœ‰å¸®åŠ©çš„å…³é”®ä¿¡æ¯,æ¯”å¦‚ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰,è¿™éƒ¨åˆ†çŸ¥è¯†ä¼šä½œä¸ºé¢†åŸŸçŸ¥è¯†ä¿å­˜åˆ°æ•°æ®åº“ä¸­,ç”¨äºåç»­çš„åˆ†æå’Œæ¨ç†,å¯ä»¥å¤ç”¨è¿™äº›çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœä¸Šé¢å·²ç»è®°å½•äº†é¢†åŸŸçŸ¥è¯†ï¼Œè¯·ä¸è¦é‡å¤è®°å½•ã€‚

**é‡è¦ - è¯­è¨€è¦æ±‚**ï¼š
- ç”¨æˆ·çš„é—®é¢˜æ˜¯**ä¸­æ–‡**
- ä½ å¿…é¡»ç”¨**ä¸­æ–‡**å›å¤JSONä¸­çš„æ‰€æœ‰å­—æ®µ
- "rewritten_query"ã€"usage"ã€"analysis_suggestions"ã€"analysis_logic" ç­‰å­—æ®µå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**
- å³ä½¿è¡¨å­—æ®µåæ˜¯ä¸­è‹±æ–‡æ··åˆï¼Œä½ çš„æè¿°å’Œåˆ†æä¹Ÿå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**

**é‡è¦ - å­—ç¬¦ä¸²/å­—æ®µç²¾ç¡®åŒ¹é…è¦æ±‚**ï¼š
- åœ¨æ”¹å†™é—®é¢˜æ—¶ï¼Œå¦‚æœç”¨æˆ·æåˆ°äº†å…·ä½“çš„éƒ¨é—¨åç§°ã€åˆ†ç±»å€¼ç­‰å­—ç¬¦ä¸²ï¼Œå¿…é¡»ä¿æŒå®Œå…¨ä¸€è‡´
- å¦‚æœç”¨æˆ·é—®é¢˜ä¸­åŒ…å«å…·ä½“çš„å­—ç¬¦ä¸²å€¼ï¼Œåœ¨"rewritten_query"ä¸­å¿…é¡»ä¿æŒåŸæ ·ï¼Œä¸èƒ½ä¿®æ”¹

**ä¸¥æ ¼è¦æ±‚ - å­—æ®µåå¿…é¡»æ¥è‡ªæä¾›çš„åˆ—è¡¨**ï¼š
- relevant_columns ä¸­çš„ column_name å¿…é¡»**ä¸¥æ ¼ä»ä¸Šé¢æä¾›çš„å­—æ®µä¿¡æ¯ä¸­é€‰æ‹©**
- **ç¦æ­¢æ¨æµ‹ã€åˆ›é€ æˆ–ç¼–é€ ä»»ä½•ä¸åœ¨å­—æ®µåˆ—è¡¨ä¸­çš„å­—æ®µå**
- å¦‚æœæ‰¾ä¸åˆ°å®Œå…¨åŒ¹é…çš„å­—æ®µï¼Œå®å¯ä¸å¡«å†™è¯¥å­—æ®µï¼Œä¹Ÿä¸è¦ç¼–é€ ç±»ä¼¼çš„å­—æ®µå

ç°åœ¨è¯·ç»“åˆå†å²ä¸Šä¸‹æ–‡åŠç”¨æˆ·å½“å‰é—®é¢˜ï¼Œåˆ†æç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜å¹¶ç”¨ä¸­æ–‡è¾“å‡ºJSONï¼š
"""
        else:
            prompt = f"""You are a data analysis expert. The user has asked a data analysis question. You need to:
1. Understand the user's real intent based on historical questions and answers, and enhance the current question
2. Enhance the user's question based on the data table field information
3. Clearly identify the columns that may be used (including filter condition columns, grouping dimension columns, and aggregation indicator columns)
4. Provide 3-5 analysis suggestions explaining how to analyze this question
5. Give a clear analysis logic
6. If the user has corrected or supplemented field usage methods, business rules, data processing techniques, or other key knowledge in the conversation or current historical Q&A context, extract and record it as the domain_knowledge field

=== Data Table Field Information ===
{simplified_schema}

**Note**: The field information may contain a `domain_knowledge` field, which is business knowledge learned from previous user conversations. Please prioritize using this.

=== Sample Data (2 rows) ===
{sample_rows_str if sample_rows_str else "No sample data available"}

=== Data Table Description ===
{table_description}

{history_context}
=== User's Current Question ===
{user_query}

=== Output Format (JSON) ===
Please strictly follow the following JSON format:
{{
  "is_relevant": true,  // Boolean value indicating whether the user's question is related to data table analysis. If it's small talk (e.g., "How's the weather today", "Did you eat"), set to false
  "rewritten_query": "The enhanced complete question, clearly indicating the dimensions and indicators to be analyzed",
  "relevant_columns": [
    {{
      "column_name": "Column name",
      "usage": "Usage description (e.g., filter condition/grouping dimension/aggregation indicator)"
    }}
  ],
  "analysis_suggestions": [
    "Suggestion 1: Specific analysis steps or considerations",
    "Suggestion 2: ...",
    "Suggestion 3: ..."
    ...
  ],
  "analysis_logic": "Detailed explanation of analysis logic, including: 1) Which data to filter 2) What dimension to group by 3) Which indicators to calculate 4) How to sort or compare",
  "domain_knowledge": {{
    "column_name": "Field name (if the user has corrected or supplemented the usage method of a field)",
    "knowledge": "Business knowledge or data processing techniques supplemented by the user (e.g., 'This field format is H1,H2, before the comma is H1 performance, after the comma is H2 performance, need to use SPLIT_PART function to split')"
  }}
}}



**About is_relevant**:
- Determine whether the user's question is related to the analysis of the current data table
- If it's a data analysis question (e.g., "What is the sales amount", "Profit ranking", "Year-over-year growth"), set to true
- If it's small talk or unrelated to the data table (e.g., "How's the weather today", "Did you eat", "Tell me a joke"), set to false
- When is_relevant is false, other fields can be simplified or omitted

**About domain_knowledge**:
- Only fill in when the user explicitly corrects, supplements, or explains the usage method of a field
- If the user is just asking a normal question, this field is not required (can be omitted or set to null)
- Knowledge should be reusable and helpful for future analysis, such as business rules, data processing techniques, etc. This knowledge will be saved to the database as domain knowledge for subsequent analysis and reasoning, and can be reused to answer user questions. If domain knowledge has already been recorded above, please do not repeat it.

**IMPORTANT - Language Requirement**:
- The user's question is in ENGLISH
- You MUST respond in ENGLISH for ALL fields in the JSON output
- The "rewritten_query", "usage", "analysis_suggestions", and "analysis_logic" fields MUST be in ENGLISH
- Even though the table field names are in Chinese, your descriptions and analysis MUST be in ENGLISH

**IMPORTANT - String Exact Matching Requirement**:
- When rewriting the question, if the user mentions specific department names, category values, or other strings, they must be kept exactly as they are
- If the user's question contains specific string values, they must be kept unchanged in "rewritten_query"

**STRICT REQUIREMENT - Column names must come from the provided list**:
- The column_name in relevant_columns MUST be **strictly selected from the field information provided above**
- **DO NOT guess, create, or fabricate any column names that are not in the field list**
- If you cannot find an exact match, leave it out rather than inventing a similar column name

Now please combine the historical context and the user's current question, analyze the user's real intent, enhance the current question and output JSON IN ENGLISH:
"""
        return prompt

    async def _llm_based_rewrite_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
        invalid_columns_hint: list = None,
        retry_count: int = 0,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæµå¼Queryæ”¹å†™
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®è¡Œ
            invalid_columns_hint: ä¸Šæ¬¡è¿”å›çš„æ— æ•ˆå­—æ®µååˆ—è¡¨ï¼Œç”¨äºæç¤ºLLMé‡æ–°ç”Ÿæˆ
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunkï¼ˆç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬ï¼‰
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœ
        """
        import inspect

        from dbgpt.core import (
            ModelMessage,
            ModelMessageRoleType,
            ModelRequest,
            ModelRequestContext,
        )

        MAX_RETRY = 2  # æœ€å¤šé‡è¯•2æ¬¡

        # æå–æœ‰æ•ˆå­—æ®µåç”¨äºæ ¡éªŒ
        valid_column_names = self._extract_valid_column_names(table_schema_json)
        logger.debug(f"æœ‰æ•ˆå­—æ®µåæ•°é‡: {len(valid_column_names)}")

        # æ£€æµ‹ç”¨æˆ·è¯­è¨€
        user_language = detect_language(user_query)

        # æ„å»ºprompt
        prompt = self._build_rewrite_prompt(
            user_query, table_schema_json, table_description, 
            chat_history=chat_history, sample_rows=sample_rows
        )
        
        # å¦‚æœæœ‰æ— æ•ˆå­—æ®µæç¤ºï¼Œè¿½åŠ åˆ°promptä¸­ï¼ˆæ ¹æ®è¯­è¨€é€‰æ‹©ï¼‰
        if invalid_columns_hint:
            if user_language == "zh":
                correction_hint = f"""

**é”™è¯¯çº æ­£**ï¼š
ä½ ä¸Šæ¬¡è¿”å›çš„ä»¥ä¸‹å­—æ®µååœ¨æ•°æ®è¡¨ä¸­ä¸å­˜åœ¨ï¼Œè¯·å‹¿ä½¿ç”¨ï¼š
{invalid_columns_hint}

è¯·ä»”ç»†æ£€æŸ¥ä¸Šé¢æä¾›çš„å­—æ®µåˆ—è¡¨ï¼Œåªä½¿ç”¨å®é™…å­˜åœ¨çš„å­—æ®µåé‡æ–°ç”Ÿæˆã€‚å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„å­—æ®µï¼Œè¯·åœ¨analysis_suggestionsä¸­è¯´æ˜è¯¥åˆ†æå¯èƒ½æ— æ³•å®Œæˆã€‚
"""
            else:
                correction_hint = f"""

**Error Correction**:
The following column names you returned do not exist in the data table, DO NOT use them:
{invalid_columns_hint}

Please carefully check the field list provided above and regenerate using only column names that actually exist. If you cannot find the corresponding field, please explain in analysis_suggestions that this analysis may not be possible.
"""
            prompt += correction_hint
            logger.info(f"ğŸ”„ é‡è¯•ç¬¬{retry_count}æ¬¡ï¼Œæ·»åŠ æ— æ•ˆå­—æ®µæç¤º: {invalid_columns_hint}")
        
        logger.debug(f"ğŸ” query_rewrite_agent prompt: {prompt[:200]}...")
        
        # è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
        request_params = {
            "messages": [ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)],
            "max_new_tokens": 2000,
            "context": ModelRequestContext(stream=True),
        }

        # å¦‚æœæœ‰model_nameï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if self.model_name:
            request_params["model"] = self.model_name

        request = ModelRequest(**request_params)

        # è·å–æµå¼å“åº”
        stream_response = self.llm_client.generate_stream(request)

        full_text = ""
        if inspect.isasyncgen(stream_response):
            async for chunk in stream_response:
                # å®‰å…¨åœ°è·å–æ–‡æœ¬å†…å®¹
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            # å¯èƒ½åªæœ‰ thinking å†…å®¹ï¼Œç»§ç»­ç­‰å¾… text å†…å®¹
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        elif inspect.isgenerator(stream_response):
            for chunk in stream_response:
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        else:
            raise Exception(f"Unexpected response type: {type(stream_response)}")

        # æµå¼è¾“å‡ºå®Œæˆåï¼Œè§£æç»“æœå¹¶è¿”å›ï¼ˆå¸¦å­—æ®µåæ ¡éªŒï¼‰
        if full_text:
            try:
                result = self._parse_rewrite_result(
                    full_text, user_query, valid_column_names=valid_column_names
                )
                yield result
            except InvalidColumnError as e:
                # å­—æ®µåæ— æ•ˆï¼Œå°è¯•é‡è¯•
                if retry_count < MAX_RETRY:
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆå­—æ®µåï¼Œè§¦å‘é‡è¯• ({retry_count + 1}/{MAX_RETRY})")
                    # é€’å½’è°ƒç”¨ï¼Œä¼ å…¥æ— æ•ˆå­—æ®µæç¤º
                    async for chunk in self._llm_based_rewrite_stream(
                        user_query,
                        table_schema_json,
                        table_description,
                        chat_history=chat_history,
                        sample_rows=sample_rows,
                        invalid_columns_hint=e.invalid_columns,
                        retry_count=retry_count + 1,
                    ):
                        yield chunk
                else:
                    logger.error(f"âŒ é‡è¯•{MAX_RETRY}æ¬¡åä»æœ‰æ— æ•ˆå­—æ®µï¼Œæ”¾å¼ƒé‡è¯•")
                    raise JSONParseError(str(e))
            except JSONParseError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                raise

    def _parse_rewrite_result(
        self, llm_output: str, original_query: str, valid_column_names: set = None
    ) -> Dict:
        """
        è§£æLLMè¾“å‡ºçš„JSONç»“æœ

        å¦‚æœè§£æå¤±è´¥æˆ–å­—æ®µåä¸å­˜åœ¨ï¼ŒæŠ›å‡º JSONParseError å¼‚å¸¸ä»¥è§¦å‘é‡è¯•æœºåˆ¶
        
        Args:
            llm_output: LLMè¾“å‡ºçš„æ–‡æœ¬
            original_query: åŸå§‹ç”¨æˆ·é—®é¢˜
            valid_column_names: æœ‰æ•ˆçš„å­—æ®µåé›†åˆï¼Œç”¨äºæ ¡éªŒ
        """
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = llm_output.find("{")
            end_idx = llm_output.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
                if "is_relevant" not in result:
                    logger.warning("JSONç¼ºå°‘ 'is_relevant' å­—æ®µï¼Œé»˜è®¤è®¾ä¸º true")
                    result["is_relevant"] = True
                
                if not result.get("rewritten_query"):
                    logger.error("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")
                    raise JSONParseError("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")

                # éªŒè¯ relevant_columns æ ¼å¼å’Œå­—æ®µåæ˜¯å¦å­˜åœ¨
                relevant_columns = result.get("relevant_columns", [])
                if relevant_columns:
                    invalid_columns = []
                    for idx, col in enumerate(relevant_columns):
                        if not isinstance(col, dict) or "column_name" not in col:
                            logger.error(f"relevant_columns[{idx}] æ ¼å¼é”™è¯¯: {col}")
                            raise JSONParseError(
                                f"relevant_columns[{idx}] ç¼ºå°‘ 'column_name' å­—æ®µ"
                            )
                        
                        # æ ¡éªŒå­—æ®µåæ˜¯å¦å­˜åœ¨äºè¡¨ä¸­
                        col_name = col.get("column_name", "")
                        if valid_column_names and col_name not in valid_column_names:
                            invalid_columns.append(col_name)
                    
                    # å¦‚æœæœ‰æ— æ•ˆå­—æ®µåï¼ŒæŠ›å‡º InvalidColumnError è§¦å‘é‡è¯•
                    if invalid_columns:
                        error_msg = f"ä»¥ä¸‹å­—æ®µåä¸å­˜åœ¨äºæ•°æ®è¡¨ä¸­: {invalid_columns}"
                        logger.warning(f"âš ï¸ å­—æ®µæ ¡éªŒå¤±è´¥: {error_msg}ï¼Œå°†è§¦å‘é‡è¯•")
                        raise InvalidColumnError(error_msg, invalid_columns)

                # æ·»åŠ åŸå§‹é—®é¢˜
                result["original_query"] = original_query

                # æå–é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚æœæœ‰ï¼‰
                domain_knowledge = result.get("domain_knowledge")
                if domain_knowledge and isinstance(domain_knowledge, dict):
                    column_name = domain_knowledge.get("column_name")
                    knowledge = domain_knowledge.get("knowledge")
                    if column_name and knowledge:
                        result["_extracted_knowledge"] = {
                            "column_name": column_name,
                            "knowledge": knowledge,
                        }

                logger.info("âœ… JSONè§£ææˆåŠŸï¼Œå­—æ®µæ ¡éªŒé€šè¿‡")
                return result
            else:
                logger.error("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSON")
                raise JSONParseError("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSONå†…å®¹")

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            raise JSONParseError(f"JSONè§£æå¤±è´¥: {e}")

    def _default_result(self, original_query: str) -> Dict:
        """
        è¿”å›é»˜è®¤ç»“æœï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰
        """
        return {
            "original_query": original_query,
            "is_relevant": True,
            "rewritten_query": original_query,
            "relevant_columns": [],
            "analysis_suggestions": ["è¯·æ˜ç¡®éœ€è¦åˆ†æçš„æ•°æ®ç»´åº¦å’ŒæŒ‡æ ‡"],
            "analysis_logic": "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ•°æ®åˆ†æ",
        }
