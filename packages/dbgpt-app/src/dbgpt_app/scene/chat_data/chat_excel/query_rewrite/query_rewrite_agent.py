"""
Queryæ”¹å†™Agent - å‚è€ƒformat_sql/backend/agents/query_rewrite_assistant.py
è´Ÿè´£æ ¹æ®æ•°æ®ç†è§£ä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç›¸å…³åˆ—å’Œåˆ†æå»ºè®®
"""
# ruff: noqa: E501

import json
import logging
from typing import AsyncIterator, Dict, List, Union

logger = logging.getLogger(__name__)


class JSONParseError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šJSONè§£æå¤±è´¥"""

    pass


class InvalidColumnError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šå­—æ®µåä¸å­˜åœ¨"""

    def __init__(self, message: str, invalid_columns: list):
        super().__init__(message)
        self.invalid_columns = invalid_columns


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€

    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬

    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€
    """
    if not text or not text.strip():
        return "en"

    import re

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_pattern = re.compile(r"[\u4e00-\u9fff]+")
    chinese_chars = chinese_pattern.findall(text)
    chinese_count = sum(len(match) for match in chinese_chars)

    # è®¡ç®—æ€»å­—ç¬¦æ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    total_chars = len(re.sub(r'[\s\.,;:!?\'"\-_()\[\]{}]', "", text))

    if total_chars == 0:
        return "en"

    # è®¡ç®—ä¸­æ–‡å æ¯”
    chinese_ratio = chinese_count / total_chars if total_chars > 0 else 0

    # å¦‚æœä¸­æ–‡å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡è¾“å…¥
    return "zh" if chinese_ratio > 0.3 else "en"


class QueryRewriteAgent:
    """
    Queryæ”¹å†™Agent - ç®€åŒ–ç‰ˆæœ¬
    åŠŸèƒ½ï¼š
    1. æ ¹æ®æ•°æ®å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„æé—®
    2. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—
    3. æä¾›åˆ†æå»ºè®®å’Œé€»è¾‘æ”¯æ’‘
    """

    def __init__(self, llm_client=None, model_name=None):
        """
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›é»˜è®¤ç»“æœï¼‰
            model_name: æ¨¡å‹åç§°
        """
        self.llm_client = llm_client
        self.model_name = model_name
    
    def _calculate_edit_distance(self, s1: str, s2: str) -> int:
        """
        è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´çš„ç¼–è¾‘è·ç¦»ï¼ˆLevenshteinè·ç¦»ï¼‰
        
        Args:
            s1: å­—ç¬¦ä¸²1
            s2: å­—ç¬¦ä¸²2
            
        Returns:
            ç¼–è¾‘è·ç¦»
        """
        if len(s1) < len(s2):
            return self._calculate_edit_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                # j+1 ä»£æ›¿ jï¼Œå› ä¸º previous_row å’Œ current_row çš„ç´¢å¼•æ¯” s2 å¤š1
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _find_similar_columns(
        self, 
        invalid_column: str, 
        valid_columns: Dict[str, set], 
        table_name: str = None,
        top_k: int = 3
    ) -> List[Dict]:
        """
        ä¸ºæ— æ•ˆå­—æ®µæ‰¾åˆ°æœ€ç›¸ä¼¼çš„æœ‰æ•ˆå­—æ®µ
        
        Args:
            invalid_column: æ— æ•ˆçš„å­—æ®µå
            valid_columns: æœ‰æ•ˆå­—æ®µå­—å…¸ {table_name: set(column_names)}
            table_name: æŒ‡å®šçš„è¡¨åï¼ˆå¦‚æœæœ‰ï¼‰
            top_k: è¿”å›å‰kä¸ªæœ€ç›¸ä¼¼çš„å­—æ®µ
            
        Returns:
            ç›¸ä¼¼å­—æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {table_name, column_name, distance, similarity}
        """
        similar_columns = []
        
        # å¦‚æœæŒ‡å®šäº†è¡¨åï¼Œåªåœ¨è¯¥è¡¨ä¸­æŸ¥æ‰¾
        if table_name and table_name in valid_columns:
            search_tables = {table_name: valid_columns[table_name]}
        else:
            search_tables = valid_columns
        
        for tbl_name, columns in search_tables.items():
            for col_name in columns:
                # è®¡ç®—ç¼–è¾‘è·ç¦»
                distance = self._calculate_edit_distance(
                    invalid_column.lower(), 
                    col_name.lower()
                )
                
                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼‰
                max_len = max(len(invalid_column), len(col_name))
                similarity = 1 - (distance / max_len) if max_len > 0 else 0
                
                similar_columns.append({
                    "table_name": tbl_name,
                    "column_name": col_name,
                    "distance": distance,
                    "similarity": similarity
                })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆä»é«˜åˆ°ä½ï¼‰
        similar_columns.sort(key=lambda x: (-x["similarity"], x["distance"]))
        
        # è¿”å›å‰kä¸ª
        return similar_columns[:top_k]

    def _extract_valid_column_names(self, table_schema_json: str) -> Union[set, Dict[str, set]]:
        """
        ä»schemaä¸­æå–æ‰€æœ‰æœ‰æ•ˆçš„å­—æ®µå
        æ”¯æŒå•è¡¨å’Œå¤šè¡¨æ¨¡å¼
        
        Returns:
            å•è¡¨æ¨¡å¼ï¼šè¿”å›å­—æ®µåçš„é›†åˆ (set)
            å¤šè¡¨æ¨¡å¼ï¼šè¿”å›å­—å…¸ {table_name: set(column_names)}
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            if not isinstance(schema_obj, dict):
                return set()

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè¡¨æ¨¡å¼
            if schema_obj.get("is_multi_table"):
                # å¤šè¡¨æ¨¡å¼ï¼šè¿”å› {table_name: set(columns)} çš„å­—å…¸
                table_columns_map = {}
                
                for table in schema_obj.get("tables", []):
                    table_name = table.get("table_name", "")
                    if not table_name:
                        continue
                    
                    table_columns = set()
                    
                    # æ–¹å¼1ï¼šä» create_table_sql ä¸­è§£æå­—æ®µå
                    create_table_sql = table.get("create_table_sql", "")
                    if create_table_sql:
                        columns_from_sql = self._extract_columns_from_sql(create_table_sql)
                        table_columns.update(columns_from_sql)
                    
                    # æ–¹å¼2ï¼šä» columns åˆ—è¡¨ä¸­æå–
                    columns = table.get("columns", [])
                    for col in columns:
                        if isinstance(col, dict):
                            col_name = col.get("column_name", "")
                            if col_name:
                                table_columns.add(col_name)
                        elif isinstance(col, str):
                            if col:
                                table_columns.add(col)
                    
                    table_columns_map[table_name] = table_columns
                
                return table_columns_map
            else:
                # å•è¡¨æ¨¡å¼ï¼šè¿”å›å­—æ®µåé›†åˆ
                valid_columns = set()
                columns = schema_obj.get("columns", [])
                for col in columns:
                    if isinstance(col, dict):
                        col_name = col.get("column_name", "")
                        if col_name:
                            valid_columns.add(col_name)
                    elif isinstance(col, str):
                        if col:
                            valid_columns.add(col)

            if valid_columns:
                logger.debug(f"âœ… æå–åˆ°çš„æœ‰æ•ˆå­—æ®µåæ•°é‡: {len(valid_columns)}")
                logger.debug(f"å­—æ®µåç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰: {list(valid_columns)[:5]}")
            else:
                logger.warning(f"âš ï¸ æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆå­—æ®µåï¼Œschemaæ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
            return valid_columns
        except Exception as e:
            logger.warning(f"æå–å­—æ®µåå¤±è´¥: {e}")
            return set()
    
    def _extract_columns_from_sql(self, create_table_sql: str) -> set:
        """
        ä» CREATE TABLE SQL è¯­å¥ä¸­æå–å­—æ®µå
        
        Args:
            create_table_sql: CREATE TABLE SQL è¯­å¥
            
        Returns:
            å­—æ®µåçš„é›†åˆ
        """
        import re
        columns = set()
        
        try:
            if not create_table_sql:
                return columns
            
            # åŒ¹é… CREATE TABLE ... (å­—æ®µå®šä¹‰) ä¸­çš„å­—æ®µ
            # æ¨¡å¼1: "å­—æ®µå" ç±»å‹
            # æ¨¡å¼2: "å­—æ®µå" ç±»å‹,
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¢«åŒå¼•å·åŒ…å›´çš„å­—æ®µå
            pattern = r'"([^"]+)"'
            matches = re.findall(pattern, create_table_sql)
            
            for match in matches:
                # è¿‡æ»¤æ‰å¯èƒ½æ˜¯è¡¨åçš„æƒ…å†µï¼ˆé€šå¸¸åœ¨ CREATE TABLE åé¢ï¼‰
                # ç®€å•æ£€æŸ¥ï¼šå¦‚æœ match åœ¨ CREATE TABLE è¯­å¥çš„è¡¨åä½ç½®ï¼Œè·³è¿‡
                # ä½†å¯¹äºå¤§å¤šæ•°æƒ…å†µï¼Œæ‰€æœ‰åŒ¹é…çš„åŒå¼•å·å†…å®¹éƒ½å¯èƒ½æ˜¯å­—æ®µå
                columns.add(match)
            
            logger.debug(f"ä»SQLä¸­æå–åˆ° {len(columns)} ä¸ªå­—æ®µå")
        except Exception as e:
            logger.warning(f"ä»SQLæå–å­—æ®µåå¤±è´¥: {e}")
        
        return columns

    def _simplify_schema_for_rewrite(self, table_schema_json: str) -> str:
        """
        ç²¾ç®€schemaç”¨äºqueryæ”¹å†™ï¼Œåªä¿ç•™å¿…è¦å­—æ®µä¿¡æ¯
        ç§»é™¤ suggested_questions_zhã€suggested_questions_en ç­‰ä¸å¿…è¦å­—æ®µ
        æ”¯æŒå¤šè¡¨ schemaï¼ˆis_multi_table=Trueï¼‰
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            if not isinstance(schema_obj, dict):
                return table_schema_json

            # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šè¡¨ schema
            if schema_obj.get("is_multi_table"):
                return self._simplify_multi_table_schema(schema_obj)

            # å•è¡¨æ¨¡å¼ï¼šåªä¿ç•™columnså­—æ®µï¼Œç§»é™¤å»ºè®®é—®é¢˜ç­‰
            simplified = {}
            if "columns" in schema_obj:
                # ç²¾ç®€æ¯ä¸ªåˆ—çš„ä¿¡æ¯
                simplified_columns = []
                for col in schema_obj["columns"]:
                    simplified_col = {
                        "column_name": col.get("column_name", ""),
                        "data_type": col.get("data_type", ""),
                    }
                    # ä¿ç•™å…³é”®å­—æ®µæ ‡è®°
                    if col.get("is_key_field"):
                        simplified_col["is_key_field"] = True
                    # ä¿ç•™ä¸šåŠ¡çŸ¥è¯†
                    if col.get("domain_knowledge"):
                        simplified_col["domain_knowledge"] = col["domain_knowledge"]
                    # ä¿ç•™åˆ†ç±»å­—æ®µçš„å¯é€‰å€¼ï¼ˆç²¾ç®€ç‰ˆï¼‰
                    if col.get("unique_values_top20"):
                        values = col["unique_values_top20"]
                        # æœ€å¤šä¿ç•™10ä¸ªå€¼
                        simplified_col["possible_values"] = values[:10] if len(values) > 10 else values
                    # ä¿ç•™æ•°å€¼ç»Ÿè®¡æ‘˜è¦
                    if col.get("statistics_summary"):
                        simplified_col["stats"] = col["statistics_summary"]
                    simplified_columns.append(simplified_col)
                simplified["columns"] = simplified_columns

            # ä¿ç•™è¡¨åç­‰åŸºæœ¬ä¿¡æ¯
            if "table_name" in schema_obj:
                simplified["table_name"] = schema_obj["table_name"]

            return json.dumps(simplified, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"ç²¾ç®€schemaå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹schema")
            return table_schema_json if isinstance(table_schema_json, str) else json.dumps(table_schema_json, ensure_ascii=False)

    def _simplify_multi_table_schema(self, schema_obj: dict) -> str:
        """
        ç²¾ç®€å¤šè¡¨ schemaï¼Œä¸ºæ¯ä¸ªè¡¨ä¿ç•™å¿…è¦çš„å­—æ®µä¿¡æ¯
        æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
        1. æ—§æ ¼å¼ï¼šåŒ…å« columns åˆ—è¡¨
        2. æ–°æ ¼å¼ï¼šåŒ…å« create_table_sql å’Œ sample_rows
        """
        simplified = {
            "is_multi_table": True,
            "table_count": schema_obj.get("table_count", 0),
            "table_names": schema_obj.get("table_names", []),
            "table_description": schema_obj.get("table_description", ""),
            "tables": [],
        }
        
        for table in schema_obj.get("tables", []):
            simplified_table = {
                "table_name": table.get("table_name", ""),
                "table_description": table.get("table_description", ""),
            }
            
            # æ–°æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ create_table_sql å’Œ sample_rows
            if table.get("create_table_sql"):
                simplified_table["create_table_sql"] = table.get("create_table_sql")
                simplified_table["sample_rows"] = table.get("sample_rows", [])
                simplified_table["columns"] = table.get("columns", [])  # åˆ—ååˆ—è¡¨
            else:
                # æ—§æ ¼å¼ï¼šå¤„ç† columns åˆ—è¡¨
                simplified_table["columns"] = []
                for col in table.get("columns", []):
                    if isinstance(col, dict):
                        simplified_col = {
                            "column_name": col.get("column_name", ""),
                            "data_type": col.get("data_type", ""),
                        }
                        # ä¿ç•™å…³é”®å­—æ®µæ ‡è®°
                        if col.get("is_key_field"):
                            simplified_col["is_key_field"] = True
                        # ä¿ç•™ä¸šåŠ¡çŸ¥è¯†
                        if col.get("domain_knowledge"):
                            simplified_col["domain_knowledge"] = col["domain_knowledge"]
                        # ä¿ç•™åˆ†ç±»å­—æ®µçš„å¯é€‰å€¼ï¼ˆç²¾ç®€ç‰ˆï¼‰
                        if col.get("unique_values_top20"):
                            values = col["unique_values_top20"]
                            simplified_col["possible_values"] = values[:10] if len(values) > 10 else values
                        # ä¿ç•™æ•°å€¼ç»Ÿè®¡æ‘˜è¦
                        if col.get("statistics_summary"):
                            simplified_col["stats"] = col["statistics_summary"]
                        simplified_table["columns"].append(simplified_col)
                    else:
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼ˆåˆ—åï¼‰ï¼Œç›´æ¥æ·»åŠ 
                        simplified_table["columns"].append(col)
            
            simplified["tables"].append(simplified_table)
        
        return json.dumps(simplified, ensure_ascii=False, indent=2)

    def _format_multi_table_info_for_prompt(self, simplified_schema: str) -> str:
        """
        å°†å¤šè¡¨ schema æ ¼å¼åŒ–ä¸º prompt ä¸­æ˜“è¯»çš„æ ¼å¼
        åŒ…å«å»ºè¡¨ SQL å’Œæ ·æœ¬æ•°æ®
        """
        try:
            schema_obj = json.loads(simplified_schema) if isinstance(simplified_schema, str) else simplified_schema
        except Exception:
            return simplified_schema
        
        if not schema_obj.get("is_multi_table"):
            return simplified_schema
        
        result_parts = []
        
        for table in schema_obj.get("tables", []):
            table_name = table.get("table_name", "æœªçŸ¥è¡¨")
            table_desc = table.get("table_description", "")
            
            table_section = f"### è¡¨å: {table_name}\n"
            if table_desc:
                table_section += f"**è¡¨æè¿°**: {table_desc}\n\n"
            
            # æ·»åŠ å»ºè¡¨ SQL
            create_sql = table.get("create_table_sql", "")
            if create_sql:
                table_section += f"**å»ºè¡¨SQL**:\n```sql\n{create_sql}\n```\n\n"
            else:
                # å¦‚æœæ²¡æœ‰å»ºè¡¨SQLï¼Œä» columns æ„å»ºå­—æ®µåˆ—è¡¨
                columns = table.get("columns", [])
                if columns:
                    if isinstance(columns[0], dict):
                        col_names = [col.get("column_name", "") for col in columns]
                    else:
                        col_names = columns
                    table_section += f"**å­—æ®µåˆ—è¡¨**: {', '.join(col_names)}\n\n"
            
            # æ·»åŠ æ ·æœ¬æ•°æ®
            sample_rows = table.get("sample_rows", [])
            columns = table.get("columns", [])
            if sample_rows:
                table_section += "**æ ·æœ¬æ•°æ®ï¼ˆå‰2è¡Œï¼‰**:\n"
                # å¦‚æœæœ‰åˆ—åï¼Œæ˜¾ç¤ºä¸ºè¡¨æ ¼å½¢å¼
                if columns:
                    if isinstance(columns[0], dict):
                        col_names = [col.get("column_name", "") for col in columns]
                    else:
                        col_names = columns
                    table_section += f"åˆ—å: {col_names}\n"
                for i, row in enumerate(sample_rows[:2]):
                    table_section += f"è¡Œ{i+1}: {row}\n"
                table_section += "\n"
            
            result_parts.append(table_section)
        
        return "\n---\n".join(result_parts)

    async def rewrite_query_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        æµå¼æ”¹å†™ç”¨æˆ·query
        
        å…ˆæµå¼è¾“å‡ºLLMçš„åŸå§‹è¾“å‡ºï¼ˆæ–‡æœ¬å’ŒJSONï¼‰ï¼Œç„¶åè¾“å‡ºè§£æåçš„ç»“æœ
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®è¡Œï¼Œæ ¼å¼ä¸º [(columns, data_rows)] æˆ–ç›´æ¥çš„è¡Œåˆ—è¡¨
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunk
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœï¼ˆå½“æµå¼è¾“å‡ºå®Œæˆæ—¶ï¼‰
        """
        logger.info(f"Queryæ”¹å†™ï¼ˆæµå¼ï¼‰ - åŸå§‹é—®é¢˜: {user_query}")

        # å°è¯•ä½¿ç”¨LLMæ”¹å†™
        if self.llm_client:
            try:
                logger.info("å°è¯•ä½¿ç”¨LLMè¿›è¡ŒQueryæ”¹å†™ï¼ˆæµå¼ï¼‰...")
                full_text = ""
                async for chunk in self._llm_based_rewrite_stream(
                    user_query,
                    table_schema_json,
                    table_description,
                    chat_history=chat_history,
                    sample_rows=sample_rows,
                ):
                    if isinstance(chunk, str):
                        # æµå¼è¾“å‡ºåŸå§‹æ–‡æœ¬chunk
                        full_text = chunk
                        yield chunk
                    elif isinstance(chunk, dict):
                        # è¿”å›æœ€ç»ˆè§£æç»“æœ
                        logger.info(f"âœ… LLMæ”¹å†™æˆåŠŸï¼ˆæµå¼ï¼‰ - æ”¹å†™ç»“æœ: {chunk.get('rewritten_query', '')}")
                        yield chunk
                        return
            except JSONParseError as e:
                logger.error(f"âŒ LLMæ”¹å†™å¤±è´¥ï¼ˆJSONè§£æé”™è¯¯ï¼‰: {e}")
                logger.warning("âš ï¸ ä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
        else:
            logger.info("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™")

        # Fallback: åŸºäºè§„åˆ™çš„ç®€å•æ”¹å†™
        result = self._rule_based_rewrite(user_query, table_schema_json)
        logger.info(f"è§„åˆ™æ”¹å†™ - æ”¹å†™ç»“æœ: {result['rewritten_query']}")
        yield result

    
    
    def _rule_based_rewrite(self, user_query: str, table_schema_json: str) -> Dict:
        """
        åŸºäºè§„åˆ™çš„Queryæ”¹å†™ï¼ˆä¸ä¾èµ–LLMï¼Œå¿«é€Ÿå¯é ï¼‰
        """
        try:
            # è§„åˆ™æ”¹å†™ä¸è¿›è¡Œç›¸å…³æ€§åˆ¤æ–­ï¼Œç»Ÿä¸€è¿”å› is_relevant=True
            # ç›¸å…³æ€§åˆ¤æ–­äº¤ç»™LLMå¤„ç†ï¼ˆæ›´å‡†ç¡®ï¼‰
            
            # è§£æschema JSON
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            # è·å–åˆ—ä¿¡æ¯
            schema_data = (
                schema_obj.get("columns", []) if isinstance(schema_obj, dict) else []
            )

            relevant_columns = []
            analysis_suggestions = []

            # åˆ†æç”¨æˆ·queryä¸­æåˆ°çš„å…³é”®è¯
            query_lower = user_query.lower()

            # æ£€æµ‹æ—¶é—´ç›¸å…³çš„åˆ†æ
            if any(
                keyword in query_lower for keyword in ["åŒæ¯”", "ç¯æ¯”", "yoy", "mom"]
            ):
                # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
                date_cols = [
                    col
                    for col in schema_data
                    if any(
                        kw in col.get("column_name", "").lower()
                        for kw in ["date", "æ—¥æœŸ", "time", "æ—¶é—´"]
                    )
                ]
                if date_cols:
                    relevant_columns.append(
                        {
                            "column_name": date_cols[0]["column_name"],
                            "usage": "æ—¶é—´ç­›é€‰å’Œåˆ†ç»„æ¡ä»¶ï¼Œéœ€è¦åŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®",
                        }
                    )

                analysis_suggestions.append(
                    "åŒæ¯”åˆ†æéœ€è¦å»å¹´åŒæœŸæ•°æ®ï¼Œç¡®ä¿WHEREæ¡ä»¶åŒ…å«è‡³å°‘ä¸¤å¹´çš„æ•°æ®"
                )
                analysis_suggestions.append(
                    "ç¯æ¯”åˆ†æéœ€è¦ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®ï¼Œä½¿ç”¨LAGçª—å£å‡½æ•°"
                )
                analysis_suggestions.append(
                    "æ—¶é—´èŒƒå›´ç¤ºä¾‹ï¼šWHERE è®¢å•æ—¥æœŸ >= '2021-01-01' AND è®¢å•æ—¥æœŸ < '2023-01-01'"
                )

            # æ£€æµ‹åœ°åŸŸåˆ†æ
            if any(
                keyword in user_query
                for keyword in [
                    "åŒºåŸŸ",
                    "åœ°åŒº",
                    "ååŒ—",
                    "åä¸œ",
                    "åå—",
                    "ä¸œåŒ—",
                    "è¥¿åŒ—",
                    "è¥¿å—",
                    "ä¸­å—",
                ]
            ):
                region_cols = [
                    col for col in schema_data if "åŒºåŸŸ" in col.get("column_name", "")
                ]
                if region_cols:
                    relevant_columns.append(
                        {
                            "column_name": region_cols[0]["column_name"],
                            "usage": "åœ°åŸŸç­›é€‰æˆ–åˆ†ç»„æ¡ä»¶",
                        }
                    )
                    analysis_suggestions.append("ä½¿ç”¨'åŒºåŸŸ'å­—æ®µè¿›è¡Œç­›é€‰æˆ–åˆ†ç»„")

            # æ£€æµ‹æŒ‡æ ‡åˆ†æ
            metric_keywords = {
                "åˆ©æ¶¦": "profit",
                "é”€å”®é¢": "sales",
                "é”€é‡": "quantity",
                "æ•°é‡": "quantity",
            }

            for cn_keyword, en_keyword in metric_keywords.items():
                if cn_keyword in user_query:
                    metric_cols = [
                        col
                        for col in schema_data
                        if cn_keyword in col.get("column_name", "")
                    ]
                    if metric_cols:
                        relevant_columns.append(
                            {
                                "column_name": metric_cols[0]["column_name"],
                                "usage": "èšåˆæŒ‡æ ‡ï¼Œéœ€è¦è¿›è¡ŒSUM/AVGç­‰èšåˆè¿ç®—",
                            }
                        )
                        analysis_suggestions.append(
                            f"å¯¹'{cn_keyword}'å­—æ®µè¿›è¡Œèšåˆè®¡ç®—ï¼ˆSUMæ±‚å’Œï¼‰"
                        )

            # æ„å»ºæ”¹å†™åçš„query
            rewritten_query = self._enhance_query(
                user_query, relevant_columns, analysis_suggestions
            )

            # æ„å»ºåˆ†æé€»è¾‘
            analysis_logic = self._build_analysis_logic(user_query, relevant_columns)

            return {
                "original_query": user_query,
                "is_relevant": True,
                "rewritten_query": rewritten_query,
                "relevant_columns": relevant_columns,
                "analysis_suggestions": analysis_suggestions,
                "analysis_logic": analysis_logic,
            }

        except Exception as e:
            logger.error(f"è§„åˆ™æ”¹å†™å¤±è´¥: {e}", exc_info=True)
            return self._default_result(user_query)

    def _enhance_query(
        self, user_query: str, relevant_columns: List[Dict], suggestions: List[str]
    ) -> str:
        """
        å¢å¼ºç”¨æˆ·query
        """
        if not relevant_columns:
            return user_query

        # æ·»åŠ æ˜ç¡®çš„å­—æ®µå¼•ç”¨
        col_names = [col["column_name"] for col in relevant_columns]
        enhanced = user_query

        # å¦‚æœæ˜¯ç®€çŸ­çš„queryï¼Œæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
        if len(user_query) < 20:
            enhanced = f"åŸºäºæ•°æ®è¡¨ï¼Œä½¿ç”¨å­—æ®µã€{', '.join(col_names)}ã€‘æ¥{user_query}"

        return enhanced

    def _build_analysis_logic(
        self, user_query: str, relevant_columns: List[Dict]
    ) -> str:
        """
        æ„å»ºåˆ†æé€»è¾‘
        """
        logic_parts = []

        # ç­›é€‰æ¡ä»¶
        filter_cols = [
            col for col in relevant_columns if "ç­›é€‰" in col.get("usage", "")
        ]
        if filter_cols:
            logic_parts.append(
                f"1. ç­›é€‰æ¡ä»¶ï¼š{', '.join([c['column_name'] for c in filter_cols])}"
            )

        # åˆ†ç»„ç»´åº¦
        group_cols = [col for col in relevant_columns if "åˆ†ç»„" in col.get("usage", "")]
        if group_cols:
            logic_parts.append(
                f"2. åˆ†ç»„ç»´åº¦ï¼š{', '.join([c['column_name'] for c in group_cols])}"
            )

        # èšåˆæŒ‡æ ‡
        agg_cols = [col for col in relevant_columns if "èšåˆ" in col.get("usage", "")]
        if agg_cols:
            logic_parts.append(
                f"3. èšåˆæŒ‡æ ‡ï¼š{', '.join([c['column_name'] for c in agg_cols])}"
            )

        if logic_parts:
            return "\n".join(logic_parts)
        else:
            return "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ ‡å‡†çš„æ•°æ®æŸ¥è¯¢å’Œåˆ†æ"

    def _format_sample_rows(self, sample_rows: list) -> str:
        """
        æ ¼å¼åŒ–ä»å¤–éƒ¨ä¼ å…¥çš„æ ·æœ¬æ•°æ®ï¼ˆåˆ—ååªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        
        Args:
            sample_rows: æ ¼å¼ä¸º (columns, data_rows) çš„å…ƒç»„ï¼Œ
                        å…¶ä¸­ columns æ˜¯åˆ—ååˆ—è¡¨ï¼Œdata_rows æ˜¯æ•°æ®è¡Œåˆ—è¡¨
        """
        try:
            if not sample_rows:
                return ""
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ (columns, data_rows) æ ¼å¼
            if isinstance(sample_rows, tuple) and len(sample_rows) == 2:
                columns, data_rows = sample_rows
                if not data_rows or not columns:
                    return ""
                
                # å…ˆæ˜¾ç¤ºåˆ—å
                lines = [f"åˆ—å: {json.dumps(list(columns), ensure_ascii=False)}"]
                
                # åªå–å‰2è¡Œï¼Œåªæ˜¾ç¤ºå€¼
                for i, row in enumerate(data_rows[:2], 1):
                    values = []
                    for v in row:
                        if v is None or (isinstance(v, float) and str(v) == 'nan'):
                            values.append(None)
                        elif hasattr(v, 'strftime'):
                            values.append(v.strftime("%Y-%m-%d"))
                        elif isinstance(v, (int, float, bool)):
                            values.append(v)
                        else:
                            values.append(str(v))
                    lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                return "\n".join(lines)
            
            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼ˆç›´æ¥çš„è¡Œåˆ—è¡¨ï¼Œæ¯è¡Œæ˜¯dictï¼‰ï¼Œæå–å…±ç”¨åˆ—å
            if isinstance(sample_rows, list) and len(sample_rows) > 0:
                first_row = sample_rows[0]
                if isinstance(first_row, dict):
                    columns = list(first_row.keys())
                    lines = [f"åˆ—å: {json.dumps(columns, ensure_ascii=False)}"]
                    for i, row in enumerate(sample_rows[:2], 1):
                        values = [row.get(col) for col in columns]
                        lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                    return "\n".join(lines)
                else:
                    # édictæ ¼å¼ï¼Œç›´æ¥è¾“å‡º
                    lines = []
                    for i, row in enumerate(sample_rows[:2], 1):
                        lines.append(f"è¡Œ{i}: {str(row)}")
                    return "\n".join(lines)
            
            return ""
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
            return ""

    def _extract_sample_rows(self, table_schema_json: str) -> str:
        """
        ä»schemaä¸­æå–æ ·æœ¬æ•°æ®å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆåˆ—ååªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        """
        try:
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            sample_rows = schema_obj.get("sample_rows", [])
            if not sample_rows:
                return ""

            # æå–åˆ—åï¼ˆä»ç¬¬ä¸€è¡Œçš„keysï¼‰
            first_row = sample_rows[0]
            if isinstance(first_row, dict):
                columns = list(first_row.keys())
                lines = [f"åˆ—å: {json.dumps(columns, ensure_ascii=False)}"]
                for i, row in enumerate(sample_rows[:2], 1):
                    values = [row.get(col) for col in columns]
                    lines.append(f"è¡Œ{i}: {json.dumps(values, ensure_ascii=False)}")
                return "\n".join(lines)
            else:
                # édictæ ¼å¼
                lines = []
                for i, row in enumerate(sample_rows[:2], 1):
                    lines.append(f"è¡Œ{i}: {json.dumps(row, ensure_ascii=False)}")
                return "\n".join(lines)
        except Exception as e:
            logger.warning(f"æå–æ ·æœ¬æ•°æ®å¤±è´¥: {e}")
            return ""

    def _build_rewrite_prompt(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
    ) -> str:
        """
        æ„å»ºæ”¹å†™prompt
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®ï¼Œæ ¼å¼ä¸º (columns, data_rows) æˆ–ç›´æ¥ä»å‚æ•°ä¼ å…¥
        """
        # ç²¾ç®€schemaï¼Œç§»é™¤å»ºè®®é—®é¢˜ç­‰ä¸å¿…è¦ä¿¡æ¯
        simplified_schema = self._simplify_schema_for_rewrite(table_schema_json)
        
        # æ£€æµ‹æ˜¯å¦ä¸ºå¤šè¡¨æ¨¡å¼
        is_multi_table = False
        try:
            schema_obj = json.loads(table_schema_json) if isinstance(table_schema_json, str) else table_schema_json
            is_multi_table = schema_obj.get("is_multi_table", False)
        except Exception:
            pass
        
        # æå–æ ·æœ¬æ•°æ®ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ sample_rowsï¼Œå…¶æ¬¡ä» schema ä¸­æå–
        sample_rows_str = ""
        if sample_rows:
            sample_rows_str = self._format_sample_rows(sample_rows)
        if not sample_rows_str:
            sample_rows_str = self._extract_sample_rows(table_schema_json)
        
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n=== å†å²å¯¹è¯ä¸Šä¸‹æ–‡ ===\n"
            # åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯ï¼ˆ6æ¡æ¶ˆæ¯ï¼‰ï¼Œé¿å…promptè¿‡é•¿
            recent_history = (
                chat_history[-9:] if len(chat_history) > 9 else chat_history
            )

            for msg in recent_history:
                role = msg.get("role", "user") if isinstance(msg, dict) else "user"
                content = (
                    str(msg.get("content", "")) if isinstance(msg, dict) else str(msg)
                )

                # æ ¹æ®å®é™…è§’è‰²æ˜¾ç¤º
                if "human" in role.lower() or "user" in role.lower():
                    role_display = "ç”¨æˆ·"
                elif "ai" in role.lower() or "assistant" in role.lower():
                    role_display = "åŠ©æ‰‹"
                else:
                    role_display = role

                history_context += f"\n{role_display}: {content}\n"

        # æ£€æµ‹ç”¨æˆ·è¾“å…¥è¯­è¨€
        user_language = detect_language(user_query)
        logger.info(f"ğŸŒ Queryæ”¹å†™ - æ£€æµ‹åˆ°ç”¨æˆ·è¾“å…¥è¯­è¨€: {user_language}, å¤šè¡¨æ¨¡å¼: {is_multi_table}")

        # æ ¹æ®è¯­è¨€å’Œæ˜¯å¦å¤šè¡¨é€‰æ‹©prompt
        if user_language == "zh":
            if is_multi_table:
                prompt = self._build_multi_table_rewrite_prompt_zh(
                    user_query, simplified_schema, table_description, 
                    sample_rows_str, history_context
                )
            else:
                prompt = self._build_single_table_rewrite_prompt_zh(
                    user_query, simplified_schema, table_description,
                    sample_rows_str, history_context
                )
        else:
            if is_multi_table:
                prompt = self._build_multi_table_rewrite_prompt_en(
                    user_query, simplified_schema, table_description,
                    sample_rows_str, history_context
                )
            else:
                prompt = self._build_single_table_rewrite_prompt_en(
                    user_query, simplified_schema, table_description,
                    sample_rows_str, history_context
                )
        return prompt

    def _build_single_table_rewrite_prompt_zh(
        self, user_query: str, simplified_schema: str, table_description: str,
        sample_rows_str: str, history_context: str
    ) -> str:
        """æ„å»ºå•è¡¨æ¨¡å¼çš„ä¸­æ–‡æ”¹å†™prompt"""
        return f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆ†æé—®é¢˜ï¼Œä½ éœ€è¦ï¼š
1. æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå›ç­”ï¼Œå……åˆ†ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜
2. æ ¹æ®æ•°æ®è¡¨çš„å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„é—®é¢˜
3. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—ï¼ˆåŒ…æ‹¬ç­›é€‰æ¡ä»¶åˆ—ã€åˆ†ç»„ç»´åº¦åˆ—ã€èšåˆæŒ‡æ ‡åˆ—ï¼‰
4. æä¾›3-5æ¡åˆ†æå»ºè®®ï¼Œè¯´æ˜å¦‚ä½•åˆ†æè¿™ä¸ªé—®é¢˜
5. ç»™å‡ºæ¸…æ™°çš„åˆ†æé€»è¾‘
6. å¦‚æœç”¨æˆ·åœ¨å¯¹è¯æˆ–å½“å‰å†å²é—®ç­”ä¸Šä¸‹æ–‡ä¸­çº æ­£æˆ–è¡¥å……äº†å­—æ®µçš„ä½¿ç”¨æ–¹æ³•ã€ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰å…³é”®çŸ¥è¯†ï¼Œè¯·æå–å¹¶è®°å½•ä½œä¸ºdomain_knowledgeå­—æ®µ

=== æ•°æ®è¡¨å­—æ®µä¿¡æ¯ ===
{simplified_schema}

**æ³¨æ„**ï¼šå­—æ®µä¿¡æ¯ä¸­å¯èƒ½åŒ…å« `domain_knowledge` å­—æ®µï¼Œè¿™æ˜¯ä¹‹å‰ä»ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ åˆ°çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä½¿ç”¨ã€‚

=== çœŸå®æ•°æ®æ ·æœ¬ï¼ˆ2è¡Œï¼‰ ===
{sample_rows_str if sample_rows_str else "æš‚æ— æ ·æœ¬æ•°æ®"}

=== æ•°æ®è¡¨æè¿° ===
{table_description}

{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{user_query}

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "is_relevant": true,
  "rewritten_query": "æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼Œæ˜ç¡®æŒ‡å‡ºéœ€è¦åˆ†æçš„ç»´åº¦å’ŒæŒ‡æ ‡",
  "analysis_suggestions": [
    "å»ºè®®1ï¼šå…·ä½“çš„åˆ†ææ­¥éª¤æˆ–æ³¨æ„äº‹é¡¹",
    "å»ºè®®2ï¼š...",
    "å»ºè®®3ï¼š..."
  ],
  "analysis_logic": "åˆ†æé€»è¾‘çš„è¯¦ç»†è¯´æ˜",
  "relevant_columns": [
    {{
      "column_name": "åˆ—å",
      "usage": "ç”¨é€”è¯´æ˜ï¼ˆå¦‚ï¼šç­›é€‰æ¡ä»¶/åˆ†ç»„ç»´åº¦/èšåˆæŒ‡æ ‡ï¼‰"
    }}
  ],
  "domain_knowledge": null
}}

**ä¸¥æ ¼è¦æ±‚ - å­—æ®µåå¿…é¡»æ¥è‡ªæä¾›çš„åˆ—è¡¨**ï¼š
- relevant_columns ä¸­çš„ column_name å¿…é¡»**ä¸¥æ ¼ä»ä¸Šé¢æä¾›çš„å­—æ®µä¿¡æ¯ä¸­é€‰æ‹©**
- **ç¦æ­¢æ¨æµ‹ã€åˆ›é€ æˆ–ç¼–é€ ä»»ä½•ä¸åœ¨å­—æ®µåˆ—è¡¨ä¸­çš„å­—æ®µå**

ç°åœ¨è¯·ç»“åˆå†å²ä¸Šä¸‹æ–‡åŠç”¨æˆ·å½“å‰é—®é¢˜ï¼Œåˆ†æç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜å¹¶ç”¨ä¸­æ–‡è¾“å‡ºJSONï¼š
"""

    def _build_multi_table_rewrite_prompt_zh(
        self, user_query: str, simplified_schema: str, table_description: str,
        sample_rows_str: str, history_context: str
    ) -> str:
        """æ„å»ºå¤šè¡¨æ¨¡å¼çš„ä¸­æ–‡æ”¹å†™prompt"""
        # è§£æ schema ä»¥è·å–å„è¡¨çš„å»ºè¡¨SQLå’Œæ ·æœ¬æ•°æ®
        tables_info_str = self._format_multi_table_info_for_prompt(simplified_schema)
        
        # æå–å­—æ®µå·®å¼‚è­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
        schema_diff_warning = ""
        try:
            schema_obj = json.loads(simplified_schema) if isinstance(simplified_schema, str) else simplified_schema
            if schema_obj.get("schema_differences"):
                schema_diff_warning = f"\n\n{schema_obj['schema_differences']}\n"
        except Exception:
            pass
        
        return f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªéœ€è¦**å¤šè¡¨è”åˆæŸ¥è¯¢**çš„æ•°æ®åˆ†æé—®é¢˜ã€‚

**é‡è¦ï¼šè¿™æ˜¯ä¸€ä¸ªå¤šè¡¨æŸ¥è¯¢åœºæ™¯ï¼Œä½ éœ€è¦è€ƒè™‘å¦‚ä½•è”åˆä½¿ç”¨å¤šä¸ªè¡¨æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚**

=== å¯ç”¨çš„æ•°æ®è¡¨ ===
{tables_info_str}
{schema_diff_warning}
**æ³¨æ„**ï¼š
1. ä¸Šé¢åŒ…å«å¤šä¸ªè¡¨çš„å»ºè¡¨SQLå’Œæ ·æœ¬æ•°æ®
2. ä½ éœ€è¦åˆ†æå“ªäº›è¡¨å’Œå­—æ®µä¸ç”¨æˆ·é—®é¢˜ç›¸å…³
3. å¦‚æœéœ€è¦è·¨è¡¨æŸ¥è¯¢ï¼Œè€ƒè™‘ä½¿ç”¨ UNION ALL åˆå¹¶ç›¸ä¼¼ç»“æ„çš„è¡¨ï¼Œæˆ–ä½¿ç”¨ JOIN å…³è”ä¸åŒè¡¨
4. **ä¸åŒè¡¨çš„å­—æ®µåå¯èƒ½ä¸åŒï¼Œä½†å«ä¹‰ç›¸ä¼¼**ï¼Œè¯·ä»”ç»†å¯¹æ¯”å„è¡¨çš„å­—æ®µåå’Œæ ·æœ¬æ•°æ®æ¥ç†è§£å­—æ®µå¯¹åº”å…³ç³»
5. **âš ï¸ å…³é”®ï¼šåœ¨ä½¿ç”¨UNION ALLæ—¶ï¼Œå¿…é¡»ç¡®ä¿SELECTçš„å­—æ®µåœ¨æ‰€æœ‰è¡¨ä¸­éƒ½å­˜åœ¨ï¼å¦‚æœæŸä¸ªå­—æ®µåªå­˜åœ¨äºéƒ¨åˆ†è¡¨ï¼Œè¯·ä½¿ç”¨NULLæˆ–é»˜è®¤å€¼å¡«å……ä¸å­˜åœ¨è¯¥å­—æ®µçš„è¡¨**

=== æ•°æ®è¡¨æè¿° ===
{table_description}

{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{user_query}

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "is_relevant": true,
  "rewritten_query": "æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼Œæ˜ç¡®æŒ‡å‡ºéœ€è¦ä»å“ªäº›è¡¨æŸ¥è¯¢ã€åˆ†æçš„ç»´åº¦å’ŒæŒ‡æ ‡",
  ],
  "analysis_suggestions": [
    "å»ºè®®1ï¼šè¯´æ˜å¦‚ä½•è”åˆå¤šä¸ªè¡¨è¿›è¡ŒæŸ¥è¯¢",
    "å»ºè®®2ï¼šå¦‚æœè¡¨ç»“æ„ç›¸ä¼¼ï¼Œå»ºè®®ä½¿ç”¨ UNION ALL åˆå¹¶åå†åˆ†æ",
    "å»ºè®®3ï¼šå¦‚æœéœ€è¦å…³è”ä¸åŒè¡¨ï¼Œè¯´æ˜ JOIN çš„æ–¹å¼",
    "å»ºè®®4ï¼š..."
  ],
  "analysis_logic": "å¤šè¡¨åˆ†æé€»è¾‘çš„è¯¦ç»†è¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š1) ä½¿ç”¨å“ªäº›è¡¨ 2) å¦‚ä½•è”åˆï¼ˆUNION/JOINï¼‰3) ç­›é€‰æ¡ä»¶ 4) åˆ†ç»„ç»´åº¦ 5) èšåˆæŒ‡æ ‡",
  "relevant_columns": [
    {{
      "table_name": "è¡¨åï¼ˆå¤šè¡¨æ—¶å¿…é¡»æŒ‡å®šï¼‰",
      "column_name": "åˆ—åï¼ˆå¿…é¡»ä¸å»ºè¡¨SQLä¸­çš„åˆ—åå®Œå…¨ä¸€è‡´ï¼‰",
      "usage": "ç”¨é€”è¯´æ˜ï¼ˆå¦‚ï¼šç­›é€‰æ¡ä»¶/åˆ†ç»„ç»´åº¦/èšåˆæŒ‡æ ‡ï¼‰"
    }}
  "multi_table_strategy": {{
    "strategy": "UNION_ALL æˆ– JOIN æˆ– SINGLE_TABLE",
    "tables_to_use": ["è¡¨å1", "è¡¨å2"],
    "join_condition": "å¦‚æœæ˜¯JOINï¼Œè¯´æ˜å…³è”æ¡ä»¶ï¼›å¦‚æœæ˜¯UNION_ALLï¼Œè¯´æ˜å­—æ®µæ˜ å°„å…³ç³»"
  }},
  "domain_knowledge": null
}}

**å¤šè¡¨æŸ¥è¯¢ç­–ç•¥è¯´æ˜**ï¼š
- **UNION_ALL**ï¼šå½“å¤šä¸ªè¡¨ç»“æ„ç›¸ä¼¼ï¼Œéœ€è¦åˆå¹¶æ‰€æœ‰æ•°æ®è¿›è¡Œåˆ†ææ—¶ä½¿ç”¨
- **JOIN**ï¼šå½“éœ€è¦å…³è”ä¸åŒè¡¨çš„æ•°æ®æ—¶ä½¿ç”¨
- **SINGLE_TABLE**ï¼šå¦‚æœåªéœ€è¦æŸ¥è¯¢å•ä¸ªè¡¨

**ä¸¥æ ¼è¦æ±‚**ï¼š
- relevant_columns ä¸­çš„ column_name å¿…é¡»**ä¸¥æ ¼ä»ä¸Šé¢æä¾›çš„å»ºè¡¨SQLä¸­é€‰æ‹©**
- å¤šè¡¨æ¨¡å¼ä¸‹ï¼Œå¿…é¡»åœ¨ relevant_columns ä¸­æŒ‡å®š table_name
- **ä¸åŒè¡¨çš„ç›¸åŒå«ä¹‰å­—æ®µåå¯èƒ½ä¸åŒï¼Œå¿…é¡»åˆ†åˆ«åˆ—å‡º**
- **ç¦æ­¢æ¨æµ‹ã€åˆ›é€ æˆ–ç¼–é€ ä»»ä½•ä¸åœ¨å­—æ®µåˆ—è¡¨ä¸­çš„å­—æ®µå**

ç°åœ¨è¯·ç»“åˆå†å²ä¸Šä¸‹æ–‡åŠç”¨æˆ·å½“å‰é—®é¢˜ï¼Œåˆ†æç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè€ƒè™‘å¤šè¡¨è”åˆæŸ¥è¯¢çš„æ–¹å¼ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜å¹¶ç”¨ä¸­æ–‡è¾“å‡ºJSONï¼š
"""

    def _build_single_table_rewrite_prompt_en(
        self, user_query: str, simplified_schema: str, table_description: str,
        sample_rows_str: str, history_context: str
    ) -> str:
        """æ„å»ºå•è¡¨æ¨¡å¼çš„è‹±æ–‡æ”¹å†™prompt"""
        return f"""You are a data analysis expert. The user has asked a data analysis question. You need to:
1. Understand the user's real intent based on historical questions and answers, and enhance the current question
2. Enhance the user's question based on the data table field information
3. Clearly identify the columns that may be used (including filter condition columns, grouping dimension columns, and aggregation indicator columns)
4. Provide 3-5 analysis suggestions explaining how to analyze this question
5. Give a clear analysis logic

=== Data Table Field Information ===
{simplified_schema}

=== Sample Data (2 rows) ===
{sample_rows_str if sample_rows_str else "No sample data available"}

=== Data Table Description ===
{table_description}

{history_context}
=== User's Current Question ===
{user_query}

=== Output Format (JSON) ===
Please strictly follow the following JSON format:
{{
  "is_relevant": true,
  "rewritten_query": "The enhanced complete question, clearly indicating the dimensions and indicators to be analyzed",
  "relevant_columns": [
    {{
      "column_name": "Column name",
      "usage": "Usage description (e.g., filter condition/grouping dimension/aggregation indicator)"
    }}
  ],
  "analysis_suggestions": [
    "Suggestion 1: Specific analysis steps or considerations",
    "Suggestion 2: ...",
    "Suggestion 3: ..."
  ],
  "analysis_logic": "Detailed explanation of analysis logic",
  "domain_knowledge": null
}}

**STRICT REQUIREMENT - Column names must come from the provided list**:
- The column_name in relevant_columns MUST be **strictly selected from the field information provided above**
- **DO NOT guess, create, or fabricate any column names that are not in the field list**

Now please analyze the user's real intent and output JSON IN ENGLISH:
"""

    def _build_multi_table_rewrite_prompt_en(
        self, user_query: str, simplified_schema: str, table_description: str,
        sample_rows_str: str, history_context: str
    ) -> str:
        """æ„å»ºå¤šè¡¨æ¨¡å¼çš„è‹±æ–‡æ”¹å†™prompt"""
        # è§£æ schema ä»¥è·å–å„è¡¨çš„å»ºè¡¨SQLå’Œæ ·æœ¬æ•°æ®
        tables_info_str = self._format_multi_table_info_for_prompt(simplified_schema)
        
        # æå–å­—æ®µå·®å¼‚è­¦å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
        schema_diff_warning = ""
        try:
            schema_obj = json.loads(simplified_schema) if isinstance(simplified_schema, str) else simplified_schema
            if schema_obj.get("schema_differences"):
                # å°†ä¸­æ–‡è­¦å‘Šè½¬æ¢ä¸ºè‹±æ–‡
                diff_text = schema_obj['schema_differences']
                # ç®€å•å¤„ç†ï¼šä¿ç•™åŸæ–‡ï¼Œå› ä¸ºè­¦å‘Šä¸­åŒ…å«å…·ä½“å­—æ®µå
                schema_diff_warning = f"\n\n{diff_text}\n"
        except Exception:
            pass
        
        return f"""You are a data analysis expert. The user has asked a data analysis question that requires **multi-table query**.

**IMPORTANT: This is a multi-table query scenario. You need to consider how to combine multiple tables to answer the user's question.**

=== Available Data Tables ===
{tables_info_str}
{schema_diff_warning}
**Note**:
1. The above contains CREATE TABLE SQL and sample data for each table
2. You need to analyze which tables and fields are relevant to the user's question
3. If cross-table query is needed, consider using UNION ALL to merge similar tables, or JOIN to relate different tables
4. **Different tables may have different column names but similar meanings**, please carefully compare column names and sample data to understand the field mapping
5. **âš ï¸ CRITICAL: When using UNION ALL, ensure that the SELECTed fields exist in ALL tables! If a field only exists in some tables, use NULL or default values to fill in the tables that don't have that field**

=== Data Table Description ===
{table_description}

{history_context}
=== User's Current Question ===
{user_query}

=== Output Format (JSON) ===
Please strictly follow the following JSON format:
{{
  "is_relevant": true,
  "rewritten_query": "The enhanced complete question, clearly indicating which tables to query, dimensions and indicators to analyze",
  "relevant_columns": [
    {{
      "table_name": "Table name (required for multi-table)",
      "column_name": "Column name (must match exactly with CREATE TABLE SQL)",
      "usage": "Usage description (e.g., filter condition/grouping dimension/aggregation indicator)"
    }}
  ],
  "analysis_suggestions": [
    "Suggestion 1: Explain how to combine multiple tables for query",
    "Suggestion 2: If table structures are similar, suggest using UNION ALL to merge before analysis",
    "Suggestion 3: If different tables need to be related, explain the JOIN method",
    "Suggestion 4: ..."
  ],
  "analysis_logic": "Detailed explanation of multi-table analysis logic, including: 1) Which tables to use 2) How to combine (UNION/JOIN) 3) Filter conditions 4) Grouping dimensions 5) Aggregation indicators",
  "multi_table_strategy": {{
    "strategy": "UNION_ALL or JOIN or SINGLE_TABLE",
    "tables_to_use": ["table1", "table2"],
    "join_condition": "If JOIN, explain the join condition; if UNION_ALL, explain the field mapping"
  }},
  "domain_knowledge": null
}}

**Multi-table Query Strategy**:
- **UNION_ALL**: Use when multiple tables have similar structures and need to merge all data for analysis
- **JOIN**: Use when data from different tables needs to be related
- **SINGLE_TABLE**: If only one table needs to be queried

**STRICT REQUIREMENT**:
- The column_name in relevant_columns MUST be **strictly selected from the CREATE TABLE SQL provided above**
- In multi-table mode, table_name MUST be specified in relevant_columns
- **Different tables may have different column names for the same meaning - list them separately**

Now please analyze the user's real intent, consider multi-table query approach, and output JSON IN ENGLISH:
"""

    async def _llm_based_rewrite_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
        sample_rows: list = None,
        invalid_columns_hint: list = None,
        retry_count: int = 0,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæµå¼Queryæ”¹å†™
        
        Args:
            sample_rows: æ ·æœ¬æ•°æ®è¡Œ
            invalid_columns_hint: ä¸Šæ¬¡è¿”å›çš„æ— æ•ˆå­—æ®µååˆ—è¡¨ï¼Œç”¨äºæç¤ºLLMé‡æ–°ç”Ÿæˆ
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunkï¼ˆç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬ï¼‰
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœ
        """
        import inspect

        from dbgpt.core import (
            ModelMessage,
            ModelMessageRoleType,
            ModelRequest,
            ModelRequestContext,
        )

        MAX_RETRY = 2  # æœ€å¤šé‡è¯•2æ¬¡

        # æå–æœ‰æ•ˆå­—æ®µåç”¨äºæ ¡éªŒ
        valid_column_names = self._extract_valid_column_names(table_schema_json)
        if valid_column_names:
            if isinstance(valid_column_names, dict):
                # å¤šè¡¨æ¨¡å¼
                total_cols = sum(len(cols) for cols in valid_column_names.values())
                logger.info(f"âœ… å¤šè¡¨æ¨¡å¼ï¼šæå–åˆ° {len(valid_column_names)} ä¸ªè¡¨ï¼Œå…± {total_cols} ä¸ªå­—æ®µç”¨äºæ ¡éªŒ")
                for tbl_name, tbl_cols in list(valid_column_names.items())[:3]:
                    logger.debug(f"  è¡¨ '{tbl_name}': {len(tbl_cols)} ä¸ªå­—æ®µ")
            else:
                # å•è¡¨æ¨¡å¼
                logger.info(f"âœ… å•è¡¨æ¨¡å¼ï¼šæå–åˆ° {len(valid_column_names)} ä¸ªæœ‰æ•ˆå­—æ®µåç”¨äºæ ¡éªŒ")
                logger.debug(f"æœ‰æ•ˆå­—æ®µåï¼ˆå‰10ä¸ªï¼‰: {list(valid_column_names)[:10]}")
        else:
            logger.warning(f"âš ï¸ æœªæå–åˆ°æœ‰æ•ˆå­—æ®µåï¼Œå°†è·³è¿‡å­—æ®µåæ ¡éªŒ")

        # æ£€æµ‹ç”¨æˆ·è¯­è¨€
        user_language = detect_language(user_query)

        # æ„å»ºprompt
        prompt = self._build_rewrite_prompt(
            user_query, table_schema_json, table_description, 
            chat_history=chat_history, sample_rows=sample_rows
        )
        
        # å¦‚æœæœ‰æ— æ•ˆå­—æ®µæç¤ºï¼Œè¿½åŠ åˆ°promptä¸­ï¼ˆæ ¹æ®è¯­è¨€é€‰æ‹©ï¼‰
        if invalid_columns_hint:
            # è§£ææ— æ•ˆå­—æ®µåˆ—è¡¨å¹¶æ‰¾ç›¸ä¼¼å­—æ®µ
            similar_suggestions = []
            import re
            
            # å¤„ç†ä¸åŒæ ¼å¼çš„invalid_columns_hint
            if isinstance(invalid_columns_hint, list):
                invalid_fields = invalid_columns_hint
            elif isinstance(invalid_columns_hint, str):
                # ä»å­—ç¬¦ä¸²ä¸­æå–å­—æ®µåï¼ˆæ ¼å¼ï¼š['å­—æ®µ1', 'å­—æ®µ2']ï¼‰
                invalid_fields = re.findall(r"'([^']+)'", invalid_columns_hint)
            else:
                invalid_fields = []
            
            if invalid_fields:
                for invalid_field in invalid_fields:
                    # æå–è¡¨åå’Œå­—æ®µå
                    table_match = re.search(r'\(å­—æ®µä¸å­˜åœ¨äºè¡¨\s*[\'"]?([^\'"ï¼‰]+)[\'"]?\s*ä¸­\)', invalid_field)
                    if table_match:
                        specified_table = table_match.group(1)
                        # å»æ‰è¡¨ååç¼€ï¼Œè·å–çº¯å­—æ®µå
                        pure_field = re.sub(r'\s*\(å­—æ®µä¸å­˜åœ¨äºè¡¨.*\)', '', invalid_field)
                    else:
                        specified_table = None
                        pure_field = invalid_field
                    
                    # æŸ¥æ‰¾ç›¸ä¼¼å­—æ®µ
                    similar_cols = self._find_similar_columns(
                        pure_field, 
                        valid_column_names,
                        specified_table,
                        top_k=3
                    )
                    
                    if similar_cols:
                        # è¿‡æ»¤ç›¸ä¼¼åº¦>0.6çš„å­—æ®µ
                        good_matches = [c for c in similar_cols if c['similarity'] > 0.6]
                        if good_matches:
                            if user_language == "zh":
                                suggestion = f"  â€¢ æ— æ•ˆå­—æ®µï¼š{invalid_field}\n    ç›¸ä¼¼å­—æ®µæ¨èï¼š"
                                for match in good_matches:
                                    suggestion += f"\n      - {match['table_name']}.{match['column_name']} (ç›¸ä¼¼åº¦: {match['similarity']:.2f})"
                            else:
                                suggestion = f"  â€¢ Invalid field: {invalid_field}\n    Similar field suggestions:"
                                for match in good_matches:
                                    suggestion += f"\n      - {match['table_name']}.{match['column_name']} (similarity: {match['similarity']:.2f})"
                            similar_suggestions.append(suggestion)
            
            # å°†invalid_columns_hintè½¬æ¢ä¸ºæ˜¾ç¤ºæ ¼å¼
            if isinstance(invalid_columns_hint, list):
                invalid_columns_display = str(invalid_columns_hint)
            else:
                invalid_columns_display = invalid_columns_hint
            
            if user_language == "zh":
                correction_hint = f"""

**é”™è¯¯çº æ­£**ï¼š
ä½ ä¸Šæ¬¡è¿”å›çš„ä»¥ä¸‹å­—æ®µååœ¨æ•°æ®è¡¨ä¸­ä¸å­˜åœ¨ï¼Œè¯·å‹¿ä½¿ç”¨ï¼š
{invalid_columns_display}
"""
                if similar_suggestions:
                    correction_hint += "\n**ç›¸ä¼¼å­—æ®µæ¨è**ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ç®—æ³•ï¼‰ï¼š\n"
                    correction_hint += "\n".join(similar_suggestions)
                    correction_hint += "\n\nè¯·å‚è€ƒä¸Šé¢æ¨èçš„ç›¸ä¼¼å­—æ®µï¼Œæˆ–ä»å­—æ®µåˆ—è¡¨ä¸­é€‰æ‹©å…¶ä»–åˆé€‚çš„å­—æ®µã€‚"
                else:
                    correction_hint += "\nè¯·ä»”ç»†æ£€æŸ¥ä¸Šé¢æä¾›çš„å­—æ®µåˆ—è¡¨ï¼Œåªä½¿ç”¨å®é™…å­˜åœ¨çš„å­—æ®µåé‡æ–°ç”Ÿæˆã€‚å¦‚æœæ‰¾ä¸åˆ°å¯¹åº”çš„å­—æ®µï¼Œè¯·åœ¨analysis_suggestionsä¸­è¯´æ˜è¯¥åˆ†æå¯èƒ½æ— æ³•å®Œæˆã€‚"
            else:
                correction_hint = f"""

**Error Correction**:
The following column names you returned do not exist in the data table, DO NOT use them:
{invalid_columns_display}
"""
                if similar_suggestions:
                    correction_hint += "\n**Similar Field Suggestions** (based on edit distance algorithm):\n"
                    correction_hint += "\n".join(similar_suggestions)
                    correction_hint += "\n\nPlease refer to the similar fields recommended above, or choose other appropriate fields from the field list."
                else:
                    correction_hint += "\nPlease carefully check the field list provided above and regenerate using only column names that actually exist. If you cannot find the corresponding field, please explain in analysis_suggestions that this analysis may not be possible."
            
            prompt += correction_hint
            logger.info(f"ğŸ”„ é‡è¯•ç¬¬{retry_count}æ¬¡ï¼Œæ·»åŠ æ— æ•ˆå­—æ®µæç¤ºåŠç›¸ä¼¼å­—æ®µæ¨è")
        
        logger.debug(f"ğŸ” query_rewrite_agent prompt: {prompt[:200]}...")
        
        # è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
        request_params = {
            "messages": [ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)],
            "max_new_tokens": 2000,
            "context": ModelRequestContext(stream=True),
        }

        # å¦‚æœæœ‰model_nameï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if self.model_name:
            request_params["model"] = self.model_name

        request = ModelRequest(**request_params)

        # è·å–æµå¼å“åº”
        stream_response = self.llm_client.generate_stream(request)

        full_text = ""
        if inspect.isasyncgen(stream_response):
            async for chunk in stream_response:
                # å®‰å…¨åœ°è·å–æ–‡æœ¬å†…å®¹
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            # å¯èƒ½åªæœ‰ thinking å†…å®¹ï¼Œç»§ç»­ç­‰å¾… text å†…å®¹
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        elif inspect.isgenerator(stream_response):
            for chunk in stream_response:
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        else:
            raise Exception(f"Unexpected response type: {type(stream_response)}")

        # æµå¼è¾“å‡ºå®Œæˆåï¼Œè§£æç»“æœå¹¶è¿”å›ï¼ˆå¸¦å­—æ®µåæ ¡éªŒï¼‰
        if full_text:
            try:
                result = self._parse_rewrite_result(
                    full_text, user_query, valid_column_names=valid_column_names
                )
                yield result
            except InvalidColumnError as e:
                # å­—æ®µåæ— æ•ˆï¼Œå°è¯•é‡è¯•
                if retry_count < MAX_RETRY:
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆå­—æ®µåï¼Œè§¦å‘é‡è¯• ({retry_count + 1}/{MAX_RETRY})")
                    # é€’å½’è°ƒç”¨ï¼Œä¼ å…¥æ— æ•ˆå­—æ®µæç¤º
                    async for chunk in self._llm_based_rewrite_stream(
                        user_query,
                        table_schema_json,
                        table_description,
                        chat_history=chat_history,
                        sample_rows=sample_rows,
                        invalid_columns_hint=e.invalid_columns,
                        retry_count=retry_count + 1,
                    ):
                        yield chunk
                else:
                    logger.error(f"âŒ é‡è¯•{MAX_RETRY}æ¬¡åä»æœ‰æ— æ•ˆå­—æ®µï¼Œæ”¾å¼ƒé‡è¯•")
                    raise JSONParseError(str(e))
            except JSONParseError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                raise

    def _parse_rewrite_result(
        self, llm_output: str, original_query: str, valid_column_names: Union[set, Dict[str, set]] = None
    ) -> Dict:
        """
        è§£æLLMè¾“å‡ºçš„JSONç»“æœ

        å¦‚æœè§£æå¤±è´¥æˆ–å­—æ®µåä¸å­˜åœ¨ï¼ŒæŠ›å‡º JSONParseError å¼‚å¸¸ä»¥è§¦å‘é‡è¯•æœºåˆ¶
        
        Args:
            llm_output: LLMè¾“å‡ºçš„æ–‡æœ¬
            original_query: åŸå§‹ç”¨æˆ·é—®é¢˜
            valid_column_names: æœ‰æ•ˆçš„å­—æ®µåï¼Œå•è¡¨æ¨¡å¼ä¸ºsetï¼Œå¤šè¡¨æ¨¡å¼ä¸ºdict {table_name: set(columns)}
        """
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = llm_output.find("{")
            end_idx = llm_output.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
                if "is_relevant" not in result:
                    logger.warning("JSONç¼ºå°‘ 'is_relevant' å­—æ®µï¼Œé»˜è®¤è®¾ä¸º true")
                    result["is_relevant"] = True
                
                if not result.get("rewritten_query"):
                    logger.error("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")
                    raise JSONParseError("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")

                # éªŒè¯ relevant_columns æ ¼å¼å’Œå­—æ®µåæ˜¯å¦å­˜åœ¨
                relevant_columns = result.get("relevant_columns", [])
                if relevant_columns and valid_column_names:
                    invalid_columns = []
                    is_multi_table = isinstance(valid_column_names, dict)
                    
                    for idx, col in enumerate(relevant_columns):
                        if not isinstance(col, dict) or "column_name" not in col:
                            logger.error(f"relevant_columns[{idx}] æ ¼å¼é”™è¯¯: {col}")
                            raise JSONParseError(
                                f"relevant_columns[{idx}] ç¼ºå°‘ 'column_name' å­—æ®µ"
                            )
                        
                        col_name = col.get("column_name", "")
                        table_name = col.get("table_name", "")
                        
                        if not col_name:
                            continue
                        
                        # å¤šè¡¨æ¨¡å¼ï¼šéœ€è¦æ ¡éªŒå­—æ®µæ˜¯å¦å­˜åœ¨äºæŒ‡å®šçš„è¡¨ä¸­
                        if is_multi_table:
                            if not table_name:
                                # å¤šè¡¨æ¨¡å¼ä¸‹å¿…é¡»æŒ‡å®šè¡¨å
                                logger.warning(f"å¤šè¡¨æ¨¡å¼ä¸‹å­—æ®µ '{col_name}' æœªæŒ‡å®š table_name")
                                # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨äºä»»æ„è¡¨ä¸­
                                found_in_any_table = False
                                for tbl_name, tbl_cols in valid_column_names.items():
                                    if col_name in tbl_cols:
                                        found_in_any_table = True
                                        logger.info(f"å­—æ®µ '{col_name}' å­˜åœ¨äºè¡¨ '{tbl_name}' ä¸­")
                                        break
                                
                                if not found_in_any_table:
                                    invalid_columns.append(f"{col_name} (æœªæŒ‡å®šè¡¨åä¸”ä¸å­˜åœ¨äºä»»ä½•è¡¨ä¸­)")
                            else:
                                # æ£€æŸ¥æŒ‡å®šçš„è¡¨æ˜¯å¦å­˜åœ¨
                                if table_name not in valid_column_names:
                                    invalid_columns.append(f"{table_name}.{col_name} (è¡¨ '{table_name}' ä¸å­˜åœ¨)")
                                    logger.warning(f"è¡¨ '{table_name}' ä¸å­˜åœ¨äºæœ‰æ•ˆè¡¨åˆ—è¡¨ä¸­")
                                else:
                                    # æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨äºæŒ‡å®šçš„è¡¨ä¸­
                                    table_cols = valid_column_names[table_name]
                                    if col_name not in table_cols:
                                        invalid_columns.append(f"{col_name} (å­—æ®µä¸å­˜åœ¨äºè¡¨ '{table_name}' ä¸­)")
                                        logger.warning(f"å­—æ®µ '{col_name}' ä¸å­˜åœ¨äºè¡¨ '{table_name}' ä¸­")
                                        logger.debug(f"è¡¨ '{table_name}' çš„æœ‰æ•ˆå­—æ®µï¼ˆå‰10ä¸ªï¼‰: {list(table_cols)[:10]}")
                        else:
                            # å•è¡¨æ¨¡å¼ï¼šç›´æ¥æ ¡éªŒå­—æ®µå
                            pure_col_name = col_name
                            if "." in col_name:
                                parts = col_name.split(".", 1)
                                if len(parts) == 2:
                                    pure_col_name = parts[1].strip('"').strip("'")
                            
                            if pure_col_name not in valid_column_names and col_name not in valid_column_names:
                                invalid_columns.append(col_name)
                                logger.warning(f"å­—æ®µåæ ¡éªŒå¤±è´¥: '{col_name}' ä¸åœ¨æœ‰æ•ˆå­—æ®µåˆ—è¡¨ä¸­")
                    
                    # å¦‚æœæœ‰æ— æ•ˆå­—æ®µåï¼ŒæŠ›å‡º InvalidColumnError è§¦å‘é‡è¯•
                    if invalid_columns:
                        error_msg = f"ä»¥ä¸‹å­—æ®µåä¸å­˜åœ¨äºæ•°æ®è¡¨ä¸­: {invalid_columns}"
                        logger.warning(f"âš ï¸ å­—æ®µæ ¡éªŒå¤±è´¥: {error_msg}ï¼Œå°†è§¦å‘é‡è¯•")
                        raise InvalidColumnError(error_msg, invalid_columns)
    
                # æ·»åŠ åŸå§‹é—®é¢˜
                result["original_query"] = original_query

                # æå–é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚æœæœ‰ï¼‰
                domain_knowledge = result.get("domain_knowledge")
                if domain_knowledge and isinstance(domain_knowledge, dict):
                    column_name = domain_knowledge.get("column_name")
                    knowledge = domain_knowledge.get("knowledge")
                    if column_name and knowledge:
                        result["_extracted_knowledge"] = {
                            "column_name": column_name,
                            "knowledge": knowledge,
                        }

                logger.info("âœ… JSONè§£ææˆåŠŸï¼Œå­—æ®µæ ¡éªŒé€šè¿‡")
                return result
            else:
                logger.error("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSON")
                raise JSONParseError("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSONå†…å®¹")

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            raise JSONParseError(f"JSONè§£æå¤±è´¥: {e}")

    def _default_result(self, original_query: str) -> Dict:
        """
        è¿”å›é»˜è®¤ç»“æœï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰
        """
        return {
            "original_query": original_query,
            "is_relevant": True,
            "rewritten_query": original_query,
            "relevant_columns": [],
            "analysis_suggestions": ["è¯·æ˜ç¡®éœ€è¦åˆ†æçš„æ•°æ®ç»´åº¦å’ŒæŒ‡æ ‡"],
            "analysis_logic": "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ•°æ®åˆ†æ",
        }
