"""
Queryæ”¹å†™Agent - å‚è€ƒformat_sql/backend/agents/query_rewrite_assistant.py
è´Ÿè´£æ ¹æ®æ•°æ®ç†è§£ä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç›¸å…³åˆ—å’Œåˆ†æå»ºè®®
"""

import json
import logging
from typing import Dict, List, Optional
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)

logger = logging.getLogger(__name__)


class JSONParseError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šJSONè§£æå¤±è´¥"""

    pass


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€

    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬

    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€
    """
    if not text or not text.strip():
        return "en"

    import re

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_pattern = re.compile(r"[\u4e00-\u9fff]+")
    chinese_chars = chinese_pattern.findall(text)
    chinese_count = sum(len(match) for match in chinese_chars)

    # è®¡ç®—æ€»å­—ç¬¦æ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    total_chars = len(re.sub(r'[\s\.,;:!?\'"\-_()\[\]{}]', "", text))

    if total_chars == 0:
        return "en"

    # è®¡ç®—ä¸­æ–‡å æ¯”
    chinese_ratio = chinese_count / total_chars if total_chars > 0 else 0

    # å¦‚æœä¸­æ–‡å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡è¾“å…¥
    return "zh" if chinese_ratio > 0.3 else "en"


class QueryRewriteAgent:
    """
    Queryæ”¹å†™Agent - ç®€åŒ–ç‰ˆæœ¬
    åŠŸèƒ½ï¼š
    1. æ ¹æ®æ•°æ®å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„æé—®
    2. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—
    3. æä¾›åˆ†æå»ºè®®å’Œé€»è¾‘æ”¯æ’‘
    """

    def __init__(self, llm_client=None, model_name=None):
        """
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›é»˜è®¤ç»“æœï¼‰
            model_name: æ¨¡å‹åç§°
        """
        self.llm_client = llm_client
        self.model_name = model_name

    def rewrite_query(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> Dict[str, any]:
        """
        æ”¹å†™ç”¨æˆ·query

        ä¼˜å…ˆä½¿ç”¨LLMæ”¹å†™ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰ï¼Œå¦‚æœLLMä¸å¯ç”¨æˆ–é‡è¯•3æ¬¡åä»å¤±è´¥åˆ™ä½¿ç”¨è§„åˆ™æ”¹å†™
        """
        logger.info(f"Queryæ”¹å†™ - åŸå§‹é—®é¢˜: {user_query}")

        # å°è¯•ä½¿ç”¨LLMæ”¹å†™
        if self.llm_client:
            try:
                logger.info("å°è¯•ä½¿ç”¨LLMè¿›è¡ŒQueryæ”¹å†™ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼Œæœ€å¤š3æ¬¡ï¼‰...")
                result = self._llm_based_rewrite(
                    user_query,
                    table_schema_json,
                    table_description,
                    chat_history=chat_history,
                )
                logger.info(f"âœ… LLMæ”¹å†™æˆåŠŸ - æ”¹å†™ç»“æœ: {result['rewritten_query']}")
                return result
            except JSONParseError as e:
                logger.error(f"âŒ LLMæ”¹å†™å¤±è´¥ï¼ˆJSONè§£æé”™è¯¯ï¼Œå·²é‡è¯•3æ¬¡ï¼‰: {e}")
                logger.warning("âš ï¸ ä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
        else:
            logger.info("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™")

        # Fallback: åŸºäºè§„åˆ™çš„ç®€å•æ”¹å†™
        result = self._rule_based_rewrite(user_query, table_schema_json)
        logger.info(f"è§„åˆ™æ”¹å†™ - æ”¹å†™ç»“æœ: {result['rewritten_query']}")

        return result

    def _rule_based_rewrite(self, user_query: str, table_schema_json: str) -> Dict:
        """
        åŸºäºè§„åˆ™çš„Queryæ”¹å†™ï¼ˆä¸ä¾èµ–LLMï¼Œå¿«é€Ÿå¯é ï¼‰
        """
        try:
            # è§£æschema JSON
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            # è·å–åˆ—ä¿¡æ¯
            schema_data = (
                schema_obj.get("columns", []) if isinstance(schema_obj, dict) else []
            )

            relevant_columns = []
            analysis_suggestions = []

            # åˆ†æç”¨æˆ·queryä¸­æåˆ°çš„å…³é”®è¯
            query_lower = user_query.lower()

            # æ£€æµ‹æ—¶é—´ç›¸å…³çš„åˆ†æ
            if any(
                keyword in query_lower for keyword in ["åŒæ¯”", "ç¯æ¯”", "yoy", "mom"]
            ):
                # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
                date_cols = [
                    col
                    for col in schema_data
                    if any(
                        kw in col.get("column_name", "").lower()
                        for kw in ["date", "æ—¥æœŸ", "time", "æ—¶é—´"]
                    )
                ]
                if date_cols:
                    relevant_columns.append(
                        {
                            "column_name": date_cols[0]["column_name"],
                            "usage": "æ—¶é—´ç­›é€‰å’Œåˆ†ç»„æ¡ä»¶ï¼Œéœ€è¦åŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®",
                        }
                    )

                analysis_suggestions.append(
                    "åŒæ¯”åˆ†æéœ€è¦å»å¹´åŒæœŸæ•°æ®ï¼Œç¡®ä¿WHEREæ¡ä»¶åŒ…å«è‡³å°‘ä¸¤å¹´çš„æ•°æ®"
                )
                analysis_suggestions.append(
                    "ç¯æ¯”åˆ†æéœ€è¦ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®ï¼Œä½¿ç”¨LAGçª—å£å‡½æ•°"
                )
                analysis_suggestions.append(
                    "æ—¶é—´èŒƒå›´ç¤ºä¾‹ï¼šWHERE è®¢å•æ—¥æœŸ >= '2021-01-01' AND è®¢å•æ—¥æœŸ < '2023-01-01'"
                )

            # æ£€æµ‹åœ°åŸŸåˆ†æ
            if any(
                keyword in user_query
                for keyword in [
                    "åŒºåŸŸ",
                    "åœ°åŒº",
                    "ååŒ—",
                    "åä¸œ",
                    "åå—",
                    "ä¸œåŒ—",
                    "è¥¿åŒ—",
                    "è¥¿å—",
                    "ä¸­å—",
                ]
            ):
                region_cols = [
                    col for col in schema_data if "åŒºåŸŸ" in col.get("column_name", "")
                ]
                if region_cols:
                    relevant_columns.append(
                        {
                            "column_name": region_cols[0]["column_name"],
                            "usage": "åœ°åŸŸç­›é€‰æˆ–åˆ†ç»„æ¡ä»¶",
                        }
                    )
                    analysis_suggestions.append(f"ä½¿ç”¨'åŒºåŸŸ'å­—æ®µè¿›è¡Œç­›é€‰æˆ–åˆ†ç»„")

            # æ£€æµ‹æŒ‡æ ‡åˆ†æ
            metric_keywords = {
                "åˆ©æ¶¦": "profit",
                "é”€å”®é¢": "sales",
                "é”€é‡": "quantity",
                "æ•°é‡": "quantity",
            }

            for cn_keyword, en_keyword in metric_keywords.items():
                if cn_keyword in user_query:
                    metric_cols = [
                        col
                        for col in schema_data
                        if cn_keyword in col.get("column_name", "")
                    ]
                    if metric_cols:
                        relevant_columns.append(
                            {
                                "column_name": metric_cols[0]["column_name"],
                                "usage": "èšåˆæŒ‡æ ‡ï¼Œéœ€è¦è¿›è¡ŒSUM/AVGç­‰èšåˆè¿ç®—",
                            }
                        )
                        analysis_suggestions.append(
                            f"å¯¹'{cn_keyword}'å­—æ®µè¿›è¡Œèšåˆè®¡ç®—ï¼ˆSUMæ±‚å’Œï¼‰"
                        )

            # æ„å»ºæ”¹å†™åçš„query
            rewritten_query = self._enhance_query(
                user_query, relevant_columns, analysis_suggestions
            )

            # æ„å»ºåˆ†æé€»è¾‘
            analysis_logic = self._build_analysis_logic(user_query, relevant_columns)

            return {
                "original_query": user_query,
                "rewritten_query": rewritten_query,
                "relevant_columns": relevant_columns,
                "analysis_suggestions": analysis_suggestions,
                "analysis_logic": analysis_logic,
            }

        except Exception as e:
            logger.error(f"è§„åˆ™æ”¹å†™å¤±è´¥: {e}", exc_info=True)
            return self._default_result(user_query)

    def _enhance_query(
        self, user_query: str, relevant_columns: List[Dict], suggestions: List[str]
    ) -> str:
        """
        å¢å¼ºç”¨æˆ·query
        """
        if not relevant_columns:
            return user_query

        # æ·»åŠ æ˜ç¡®çš„å­—æ®µå¼•ç”¨
        col_names = [col["column_name"] for col in relevant_columns]
        enhanced = user_query

        # å¦‚æœæ˜¯ç®€çŸ­çš„queryï¼Œæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
        if len(user_query) < 20:
            enhanced = f"åŸºäºæ•°æ®è¡¨ï¼Œä½¿ç”¨å­—æ®µã€{', '.join(col_names)}ã€‘æ¥{user_query}"

        return enhanced

    def _build_analysis_logic(
        self, user_query: str, relevant_columns: List[Dict]
    ) -> str:
        """
        æ„å»ºåˆ†æé€»è¾‘
        """
        logic_parts = []

        # ç­›é€‰æ¡ä»¶
        filter_cols = [
            col for col in relevant_columns if "ç­›é€‰" in col.get("usage", "")
        ]
        if filter_cols:
            logic_parts.append(
                f"1. ç­›é€‰æ¡ä»¶ï¼š{', '.join([c['column_name'] for c in filter_cols])}"
            )

        # åˆ†ç»„ç»´åº¦
        group_cols = [col for col in relevant_columns if "åˆ†ç»„" in col.get("usage", "")]
        if group_cols:
            logic_parts.append(
                f"2. åˆ†ç»„ç»´åº¦ï¼š{', '.join([c['column_name'] for c in group_cols])}"
            )

        # èšåˆæŒ‡æ ‡
        agg_cols = [col for col in relevant_columns if "èšåˆ" in col.get("usage", "")]
        if agg_cols:
            logic_parts.append(
                f"3. èšåˆæŒ‡æ ‡ï¼š{', '.join([c['column_name'] for c in agg_cols])}"
            )

        if logic_parts:
            return "\n".join(logic_parts)
        else:
            return "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ ‡å‡†çš„æ•°æ®æŸ¥è¯¢å’Œåˆ†æ"

    def _build_rewrite_prompt(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> str:
        """
        æ„å»ºæ”¹å†™prompt
        """
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n=== å†å²å¯¹è¯ä¸Šä¸‹æ–‡ ===\n"
            # åªä¿ç•™æœ€è¿‘4è½®å¯¹è¯ï¼ˆ8æ¡æ¶ˆæ¯ï¼‰ï¼Œé¿å…promptè¿‡é•¿
            recent_history = (
                chat_history[-8:] if len(chat_history) > 8 else chat_history
            )

            for msg in recent_history:
                role = msg.get("role", "user") if isinstance(msg, dict) else "user"
                content = (
                    str(msg.get("content", "")) if isinstance(msg, dict) else str(msg)
                )

                # æ ¹æ®å®é™…è§’è‰²æ˜¾ç¤º
                if "human" in role.lower() or "user" in role.lower():
                    role_display = "ç”¨æˆ·"
                elif "ai" in role.lower() or "assistant" in role.lower():
                    role_display = "åŠ©æ‰‹"
                else:
                    role_display = role

                history_context += f"\n{role_display}: {content}\n"

        # æ£€æµ‹ç”¨æˆ·è¾“å…¥è¯­è¨€
        user_language = detect_language(user_query)
        logger.info(f"ğŸŒ Queryæ”¹å†™ - æ£€æµ‹åˆ°ç”¨æˆ·è¾“å…¥è¯­è¨€: {user_language}")

        # æ ¹æ®è¯­è¨€é€‰æ‹©prompt
        if user_language == "zh":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆ†æé—®é¢˜ï¼Œä½ éœ€è¦ï¼š
1. æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå›ç­”ï¼Œå……åˆ†ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜
2. æ ¹æ®æ•°æ®è¡¨çš„å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„é—®é¢˜
3. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—ï¼ˆåŒ…æ‹¬ç­›é€‰æ¡ä»¶åˆ—ã€åˆ†ç»„ç»´åº¦åˆ—ã€èšåˆæŒ‡æ ‡åˆ—ï¼‰
4. æä¾›3-5æ¡åˆ†æå»ºè®®ï¼Œè¯´æ˜å¦‚ä½•åˆ†æè¿™ä¸ªé—®é¢˜
5. ç»™å‡ºæ¸…æ™°çš„åˆ†æé€»è¾‘
6. å¦‚æœç”¨æˆ·åœ¨å¯¹è¯æˆ–å½“å‰å†å²é—®ç­”ä¸Šä¸‹æ–‡ä¸­çº æ­£æˆ–è¡¥å……äº†å­—æ®µçš„ä½¿ç”¨æ–¹æ³•ã€ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰å…³é”®çŸ¥è¯†ï¼Œè¯·æå–å¹¶è®°å½•ä½œä¸ºdomain_knowledgeå­—æ®µ

=== æ•°æ®è¡¨å­—æ®µè¯¦ç»†ä¿¡æ¯ ===
{table_schema_json}

**æ³¨æ„**ï¼šå­—æ®µä¿¡æ¯ä¸­å¯èƒ½åŒ…å« `domain_knowledge` å­—æ®µï¼Œè¿™æ˜¯ä¹‹å‰ä»ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ åˆ°çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä½¿ç”¨ã€‚

=== æ•°æ®è¡¨æè¿° ===
{table_description}

{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{user_query}

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "rewritten_query": "æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼Œæ˜ç¡®æŒ‡å‡ºéœ€è¦åˆ†æçš„ç»´åº¦å’ŒæŒ‡æ ‡",
  "relevant_columns": [
    {{
      "column_name": "åˆ—å",
      "usage": "ç”¨é€”è¯´æ˜ï¼ˆå¦‚ï¼šç­›é€‰æ¡ä»¶/åˆ†ç»„ç»´åº¦/èšåˆæŒ‡æ ‡ï¼‰"
    }}
  ],
  "analysis_suggestions": [
    "å»ºè®®1ï¼šå…·ä½“çš„åˆ†ææ­¥éª¤æˆ–æ³¨æ„äº‹é¡¹",
    "å»ºè®®2ï¼š...",
    "å»ºè®®3ï¼š..."
  ],
  "analysis_logic": "åˆ†æé€»è¾‘çš„è¯¦ç»†è¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š1) éœ€è¦ç­›é€‰å“ªäº›æ•°æ® 2) æŒ‰ä»€ä¹ˆç»´åº¦åˆ†ç»„ 3) è®¡ç®—å“ªäº›æŒ‡æ ‡ 4) å¦‚ä½•æ’åºæˆ–å¯¹æ¯”",
  "domain_knowledge": {{
    "column_name": "å­—æ®µåï¼ˆå¦‚æœç”¨æˆ·çº æ­£æˆ–è¡¥å……äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•ï¼‰",
    "knowledge": "ç”¨æˆ·è¡¥å……çš„ä¸šåŠ¡çŸ¥è¯†æˆ–æ•°æ®å¤„ç†æŠ€å·§ï¼ˆä¾‹å¦‚ï¼š'è¯¥å­—æ®µæ ¼å¼ä¸ºH1,H2ï¼Œé€—å·å‰æ˜¯H1ç»©æ•ˆï¼Œé€—å·åæ˜¯H2ç»©æ•ˆï¼Œéœ€è¦ç”¨SPLIT_PARTå‡½æ•°åˆ†å‰²'ï¼‰"
  }}
}}

**å…³äº domain_knowledge çš„è¯´æ˜**ï¼š
- åªæœ‰å½“ç”¨æˆ·æ˜ç¡®çº æ­£ã€è¡¥å……æˆ–è¯´æ˜äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•æ—¶æ‰éœ€è¦å¡«å†™
- å¦‚æœç”¨æˆ·åªæ˜¯æ™®é€šæé—®ï¼Œä¸éœ€è¦å¡«å†™æ­¤å­—æ®µï¼ˆå¯ä»¥çœç•¥æˆ–è®¾ä¸º nullï¼‰
- çŸ¥è¯†åº”è¯¥æ˜¯å¯å¤ç”¨çš„ã€å¯¹æœªæ¥åˆ†ææœ‰å¸®åŠ©çš„å…³é”®ä¿¡æ¯,æ¯”å¦‚ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰,è¿™éƒ¨åˆ†çŸ¥è¯†ä¼šä½œä¸ºé¢†åŸŸçŸ¥è¯†ä¿å­˜åˆ°æ•°æ®åº“ä¸­,ç”¨äºåç»­çš„åˆ†æå’Œæ¨ç†,å¯ä»¥å¤ç”¨è¿™äº›çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœä¸Šé¢å·²ç»è®°å½•äº†é¢†åŸŸçŸ¥è¯†ï¼Œè¯·ä¸è¦é‡å¤è®°å½•ã€‚

**é‡è¦ - è¯­è¨€è¦æ±‚**ï¼š
- ç”¨æˆ·çš„é—®é¢˜æ˜¯**ä¸­æ–‡**
- ä½ å¿…é¡»ç”¨**ä¸­æ–‡**å›å¤JSONä¸­çš„æ‰€æœ‰å­—æ®µ
- "rewritten_query"ã€"usage"ã€"analysis_suggestions"ã€"analysis_logic" ç­‰å­—æ®µå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**
- å³ä½¿è¡¨å­—æ®µåæ˜¯ä¸­è‹±æ–‡æ··åˆï¼Œä½ çš„æè¿°å’Œåˆ†æä¹Ÿå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**

ç°åœ¨è¯·ç»“åˆå†å²ä¸Šä¸‹æ–‡åŠç”¨æˆ·å½“å‰é—®é¢˜ï¼Œåˆ†æç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜å¹¶ç”¨ä¸­æ–‡è¾“å‡ºJSONï¼š
"""
        else:
            prompt = f"""You are a data analysis expert. The user has asked a data analysis question. You need to:
1. Understand the user's real intent based on historical questions and answers, and enhance the current question
2. Enhance the user's question based on the data table field information
3. Clearly identify the columns that may be used (including filter condition columns, grouping dimension columns, and aggregation indicator columns)
4. Provide 3-5 analysis suggestions explaining how to analyze this question
5. Give a clear analysis logic
6. If the user has corrected or supplemented field usage methods, business rules, data processing techniques, or other key knowledge in the conversation or current historical Q&A context, extract and record it as the domain_knowledge field

=== Data Table Field Details ===
{table_schema_json}

**Note**: The field information may contain a `domain_knowledge` field, which is business knowledge learned from previous user conversations. Please prioritize using this.

=== Data Table Description ===
{table_description}

{history_context}
=== User's Current Question ===
{user_query}

=== Output Format (JSON) ===
Please strictly follow the following JSON format:
{{
  "rewritten_query": "The enhanced complete question, clearly indicating the dimensions and indicators to be analyzed",
  "relevant_columns": [
    {{
      "column_name": "Column name",
      "usage": "Usage description (e.g., filter condition/grouping dimension/aggregation indicator)"
    }}
  ],
  "analysis_suggestions": [
    "Suggestion 1: Specific analysis steps or considerations",
    "Suggestion 2: ...",
    "Suggestion 3: ..."
  ],
  "analysis_logic": "Detailed explanation of analysis logic, including: 1) Which data to filter 2) What dimension to group by 3) Which indicators to calculate 4) How to sort or compare",
  "domain_knowledge": {{
    "column_name": "Field name (if the user has corrected or supplemented the usage method of a field)",
    "knowledge": "Business knowledge or data processing techniques supplemented by the user (e.g., 'This field format is H1,H2, before the comma is H1 performance, after the comma is H2 performance, need to use SPLIT_PART function to split')"
  }}
}}

**About domain_knowledge**:
- Only fill in when the user explicitly corrects, supplements, or explains the usage method of a field
- If the user is just asking a normal question, this field is not required (can be omitted or set to null)
- Knowledge should be reusable and helpful for future analysis, such as business rules, data processing techniques, etc. This knowledge will be saved to the database as domain knowledge for subsequent analysis and reasoning, and can be reused to answer user questions. If domain knowledge has already been recorded above, please do not repeat it.

**IMPORTANT - Language Requirement**:
- The user's question is in ENGLISH
- You MUST respond in ENGLISH for ALL fields in the JSON output
- The "rewritten_query", "usage", "analysis_suggestions", and "analysis_logic" fields MUST be in ENGLISH
- Even though the table field names are in Chinese, your descriptions and analysis MUST be in ENGLISH

Now please combine the historical context and the user's current question, analyze the user's real intent, enhance the current question and output JSON IN ENGLISH:
"""
        return prompt

    @retry(
        retry=retry_if_exception_type(JSONParseError),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=4),
        reraise=True,
        before_sleep=lambda retry_state: logger.warning(
            f"ğŸ”„ JSONè§£æå¤±è´¥ï¼Œç¬¬ {retry_state.attempt_number} æ¬¡é‡è¯•..."
        ),
    )
    def _llm_based_rewrite(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> Dict:
        """
        ä½¿ç”¨LLMè¿›è¡ŒQueryæ”¹å†™ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        å¦‚æœJSONè§£æå¤±è´¥ï¼Œä¼šè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡ï¼š
        - ç¬¬1æ¬¡é‡è¯•ï¼šç­‰å¾…1ç§’
        - ç¬¬2æ¬¡é‡è¯•ï¼šç­‰å¾…2ç§’
        - ç¬¬3æ¬¡é‡è¯•ï¼šç­‰å¾…4ç§’
        """
        import asyncio
        import inspect
        from dbgpt.core import (
            ModelRequest,
            ModelMessage,
            ModelMessageRoleType,
            ModelRequestContext,
        )

        # æ„å»ºprompt
        prompt = self._build_rewrite_prompt(
            user_query, table_schema_json, table_description, chat_history=chat_history
        )
        print(f"ğŸ” query_rewrite_agent prompt: {prompt}")
        # è°ƒç”¨LLMï¼ˆéæµå¼ï¼‰
        request_params = {
            "messages": [ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)],
            "temperature": 0.1,
            "max_new_tokens": 2000,
            "context": ModelRequestContext(stream=False),
        }

        # å¦‚æœæœ‰model_nameï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if self.model_name:
            request_params["model"] = self.model_name

        request = ModelRequest(**request_params)

        # è·å–å“åº”
        stream_response = self.llm_client.generate_stream(request)

        full_text = ""
        if inspect.isasyncgen(stream_response):

            async def collect_async():
                text = ""
                async for chunk in stream_response:
                    # å®‰å…¨åœ°è·å–æ–‡æœ¬å†…å®¹ï¼Œé¿å… "The content type is not text" é”™è¯¯
                    try:
                        if hasattr(chunk, "has_text") and chunk.has_text:
                            text = chunk.text
                        elif hasattr(chunk, "text"):
                            # å°è¯•è·å– textï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
                            try:
                                text = chunk.text
                            except ValueError:
                                # å¯èƒ½åªæœ‰ thinking å†…å®¹ï¼Œç»§ç»­ç­‰å¾… text å†…å®¹
                                pass
                    except Exception as e:
                        logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                        pass
                return text

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                full_text = loop.run_until_complete(collect_async())
            finally:
                loop.close()
        elif inspect.isgenerator(stream_response):
            for chunk in stream_response:
                # å®‰å…¨åœ°è·å–æ–‡æœ¬å†…å®¹ï¼Œé¿å… "The content type is not text" é”™è¯¯
                try:
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        full_text = chunk.text
                    elif hasattr(chunk, "text"):
                        # å°è¯•è·å– textï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
                        try:
                            full_text = chunk.text
                        except ValueError:
                            # å¯èƒ½åªæœ‰ thinking å†…å®¹ï¼Œç»§ç»­ç­‰å¾… text å†…å®¹
                            pass
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    pass
        else:
            raise Exception(f"Unexpected response type: {type(stream_response)}")

        # è§£æç»“æœ
        result = self._parse_rewrite_result(full_text, user_query)

        return result

    def _parse_rewrite_result(self, llm_output: str, original_query: str) -> Dict:
        """
        è§£æLLMè¾“å‡ºçš„JSONç»“æœ

        å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡º JSONParseError å¼‚å¸¸ä»¥è§¦å‘é‡è¯•æœºåˆ¶
        """
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = llm_output.find("{")
            end_idx = llm_output.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
                if not result.get("rewritten_query"):
                    logger.error("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")
                    raise JSONParseError("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")

                # éªŒè¯ relevant_columns æ ¼å¼
                relevant_columns = result.get("relevant_columns", [])
                if relevant_columns:
                    for idx, col in enumerate(relevant_columns):
                        if not isinstance(col, dict) or "column_name" not in col:
                            logger.error(f"relevant_columns[{idx}] æ ¼å¼é”™è¯¯: {col}")
                            raise JSONParseError(
                                f"relevant_columns[{idx}] ç¼ºå°‘ 'column_name' å­—æ®µ"
                            )

                # æ·»åŠ åŸå§‹é—®é¢˜
                result["original_query"] = original_query

                # æå–é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚æœæœ‰ï¼‰
                domain_knowledge = result.get("domain_knowledge")
                if domain_knowledge and isinstance(domain_knowledge, dict):
                    column_name = domain_knowledge.get("column_name")
                    knowledge = domain_knowledge.get("knowledge")
                    if column_name and knowledge:
                        result["_extracted_knowledge"] = {
                            "column_name": column_name,
                            "knowledge": knowledge,
                        }

                logger.info("âœ… JSONè§£ææˆåŠŸ")
                return result
            else:
                logger.error("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSON")
                raise JSONParseError("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSONå†…å®¹")

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            raise JSONParseError(f"JSONè§£æå¤±è´¥: {e}")

    def _default_result(self, original_query: str) -> Dict:
        """
        è¿”å›é»˜è®¤ç»“æœï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰
        """
        return {
            "original_query": original_query,
            "rewritten_query": original_query,
            "relevant_columns": [],
            "analysis_suggestions": ["è¯·æ˜ç¡®éœ€è¦åˆ†æçš„æ•°æ®ç»´åº¦å’ŒæŒ‡æ ‡"],
            "analysis_logic": "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ•°æ®åˆ†æ",
        }
