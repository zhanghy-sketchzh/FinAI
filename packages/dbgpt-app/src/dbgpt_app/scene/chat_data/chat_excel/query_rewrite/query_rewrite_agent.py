"""
Queryæ”¹å†™Agent - å‚è€ƒformat_sql/backend/agents/query_rewrite_assistant.py
è´Ÿè´£æ ¹æ®æ•°æ®ç†è§£ä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·é—®é¢˜ï¼Œæ˜ç¡®ç›¸å…³åˆ—å’Œåˆ†æå»ºè®®
"""
# ruff: noqa: E501

import json
import logging
from typing import AsyncIterator, Dict, List, Union

logger = logging.getLogger(__name__)


class JSONParseError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼šJSONè§£æå¤±è´¥"""

    pass


def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€

    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬

    Returns:
        "zh" å¦‚æœä¸»è¦æ˜¯ä¸­æ–‡ï¼Œ"en" å¦‚æœä¸»è¦æ˜¯è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€
    """
    if not text or not text.strip():
        return "en"

    import re

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_pattern = re.compile(r"[\u4e00-\u9fff]+")
    chinese_chars = chinese_pattern.findall(text)
    chinese_count = sum(len(match) for match in chinese_chars)

    # è®¡ç®—æ€»å­—ç¬¦æ•°ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ï¼‰
    total_chars = len(re.sub(r'[\s\.,;:!?\'"\-_()\[\]{}]', "", text))

    if total_chars == 0:
        return "en"

    # è®¡ç®—ä¸­æ–‡å æ¯”
    chinese_ratio = chinese_count / total_chars if total_chars > 0 else 0

    # å¦‚æœä¸­æ–‡å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡è¾“å…¥
    return "zh" if chinese_ratio > 0.3 else "en"


class QueryRewriteAgent:
    """
    Queryæ”¹å†™Agent - ç®€åŒ–ç‰ˆæœ¬
    åŠŸèƒ½ï¼š
    1. æ ¹æ®æ•°æ®å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„æé—®
    2. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—
    3. æä¾›åˆ†æå»ºè®®å’Œé€»è¾‘æ”¯æ’‘
    """

    def __init__(self, llm_client=None, model_name=None):
        """
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸ºNoneåˆ™è¿”å›é»˜è®¤ç»“æœï¼‰
            model_name: æ¨¡å‹åç§°
        """
        self.llm_client = llm_client
        self.model_name = model_name

    async def rewrite_query_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        æµå¼æ”¹å†™ç”¨æˆ·query
        
        å…ˆæµå¼è¾“å‡ºLLMçš„åŸå§‹è¾“å‡ºï¼ˆæ–‡æœ¬å’ŒJSONï¼‰ï¼Œç„¶åè¾“å‡ºè§£æåçš„ç»“æœ
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunk
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœï¼ˆå½“æµå¼è¾“å‡ºå®Œæˆæ—¶ï¼‰
        """
        logger.info(f"Queryæ”¹å†™ï¼ˆæµå¼ï¼‰ - åŸå§‹é—®é¢˜: {user_query}")

        # å°è¯•ä½¿ç”¨LLMæ”¹å†™
        if self.llm_client:
            try:
                logger.info("å°è¯•ä½¿ç”¨LLMè¿›è¡ŒQueryæ”¹å†™ï¼ˆæµå¼ï¼‰...")
                full_text = ""
                async for chunk in self._llm_based_rewrite_stream(
                    user_query,
                    table_schema_json,
                    table_description,
                    chat_history=chat_history,
                ):
                    if isinstance(chunk, str):
                        # æµå¼è¾“å‡ºåŸå§‹æ–‡æœ¬chunk
                        full_text = chunk
                        yield chunk
                    elif isinstance(chunk, dict):
                        # è¿”å›æœ€ç»ˆè§£æç»“æœ
                        logger.info(f"âœ… LLMæ”¹å†™æˆåŠŸï¼ˆæµå¼ï¼‰ - æ”¹å†™ç»“æœ: {chunk.get('rewritten_query', '')}")
                        yield chunk
                        return
            except JSONParseError as e:
                logger.error(f"âŒ LLMæ”¹å†™å¤±è´¥ï¼ˆJSONè§£æé”™è¯¯ï¼‰: {e}")
                logger.warning("âš ï¸ ä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
            except Exception as e:
                logger.warning(f"âš ï¸ LLMæ”¹å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™ä½œä¸ºfallback")
        else:
            logger.info("LLMå®¢æˆ·ç«¯æœªé…ç½®ï¼Œä½¿ç”¨è§„åˆ™æ”¹å†™")

        # Fallback: åŸºäºè§„åˆ™çš„ç®€å•æ”¹å†™
        result = self._rule_based_rewrite(user_query, table_schema_json)
        logger.info(f"è§„åˆ™æ”¹å†™ - æ”¹å†™ç»“æœ: {result['rewritten_query']}")
        yield result

    
    
    def _rule_based_rewrite(self, user_query: str, table_schema_json: str) -> Dict:
        """
        åŸºäºè§„åˆ™çš„Queryæ”¹å†™ï¼ˆä¸ä¾èµ–LLMï¼Œå¿«é€Ÿå¯é ï¼‰
        """
        try:
            # è§„åˆ™æ”¹å†™ä¸è¿›è¡Œç›¸å…³æ€§åˆ¤æ–­ï¼Œç»Ÿä¸€è¿”å› is_relevant=True
            # ç›¸å…³æ€§åˆ¤æ–­äº¤ç»™LLMå¤„ç†ï¼ˆæ›´å‡†ç¡®ï¼‰
            
            # è§£æschema JSON
            if isinstance(table_schema_json, str):
                schema_obj = json.loads(table_schema_json)
            else:
                schema_obj = table_schema_json

            # è·å–åˆ—ä¿¡æ¯
            schema_data = (
                schema_obj.get("columns", []) if isinstance(schema_obj, dict) else []
            )

            relevant_columns = []
            analysis_suggestions = []

            # åˆ†æç”¨æˆ·queryä¸­æåˆ°çš„å…³é”®è¯
            query_lower = user_query.lower()

            # æ£€æµ‹æ—¶é—´ç›¸å…³çš„åˆ†æ
            if any(
                keyword in query_lower for keyword in ["åŒæ¯”", "ç¯æ¯”", "yoy", "mom"]
            ):
                # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
                date_cols = [
                    col
                    for col in schema_data
                    if any(
                        kw in col.get("column_name", "").lower()
                        for kw in ["date", "æ—¥æœŸ", "time", "æ—¶é—´"]
                    )
                ]
                if date_cols:
                    relevant_columns.append(
                        {
                            "column_name": date_cols[0]["column_name"],
                            "usage": "æ—¶é—´ç­›é€‰å’Œåˆ†ç»„æ¡ä»¶ï¼Œéœ€è¦åŒ…å«è¶³å¤Ÿçš„å†å²æ•°æ®",
                        }
                    )

                analysis_suggestions.append(
                    "åŒæ¯”åˆ†æéœ€è¦å»å¹´åŒæœŸæ•°æ®ï¼Œç¡®ä¿WHEREæ¡ä»¶åŒ…å«è‡³å°‘ä¸¤å¹´çš„æ•°æ®"
                )
                analysis_suggestions.append(
                    "ç¯æ¯”åˆ†æéœ€è¦ä¸Šä¸€ä¸ªå‘¨æœŸæ•°æ®ï¼Œä½¿ç”¨LAGçª—å£å‡½æ•°"
                )
                analysis_suggestions.append(
                    "æ—¶é—´èŒƒå›´ç¤ºä¾‹ï¼šWHERE è®¢å•æ—¥æœŸ >= '2021-01-01' AND è®¢å•æ—¥æœŸ < '2023-01-01'"
                )

            # æ£€æµ‹åœ°åŸŸåˆ†æ
            if any(
                keyword in user_query
                for keyword in [
                    "åŒºåŸŸ",
                    "åœ°åŒº",
                    "ååŒ—",
                    "åä¸œ",
                    "åå—",
                    "ä¸œåŒ—",
                    "è¥¿åŒ—",
                    "è¥¿å—",
                    "ä¸­å—",
                ]
            ):
                region_cols = [
                    col for col in schema_data if "åŒºåŸŸ" in col.get("column_name", "")
                ]
                if region_cols:
                    relevant_columns.append(
                        {
                            "column_name": region_cols[0]["column_name"],
                            "usage": "åœ°åŸŸç­›é€‰æˆ–åˆ†ç»„æ¡ä»¶",
                        }
                    )
                    analysis_suggestions.append("ä½¿ç”¨'åŒºåŸŸ'å­—æ®µè¿›è¡Œç­›é€‰æˆ–åˆ†ç»„")

            # æ£€æµ‹æŒ‡æ ‡åˆ†æ
            metric_keywords = {
                "åˆ©æ¶¦": "profit",
                "é”€å”®é¢": "sales",
                "é”€é‡": "quantity",
                "æ•°é‡": "quantity",
            }

            for cn_keyword, en_keyword in metric_keywords.items():
                if cn_keyword in user_query:
                    metric_cols = [
                        col
                        for col in schema_data
                        if cn_keyword in col.get("column_name", "")
                    ]
                    if metric_cols:
                        relevant_columns.append(
                            {
                                "column_name": metric_cols[0]["column_name"],
                                "usage": "èšåˆæŒ‡æ ‡ï¼Œéœ€è¦è¿›è¡ŒSUM/AVGç­‰èšåˆè¿ç®—",
                            }
                        )
                        analysis_suggestions.append(
                            f"å¯¹'{cn_keyword}'å­—æ®µè¿›è¡Œèšåˆè®¡ç®—ï¼ˆSUMæ±‚å’Œï¼‰"
                        )

            # æ„å»ºæ”¹å†™åçš„query
            rewritten_query = self._enhance_query(
                user_query, relevant_columns, analysis_suggestions
            )

            # æ„å»ºåˆ†æé€»è¾‘
            analysis_logic = self._build_analysis_logic(user_query, relevant_columns)

            return {
                "original_query": user_query,
                "is_relevant": True,
                "rewritten_query": rewritten_query,
                "relevant_columns": relevant_columns,
                "analysis_suggestions": analysis_suggestions,
                "analysis_logic": analysis_logic,
            }

        except Exception as e:
            logger.error(f"è§„åˆ™æ”¹å†™å¤±è´¥: {e}", exc_info=True)
            return self._default_result(user_query)

    def _enhance_query(
        self, user_query: str, relevant_columns: List[Dict], suggestions: List[str]
    ) -> str:
        """
        å¢å¼ºç”¨æˆ·query
        """
        if not relevant_columns:
            return user_query

        # æ·»åŠ æ˜ç¡®çš„å­—æ®µå¼•ç”¨
        col_names = [col["column_name"] for col in relevant_columns]
        enhanced = user_query

        # å¦‚æœæ˜¯ç®€çŸ­çš„queryï¼Œæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
        if len(user_query) < 20:
            enhanced = f"åŸºäºæ•°æ®è¡¨ï¼Œä½¿ç”¨å­—æ®µã€{', '.join(col_names)}ã€‘æ¥{user_query}"

        return enhanced

    def _build_analysis_logic(
        self, user_query: str, relevant_columns: List[Dict]
    ) -> str:
        """
        æ„å»ºåˆ†æé€»è¾‘
        """
        logic_parts = []

        # ç­›é€‰æ¡ä»¶
        filter_cols = [
            col for col in relevant_columns if "ç­›é€‰" in col.get("usage", "")
        ]
        if filter_cols:
            logic_parts.append(
                f"1. ç­›é€‰æ¡ä»¶ï¼š{', '.join([c['column_name'] for c in filter_cols])}"
            )

        # åˆ†ç»„ç»´åº¦
        group_cols = [col for col in relevant_columns if "åˆ†ç»„" in col.get("usage", "")]
        if group_cols:
            logic_parts.append(
                f"2. åˆ†ç»„ç»´åº¦ï¼š{', '.join([c['column_name'] for c in group_cols])}"
            )

        # èšåˆæŒ‡æ ‡
        agg_cols = [col for col in relevant_columns if "èšåˆ" in col.get("usage", "")]
        if agg_cols:
            logic_parts.append(
                f"3. èšåˆæŒ‡æ ‡ï¼š{', '.join([c['column_name'] for c in agg_cols])}"
            )

        if logic_parts:
            return "\n".join(logic_parts)
        else:
            return "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ ‡å‡†çš„æ•°æ®æŸ¥è¯¢å’Œåˆ†æ"

    def _build_rewrite_prompt(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> str:
        """
        æ„å»ºæ”¹å†™prompt
        """
        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = ""
        if chat_history and len(chat_history) > 0:
            history_context = "\n=== å†å²å¯¹è¯ä¸Šä¸‹æ–‡ ===\n"
            # åªä¿ç•™æœ€è¿‘4è½®å¯¹è¯ï¼ˆ8æ¡æ¶ˆæ¯ï¼‰ï¼Œé¿å…promptè¿‡é•¿
            recent_history = (
                chat_history[-8:] if len(chat_history) > 8 else chat_history
            )

            for msg in recent_history:
                role = msg.get("role", "user") if isinstance(msg, dict) else "user"
                content = (
                    str(msg.get("content", "")) if isinstance(msg, dict) else str(msg)
                )

                # æ ¹æ®å®é™…è§’è‰²æ˜¾ç¤º
                if "human" in role.lower() or "user" in role.lower():
                    role_display = "ç”¨æˆ·"
                elif "ai" in role.lower() or "assistant" in role.lower():
                    role_display = "åŠ©æ‰‹"
                else:
                    role_display = role

                history_context += f"\n{role_display}: {content}\n"

        # æ£€æµ‹ç”¨æˆ·è¾“å…¥è¯­è¨€
        user_language = detect_language(user_query)
        logger.info(f"ğŸŒ Queryæ”¹å†™ - æ£€æµ‹åˆ°ç”¨æˆ·è¾“å…¥è¯­è¨€: {user_language}")

        # æ ¹æ®è¯­è¨€é€‰æ‹©prompt
        if user_language == "zh":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªæ•°æ®åˆ†æé—®é¢˜ï¼Œä½ éœ€è¦ï¼š
1. æ ¹æ®ç”¨æˆ·å†å²é—®é¢˜å’Œå›ç­”ï¼Œå……åˆ†ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜
2. æ ¹æ®æ•°æ®è¡¨çš„å­—æ®µä¿¡æ¯ï¼Œè¡¥å……å®Œå–„ç”¨æˆ·çš„é—®é¢˜
3. æ˜ç¡®æŒ‡å‡ºå¯èƒ½ç”¨åˆ°çš„åˆ—ï¼ˆåŒ…æ‹¬ç­›é€‰æ¡ä»¶åˆ—ã€åˆ†ç»„ç»´åº¦åˆ—ã€èšåˆæŒ‡æ ‡åˆ—ï¼‰
4. æä¾›3-5æ¡åˆ†æå»ºè®®ï¼Œè¯´æ˜å¦‚ä½•åˆ†æè¿™ä¸ªé—®é¢˜
5. ç»™å‡ºæ¸…æ™°çš„åˆ†æé€»è¾‘
6. å¦‚æœç”¨æˆ·åœ¨å¯¹è¯æˆ–å½“å‰å†å²é—®ç­”ä¸Šä¸‹æ–‡ä¸­çº æ­£æˆ–è¡¥å……äº†å­—æ®µçš„ä½¿ç”¨æ–¹æ³•ã€ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰å…³é”®çŸ¥è¯†ï¼Œè¯·æå–å¹¶è®°å½•ä½œä¸ºdomain_knowledgeå­—æ®µ

=== æ•°æ®è¡¨å­—æ®µè¯¦ç»†ä¿¡æ¯ ===
{table_schema_json}

**æ³¨æ„**ï¼šå­—æ®µä¿¡æ¯ä¸­å¯èƒ½åŒ…å« `domain_knowledge` å­—æ®µï¼Œè¿™æ˜¯ä¹‹å‰ä»ç”¨æˆ·å¯¹è¯ä¸­å­¦ä¹ åˆ°çš„ä¸šåŠ¡çŸ¥è¯†ï¼Œè¯·ä¼˜å…ˆå‚è€ƒä½¿ç”¨ã€‚

=== æ•°æ®è¡¨æè¿° ===
{table_description}

{history_context}
=== ç”¨æˆ·å½“å‰é—®é¢˜ ===
{user_query}

=== è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ ===
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "is_relevant": true,  // å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸æ•°æ®è¡¨åˆ†æç›¸å…³ã€‚å¦‚æœæ˜¯é—²èŠï¼ˆå¦‚"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"ã€"ä½ åƒé¥­äº†å—"ï¼‰åˆ™ä¸ºfalse
  "conversation_title": "å¯¹è¯ä¸»é¢˜ï¼ˆ10å­—ä»¥å†…ï¼Œæ¦‚æ‹¬å½“å‰é—®é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼Œå¦‚ï¼šé”€å”®é¢åˆ†æã€åˆ©æ¶¦æ’åã€å‘˜å·¥ç»Ÿè®¡ï¼‰",
  "rewritten_query": "æ”¹å†™åçš„å®Œæ•´é—®é¢˜ï¼Œæ˜ç¡®æŒ‡å‡ºéœ€è¦åˆ†æçš„ç»´åº¦å’ŒæŒ‡æ ‡",
  "relevant_columns": [
    {{
      "column_name": "åˆ—å",
      "usage": "ç”¨é€”è¯´æ˜ï¼ˆå¦‚ï¼šç­›é€‰æ¡ä»¶/åˆ†ç»„ç»´åº¦/èšåˆæŒ‡æ ‡ï¼‰"
    }}
  ],
  "analysis_suggestions": [
    "å»ºè®®1ï¼šå…·ä½“çš„åˆ†ææ­¥éª¤æˆ–æ³¨æ„äº‹é¡¹",
    "å»ºè®®2ï¼š...",
    "å»ºè®®3ï¼š..."
    ...
  ],
  "analysis_logic": "åˆ†æé€»è¾‘çš„è¯¦ç»†è¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š1) éœ€è¦ç­›é€‰å“ªäº›æ•°æ® 2) æŒ‰ä»€ä¹ˆç»´åº¦åˆ†ç»„ 3) è®¡ç®—å“ªäº›æŒ‡æ ‡ 4) å¦‚ä½•æ’åºæˆ–å¯¹æ¯”",
  "domain_knowledge": {{
    "column_name": "å­—æ®µåï¼ˆå¦‚æœç”¨æˆ·çº æ­£æˆ–è¡¥å……äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•ï¼‰",
    "knowledge": "ç”¨æˆ·è¡¥å……çš„ä¸šåŠ¡çŸ¥è¯†æˆ–æ•°æ®å¤„ç†æŠ€å·§ï¼ˆä¾‹å¦‚ï¼š'è¯¥å­—æ®µæ ¼å¼ä¸ºH1,H2ï¼Œé€—å·å‰æ˜¯H1ç»©æ•ˆï¼Œé€—å·åæ˜¯H2ç»©æ•ˆï¼Œéœ€è¦ç”¨SPLIT_PARTå‡½æ•°åˆ†å‰²'ï¼‰"
  }}
}}


**å…³äº is_relevant çš„è¯´æ˜**ï¼š
- åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸å½“å‰æ•°æ®è¡¨çš„åˆ†æç›¸å…³
- å¦‚æœæ˜¯æ•°æ®åˆ†æé—®é¢˜ï¼ˆå¦‚"é”€å”®é¢æ˜¯å¤šå°‘"ã€"åˆ©æ¶¦æ’å"ã€"åŒæ¯”å¢é•¿"ç­‰ï¼‰ï¼Œè®¾ä¸º true
- å¦‚æœæ˜¯é—²èŠæˆ–ä¸æ•°æ®è¡¨æ— å…³çš„é—®é¢˜ï¼ˆå¦‚"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"ã€"ä½ åƒé¥­äº†å—"ã€"è®²ä¸ªç¬‘è¯"ç­‰ï¼‰ï¼Œè®¾ä¸º false
- å½“ is_relevant ä¸º false æ—¶ï¼Œå…¶ä»–å­—æ®µå¯ä»¥ç®€åŒ–æˆ–çœç•¥

**å…³äº conversation_title çš„è¯´æ˜**ï¼š
- ç”¨10ä¸ªå­—ä»¥å†…æ¦‚æ‹¬å½“å‰é—®é¢˜çš„æ ¸å¿ƒåˆ†æä¸»é¢˜
- åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¾¿äºç”¨æˆ·åœ¨å¯¹è¯åˆ—è¡¨ä¸­å¿«é€Ÿè¯†åˆ«
- ç¤ºä¾‹ï¼š"é”€å”®é¢è¶‹åŠ¿"ã€"åˆ©æ¶¦æ’ååˆ†æ"ã€"å‘˜å·¥æ•°é‡ç»Ÿè®¡"ã€"åŒºåŸŸå¯¹æ¯”"
- å¦‚æœæœ‰å†å²å¯¹è¯ï¼Œåº”ç»“åˆå†å²ä¸Šä¸‹æ–‡ç”Ÿæˆæ›´å‡†ç¡®çš„ä¸»é¢˜

**å…³äº domain_knowledge çš„è¯´æ˜**ï¼š
- åªæœ‰å½“ç”¨æˆ·æ˜ç¡®çº æ­£ã€è¡¥å……æˆ–è¯´æ˜äº†æŸä¸ªå­—æ®µçš„ä½¿ç”¨æ–¹æ³•æ—¶æ‰éœ€è¦å¡«å†™
- å¦‚æœç”¨æˆ·åªæ˜¯æ™®é€šæé—®ï¼Œä¸éœ€è¦å¡«å†™æ­¤å­—æ®µï¼ˆå¯ä»¥çœç•¥æˆ–è®¾ä¸º nullï¼‰
- çŸ¥è¯†åº”è¯¥æ˜¯å¯å¤ç”¨çš„ã€å¯¹æœªæ¥åˆ†ææœ‰å¸®åŠ©çš„å…³é”®ä¿¡æ¯,æ¯”å¦‚ä¸šåŠ¡è§„åˆ™ã€æ•°æ®å¤„ç†æŠ€å·§ç­‰,è¿™éƒ¨åˆ†çŸ¥è¯†ä¼šä½œä¸ºé¢†åŸŸçŸ¥è¯†ä¿å­˜åˆ°æ•°æ®åº“ä¸­,ç”¨äºåç»­çš„åˆ†æå’Œæ¨ç†,å¯ä»¥å¤ç”¨è¿™äº›çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¦‚æœä¸Šé¢å·²ç»è®°å½•äº†é¢†åŸŸçŸ¥è¯†ï¼Œè¯·ä¸è¦é‡å¤è®°å½•ã€‚

**é‡è¦ - è¯­è¨€è¦æ±‚**ï¼š
- ç”¨æˆ·çš„é—®é¢˜æ˜¯**ä¸­æ–‡**
- ä½ å¿…é¡»ç”¨**ä¸­æ–‡**å›å¤JSONä¸­çš„æ‰€æœ‰å­—æ®µ
- "rewritten_query"ã€"usage"ã€"analysis_suggestions"ã€"analysis_logic" ç­‰å­—æ®µå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**
- å³ä½¿è¡¨å­—æ®µåæ˜¯ä¸­è‹±æ–‡æ··åˆï¼Œä½ çš„æè¿°å’Œåˆ†æä¹Ÿå¿…é¡»ä½¿ç”¨**ä¸­æ–‡**

**é‡è¦ - å­—ç¬¦ä¸²ç²¾ç¡®åŒ¹é…è¦æ±‚**ï¼š
- åœ¨æ”¹å†™é—®é¢˜æ—¶ï¼Œå¦‚æœç”¨æˆ·æåˆ°äº†å…·ä½“çš„éƒ¨é—¨åç§°ã€åˆ†ç±»å€¼ç­‰å­—ç¬¦ä¸²ï¼Œå¿…é¡»ä¿æŒå®Œå…¨ä¸€è‡´
- å¦‚æœç”¨æˆ·é—®é¢˜ä¸­åŒ…å«å…·ä½“çš„å­—ç¬¦ä¸²å€¼ï¼Œåœ¨"rewritten_query"ä¸­å¿…é¡»ä¿æŒåŸæ ·ï¼Œä¸èƒ½ä¿®æ”¹

ç°åœ¨è¯·ç»“åˆå†å²ä¸Šä¸‹æ–‡åŠç”¨æˆ·å½“å‰é—®é¢˜ï¼Œåˆ†æç”¨æˆ·çš„çœŸå®æ„å›¾ï¼Œè¡¥å……æ”¹å†™å½“å‰é—®é¢˜å¹¶ç”¨ä¸­æ–‡è¾“å‡ºJSONï¼š
"""
        else:
            prompt = f"""You are a data analysis expert. The user has asked a data analysis question. You need to:
1. Understand the user's real intent based on historical questions and answers, and enhance the current question
2. Enhance the user's question based on the data table field information
3. Clearly identify the columns that may be used (including filter condition columns, grouping dimension columns, and aggregation indicator columns)
4. Provide 3-5 analysis suggestions explaining how to analyze this question
5. Give a clear analysis logic
6. If the user has corrected or supplemented field usage methods, business rules, data processing techniques, or other key knowledge in the conversation or current historical Q&A context, extract and record it as the domain_knowledge field

=== Data Table Field Details ===
{table_schema_json}

**Note**: The field information may contain a `domain_knowledge` field, which is business knowledge learned from previous user conversations. Please prioritize using this.

=== Data Table Description ===
{table_description}

{history_context}
=== User's Current Question ===
{user_query}

=== Output Format (JSON) ===
Please strictly follow the following JSON format:
{{
  "is_relevant": true,  // Boolean value indicating whether the user's question is related to data table analysis. If it's small talk (e.g., "How's the weather today", "Did you eat"), set to false
  "conversation_title": "Conversation topic (within 10 characters, summarizing the core content of the current question, e.g., Sales Analysis, Profit Ranking, Employee Stats)",
  "rewritten_query": "The enhanced complete question, clearly indicating the dimensions and indicators to be analyzed",
  "relevant_columns": [
    {{
      "column_name": "Column name",
      "usage": "Usage description (e.g., filter condition/grouping dimension/aggregation indicator)"
    }}
  ],
  "analysis_suggestions": [
    "Suggestion 1: Specific analysis steps or considerations",
    "Suggestion 2: ...",
    "Suggestion 3: ..."
    ...
  ],
  "analysis_logic": "Detailed explanation of analysis logic, including: 1) Which data to filter 2) What dimension to group by 3) Which indicators to calculate 4) How to sort or compare",
  "domain_knowledge": {{
    "column_name": "Field name (if the user has corrected or supplemented the usage method of a field)",
    "knowledge": "Business knowledge or data processing techniques supplemented by the user (e.g., 'This field format is H1,H2, before the comma is H1 performance, after the comma is H2 performance, need to use SPLIT_PART function to split')"
  }}
}}



**About is_relevant**:
- Determine whether the user's question is related to the analysis of the current data table
- If it's a data analysis question (e.g., "What is the sales amount", "Profit ranking", "Year-over-year growth"), set to true
- If it's small talk or unrelated to the data table (e.g., "How's the weather today", "Did you eat", "Tell me a joke"), set to false
- When is_relevant is false, other fields can be simplified or omitted

**About conversation_title**:
- Summarize the core analysis topic of the current question in 10 characters or less
- Should be concise and clear for users to quickly identify in the conversation list
- Examples: "Sales Trend", "Profit Ranking", "Employee Count", "Region Compare"
- If there is historical conversation, combine the historical context to generate a more accurate topic

**About domain_knowledge**:
- Only fill in when the user explicitly corrects, supplements, or explains the usage method of a field
- If the user is just asking a normal question, this field is not required (can be omitted or set to null)
- Knowledge should be reusable and helpful for future analysis, such as business rules, data processing techniques, etc. This knowledge will be saved to the database as domain knowledge for subsequent analysis and reasoning, and can be reused to answer user questions. If domain knowledge has already been recorded above, please do not repeat it.

**IMPORTANT - Language Requirement**:
- The user's question is in ENGLISH
- You MUST respond in ENGLISH for ALL fields in the JSON output
- The "rewritten_query", "usage", "analysis_suggestions", and "analysis_logic" fields MUST be in ENGLISH
- Even though the table field names are in Chinese, your descriptions and analysis MUST be in ENGLISH

**IMPORTANT - String Exact Matching Requirement**:
- When rewriting the question, if the user mentions specific department names, category values, or other strings, they must be kept exactly as they are
- If the user's question contains specific string values, they must be kept unchanged in "rewritten_query"

Now please combine the historical context and the user's current question, analyze the user's real intent, enhance the current question and output JSON IN ENGLISH:
"""
        return prompt

    async def _llm_based_rewrite_stream(
        self,
        user_query: str,
        table_schema_json: str,
        table_description: str,
        chat_history: list = None,
    ) -> AsyncIterator[Union[str, Dict]]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæµå¼Queryæ”¹å†™
        
        Yields:
            str: æµå¼è¾“å‡ºçš„åŸå§‹æ–‡æœ¬chunkï¼ˆç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬ï¼‰
            Dict: æœ€ç»ˆè§£æåçš„æ”¹å†™ç»“æœ
        """
        import inspect

        from dbgpt.core import (
            ModelMessage,
            ModelMessageRoleType,
            ModelRequest,
            ModelRequestContext,
        )

        # æ„å»ºprompt
        prompt = self._build_rewrite_prompt(
            user_query, table_schema_json, table_description, chat_history=chat_history
        )
        logger.debug(f"ğŸ” query_rewrite_agent prompt: {prompt[:200]}...")
        
        # è°ƒç”¨LLMï¼ˆæµå¼ï¼‰
        request_params = {
            "messages": [ModelMessage(role=ModelMessageRoleType.HUMAN, content=prompt)],
            "temperature": 0.1,
            "max_new_tokens": 2000,
            "context": ModelRequestContext(stream=True),
        }

        # å¦‚æœæœ‰model_nameï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
        if self.model_name:
            request_params["model"] = self.model_name

        request = ModelRequest(**request_params)

        # è·å–æµå¼å“åº”
        stream_response = self.llm_client.generate_stream(request)

        full_text = ""
        if inspect.isasyncgen(stream_response):
            async for chunk in stream_response:
                # å®‰å…¨åœ°è·å–æ–‡æœ¬å†…å®¹
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            # å¯èƒ½åªæœ‰ thinking å†…å®¹ï¼Œç»§ç»­ç­‰å¾… text å†…å®¹
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        elif inspect.isgenerator(stream_response):
            for chunk in stream_response:
                try:
                    chunk_text = ""
                    if hasattr(chunk, "has_text") and chunk.has_text:
                        chunk_text = chunk.text
                    elif hasattr(chunk, "text"):
                        try:
                            chunk_text = chunk.text
                        except ValueError:
                            continue
                    
                    if chunk_text:
                        full_text = chunk_text
                        # æµå¼è¾“å‡ºç´¯ç§¯çš„å®Œæ•´æ–‡æœ¬
                        yield full_text
                except Exception as e:
                    logger.debug(f"è·å–chunk.textå¤±è´¥: {e}")
                    continue
        else:
            raise Exception(f"Unexpected response type: {type(stream_response)}")

        # æµå¼è¾“å‡ºå®Œæˆåï¼Œè§£æç»“æœå¹¶è¿”å›
        if full_text:
            try:
                result = self._parse_rewrite_result(full_text, user_query)
                yield result
            except JSONParseError as e:
                logger.error(f"JSONè§£æå¤±è´¥: {e}")
                raise

    def _parse_rewrite_result(self, llm_output: str, original_query: str) -> Dict:
        """
        è§£æLLMè¾“å‡ºçš„JSONç»“æœ

        å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡º JSONParseError å¼‚å¸¸ä»¥è§¦å‘é‡è¯•æœºåˆ¶
        """
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = llm_output.find("{")
            end_idx = llm_output.rfind("}") + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                result = json.loads(json_str)

                # éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
                if "is_relevant" not in result:
                    logger.warning("JSONç¼ºå°‘ 'is_relevant' å­—æ®µï¼Œé»˜è®¤è®¾ä¸º true")
                    result["is_relevant"] = True
                
                if not result.get("rewritten_query"):
                    logger.error("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")
                    raise JSONParseError("JSONç¼ºå°‘å¿…è¦å­—æ®µ 'rewritten_query'")
                
                # conversation_title ç”± LLM ç”Ÿæˆï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸è®¾ç½®ï¼ˆå‰ç«¯ä¼šä½¿ç”¨ user_input ä½œä¸ºé»˜è®¤å€¼ï¼‰

                # éªŒè¯ relevant_columns æ ¼å¼
                relevant_columns = result.get("relevant_columns", [])
                if relevant_columns:
                    for idx, col in enumerate(relevant_columns):
                        if not isinstance(col, dict) or "column_name" not in col:
                            logger.error(f"relevant_columns[{idx}] æ ¼å¼é”™è¯¯: {col}")
                            raise JSONParseError(
                                f"relevant_columns[{idx}] ç¼ºå°‘ 'column_name' å­—æ®µ"
                            )

                # æ·»åŠ åŸå§‹é—®é¢˜
                result["original_query"] = original_query

                # æå–é¢†åŸŸçŸ¥è¯†ï¼ˆå¦‚æœæœ‰ï¼‰
                domain_knowledge = result.get("domain_knowledge")
                if domain_knowledge and isinstance(domain_knowledge, dict):
                    column_name = domain_knowledge.get("column_name")
                    knowledge = domain_knowledge.get("knowledge")
                    if column_name and knowledge:
                        result["_extracted_knowledge"] = {
                            "column_name": column_name,
                            "knowledge": knowledge,
                        }

                logger.info("âœ… JSONè§£ææˆåŠŸ")
                return result
            else:
                logger.error("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSON")
                raise JSONParseError("æ— æ³•ä»LLMè¾“å‡ºä¸­æå–JSONå†…å®¹")

        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            raise JSONParseError(f"JSONè§£æå¤±è´¥: {e}")

    def _default_result(self, original_query: str) -> Dict:
        """
        è¿”å›é»˜è®¤ç»“æœï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰
        """
        return {
            "original_query": original_query,
            "is_relevant": True,
            "rewritten_query": original_query,
            "relevant_columns": [],
            "analysis_suggestions": ["è¯·æ˜ç¡®éœ€è¦åˆ†æçš„æ•°æ®ç»´åº¦å’ŒæŒ‡æ ‡"],
            "analysis_logic": "åŸºäºç”¨æˆ·é—®é¢˜è¿›è¡Œæ•°æ®åˆ†æ",
        }
